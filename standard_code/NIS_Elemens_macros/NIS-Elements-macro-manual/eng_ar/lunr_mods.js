






 

// creates sourceData array
var sourceDatamods = [
   
// srcData #1 - from HTML page content + frontmatter
   
   { title: "Define the classifier of air voids",
     xmlid: "id|CR_CONC_ClassifyAirVoidsx",
     content: " Opens the concrete Classifier definition. The classification of air voids is based on a neural network which must be “taught” first in order to produce reliable results. During the classifier definition, every color shade in the image (either representing background or air voids) should be clicked at least once (i.e: if you have a two-color background, click on each color at least once). The classifier is applied only if both the air voids (foreground) and the background were defined.Once the classifier is taught, click Update. The current image will be classified and a binary layer with the objects detected as air voids will be displayed over it. You can toggle the binary layer visibility using Show. When testing the classifier by the Test button, inaccurate results may appear. This happens when the learning mechanism of the neural network gets confused. In such cases, determining additional air voids/background points should help. Or reset the classifier completely and teach it from scratch. Once the test passes successfully, the classifier can not fail during the measurement. Figure&nbsp;593.&nbsp; Live Turns on the live signal from the camera and activates the joystick. You can move the stage and look for an optimal texture suitable for the classifier definition. Reset classifier Resets the current classifier. Define air voids Use this button to teach the classifier to recognize air voids - click in the image on the air voids several times. Define background Use this button to determine the background - click in the image on the background several times.Show Toggle the binary layer visibility.Update Updates the classifier with the new definition.Auto load/save If checked, the classifier settings will be remembered for future use and it will be loaded automatically when NIS-Elements is started the next time.OK Confirms the settings and closes the Classifier definition window.Cancel Discards all the settings and closes the Classifier definition window. ",
     id: 0 }, 
   { title: "Measure",
     xmlid: "id|CR_CONC_Meteringx",
     content: " This button executes the concrete measurement. At first a wizard appears where the Measurement name has to be filled. A new directory with this name will be created in the shown Report directory. The measurement report will be saved to this directory. It is required to insert a unique measurement name in the scope of the Root directory otherwise the wizard will not proceed. Please, do not use “test” as the measurement name. It is reserved for testing purposes. The reports in the “test” directory will be overwritten without notice.After the measurement name is inserted, click Next &gt; to start prefocusing on the concrete sample. You will be prompt to focus manually (if the three-point prefocus is selected, you will need to focus three-times, see Focusing in the Measurement tab in   ). The focusing can be done by the mouse wheel or by the joystick wheel. Click Next &gt; and confirm the calibration of the document by clicking Finish. Then the Measurement progress window appears. Information about the measurement progress is displayed on the left. The top bar shows the task progress in the scope of the current concrete sample and the bottom bar in the scope of the whole measurement. Figure&nbsp;592.&nbsp; Save current image... Saves the current image to a .jpg file. Refresh detection Refreshes the current detection. Focus manually... Manually focuses on the current measurement step. Previous step Moves to the previous step in the measurement sequence. It is enabled only if the  Pause button is pushed. Stop Finalizes the measurement and creates a report. Pause Pauses the automatic measurement. Go Starts the automatic measurement. Next step Moves to the next measurement step. It is enabled only if the  Pause button is pushed.Keyboard shortcuts used while measuring:Next measurement field - secondary mouse button.Previous measurement field - Ctrl + secondary mouse button.Measure air void - Middle mouse button.Delete air void - Ctrl + middle mouse button.Delete air void part (classic method) - Primary mouse button.Delete air void part (CAD system method) - Primary mouse button (left to right).Define air void (classic method) - Ctrl + primary mouse button.Define air void (CAD system method) - Primary mouse button. ",
     id: 1 }, 
   { title: "Options",
     xmlid: "id|CR_CONC_Settingsx",
     content: " This command opens the concrete Settings dialog window containing the following tabs.SpecimenIn this tab, set the number of concrete samples (specimens) to be measured, their dimensions, the length of the traverse lines and their layout over the specimen. When you press the EN 480-11 button, values which are needed to fulfill this Euronorm requirements will be set.Number of specimens Specifies the number of samples to be measured.Length (Y Axis) Specifies the length of the specimen.Width (X Axis) Specifies the width of the specimen.Non-metering margin (top/bottom) Specifies the top and bottom non-metering margins of the specimen.Non-metering margin (left/right) Specifies the left and right non-metering margins of the specimen.Volume paste content Specifies the percentage of the volume paste present in the specimen.Total length of the traverse line Specifies the total length of the traverse line.Line spacing Specifies the spacing between the traverse lines.Number of lines on margins Specifies the number of lines on margins.EN 480-11 Sets all the specimen parameters according to the Euronorm EN 480-11.MeasurementEditing method of the detected air voids, XY stage settings, automatic focusing method, and the result filtering parameters can be adjusted in this tab.Line editing method ClassicLeft mouse button marks the beginning and the end of a line section. This section will be erased.Left mouse button + Ctrl marks the beginning and the end of a new air void. CAD systemLeft mouse button marks the beginning and the end (left to right) of a line section of a detected air void. This section will be erased.Left mouse button marks the beginning and the end (right to left) of a new air void.XY Stage Check Manual XY if you use a manual XY stage.To measure only along the X axis, check Measurement along X.Check User-defined XY scanning start position to set a new scanning starting position based on the current stage coordinates. Click Define... to set the new scanning start position. To move the stage to this position, click Move to....Focusing When scanning a concrete sample, the system focuses repeatedly every n-th frame (n=step). If the concrete sample is well prepared (perfectly flat with constant thickness), you can set the step to 5 or higher. To use auto focus, first Learn autofocus. For manual focusing, check Manual focus.Filtering Cleaning factor [microns] specifies the minimal size of the detected air voids in micrometers. Air voids smaller than that will be omitted. On the contrary, if an empty space between two air voids is smaller than the Filling factor [microns] value, the two air voids will be merged into one. The Cleaning factor is applied first.Classifier Select a classification method of evaluating the binary objects.  Neural Network defines the classes via neutral network algorithm from the defined samples. If Bayes is selected, classes are defined via an algorithm for calculating classes from the defined samples. Approximation of detected air voids with inscribed circles speeds-up the classification.ReportAt the end of each measurement task, a report is created containing the measurement results. The report consists of several files, all placed into the Root directory. Here you can specify the destination file names.Root directory Directory where the report files will be placed.RTF template Path to a predefined .rtf template.XLS template Path to a predefined .xls template.Report name, .rtf Specifies the name of the .rtf file.Report name, .xls Specifies the name of the .xlsn file.Data export, .txt Specifies the name of the .txt file.Histogram name, .bmp Specifies the name of the histogram .bmp image.Store album of air voids Stores an image file containing the album of detected air voids. Each measurement frame is cropped so only a thin stripe containing the traverse line remains. These stripes are stitched together, labeled and saved to the album file. Measurement results can be verified visually using this image.Album name, .bmp Specifies the name of the album .bmp image.LinesGraphic properties of the annotation objects used during the measurement can be set here.Traverse line Color and width of the traverse line can be adjusted here.Chord Color and width of the chord can be adjusted here.Help lines Color and width of the help lines can be adjusted here.Classifier Color of the air voids and paste can be changed here.Small air void highlighting Voids smaller than the specified size can be highlighted with a selected color, width and length. Just check Highlighting voids less than [in microns] and adjust the parameters.OtherDo not show error if the Z drive is not present If the motorized Z drive is not present in the system, an error message appears whenever the stage is being initialized. Here you can suppress this message. ",
     id: 2 }, 
   { title: "Database View within Organizer",
     xmlid: "id|Database.Viewx",
     content: " You can browse the database using the built-in Organizer. Use the    command (or press the button located in the top right corner of the screen) to display the Organizer. Here you can either browse images saved on the hard disk or browse the database. Switch between the two views by the following buttons: Files This button switches the pane to show a directory tree and images from the selected folder (and optionally its sub-folders). Database This button switches the pane to show the database structure and lists images from the currently selected database table.See   for general information about how to use the Organizer.Features Available in the Database View Figure&nbsp;594.&nbsp;The Database View This button displays the detailed information about the selected image. You can switch it off to get additional space to display images.The nearby pull-down menu displays the database connection name and enables the user to switch between active connections.The next pull-down menu enables you to select a database table to be displayed.Operating with Images&nbsp;You can insert images to a database by “drag and drop”. Simply drag the image from a folder and drop it onto the pane, where the database table is opened. ",
     id: 3 }, 
   { title: "Functions, groups and sections",
     xmlid: "id|GA3_Functionsx",
     content: " Click Ctrl + Shift and drag over the nodes to select them. With the nodes selected, use the icons on the main toolbar to make a function, group or section: Make function Complicated chains of nodes can be dismembered and merged into single functions. These functions can be used to simplify the analysis structure or to save / duplicate separated parts of an analysis for another use. After clicking this button a new function named Function is created and the selected nodes are hidden inside as its inner nodes. To decompose a function, reveal the context menu over the function and select  Decompose function. All inner nodes of the function will be restored to their original positions.To edit a function, select   Edit Function from the context menu. GA3 editor is switched to the function editing mode showing only the inner nodes of the edited function. The top input parameters are substituted with inputs of the function. It is possible to change the inner node settings or the structure of the graph inside the function. All changes are reflected in the function inputs and outputs. Use the  Close the function button to end the editing mode.Click on  Build Function GUI in the context menu over a function. Then check which inner nodes will be visible in the parameters window of the function being set. Make group Aggregates nodes below each other within a single group node. To decompose the group click  Decompose group in the context menu. Make section Creates a section (color rectangle) which can be further sized by pulling the bottom right rectangle. More nodes can be added to the section by dragging and dropping. Duplicate Duplicates the selected function, group or section. Delete Deletes the selected function, group or section. ",
     id: 4 }, 
   { title: "Illumination Sequence",
     xmlid: "id|GS_HowTo.IlluminationSequencex",
     content: "            (requires:  )Illumination sequence (IS) module represents a universal interface for camera-to-device triggering. Drag-drop items make it easy to create any complex illumination or stimulation procedures in a matter of minutes. ",
     id: 5 }, 
   { title: "Creating illumination patterns",
     xmlid: "id|GS_HowTo.IlluminationSequence.patternx",
     content: " Drag-drop a laser line into the Illumination row of the Phase table and double-click on it. A new dialog window appears. In the first combo box you can change your illumination, set its power or assign the power value from the device pad. The middle section contains a preview of the defined illumination cycle. Figure&nbsp;922.&nbsp; The settings defining the illumination pattern itself are represented by tabs at the bottom of the dialog window. Three possible illumination modes are available:Follow Exposure  Figure&nbsp;923.&nbsp;This mode uses the exposure signal as a pattern for illumination. Thus, illumination is performed only during exposure. This is indicated by a full color in the preview area in the center of the dialog window and in each cell using this illumination mode. Start by selecting for how many frames will the illumination be repeated (Repeat for).Illumination pulsing can be performed over the defined illumination interval. Simply check Pulsed Illumination [Min Pulse Width ~ 10 msec], specify the Pulse Width, whether its a Single pulse or it contains a Period (gap between pulses). You can also set the Count (number of pulses) or Fill the whole Exposure.Continuous  Figure&nbsp;924.&nbsp;This mode illuminates over the whole camera period (Exposure + Dead). You can set its Duration in Cycle Times or in time units.Illumination pulsing can be enabled by checking the Pulsed Illumination [Min Pulse Width ~ 10 msec] check box. Then you should specify the Pulse Width and Period (time gap between pulses). Now set their Count or use Fill Selected Time to fill the whole illumination interval with the previous settings. After adjusting your illumination, click OK. Elements using the Continuous mode will be indicated by a wavy line.Custom  Figure&nbsp;925.&nbsp;Custom mode is the most complex mode which combines both previously mentioned modes. Start by selecting for how many frames will the illumination be repeated (Repeat for). Choose at which point will the illumination Start/Stop - Exposure Start/End, Dead Time Start/End or Next Exposure Start. The current illumination setting is always displayed in the preview section (Cycle Definition) of the dialog window. Time offset from the Start At point can be set in the Offset edit box.Illumination pulsing uses the same features as the Continuous mode (see above). If you have set the exact Pulse Width and you know the number of pulses you want to use, check Compute period to fill selected time and enter their Count. After adjusting your illumination, click OK. Elements using the Custom mode will be indicated by the “Custom” text.For IS examples, please see  . ",
     id: 6 }, 
   { title: "Ratio Experiment, Calcium Calibration",
     xmlid: "id|GS_HowTo.ratiox",
     content: "        (requires:  )This “how to” expects you to perform a ratio experiment.See also:  .The intended procedure consists of the following portions:Setting up Optical Configurations6D: Time and MultiChannel Set UpCreate/ Save/ Open ROIsTime Measurements Set UpCapturing data setCalcium CalibrationExport to Excel/ Text File Create two optical configurations containing filter settings (use the    command). Call them “340” and “380” (indicating excitation wavelengths). Figure&nbsp;666.&nbsp; If different camera settings (exposure, EM gain, etc) are desired for individual channels, then create microscope + camera settings optical configurations.Active Shutter will most likely be the EPI Shutter.The only difference between these two Optical Configurations may be the filter state of the excitation of 340 and excitation of 380. Run the    command.In the Time tab, adjust the settings according to the image: Figure&nbsp;667.&nbsp;In the Lambda (wavelength) tab, define two channels: Figure&nbsp;668.&nbsp;In the case of 340 and 380, this is a dual excitation experiment. These two channels will share the same emission (510nm). In this case, the component color (pseudocolor) of each of these channels will be the same because NIS-Elements uses the emission wavelength as the default to set the component color. If it is desired to have differing component colors for 340 and 380, then select the drop down list of component colors to change (or both) of the lambdas. At this point, it is recommended (if not completed already) to optimize the correct camera exposures for 340 and 380. In a typical Fura-2 experiment, 380 emission will appear dimmer than 340, an indicator that cellular calcium is bound, not free. When establishing optimal exposure, be cognizant that in an experiment that induces a release of calcium, the emission of 380 will increase. Set exposures to match the dynamic range of the camera and also the full length of the time-lapse and possible biological responses to drug additions.In the Time tab, select Use Ratio and press the Define Ratio button. Figure&nbsp;669.&nbsp;The Ratio Range is a parameter that specifies the Intensity Modulation Display (IMD). Using the Auto range option will automatically scale the IMD based on ratios in the dataset. (Auto range may not be available at the onset of an experiment/ Timelapse because the data for the ratio has not yet been generated.) Use [Ca2+] calibration will be an important item to check at later time after acquisition. There are instances when [Ca2+] calibration can be specified before acquisition. Values from a previously run experiment can be inserted here.Confirm that Perform Time Measurement is checked within the Time tab.Open up Time Measurement dialog by the    command. Press the 1 time loop button in the 6D dialog window.Run   .In the ROI editor, select the Polygon tool.Draw several ROIs and one designated to indicate background. Figure&nbsp;670.&nbsp;Right-click the background ROI (#4) and select Use as Background ROI from the context menu. Figure&nbsp;671.&nbsp;Once a background ROI is specified, it immediately begins to perform a subtraction and it will be noticeable in the IMD display. The background probe icon  on the image window will also become active. This drop-down menu provides more options for how to utilize this background ROI. In this case, “Keep Updating Background Offset” is selected.Save ROIs to a file to be able to load it later. Right-click the ROI button  on the right image toolbar and select Save ROI As. The ROIs will be saved in a common image format (TIFF, BMP, PNG). Open up Time Measurement dialog by the    command (if it has not been already open). Figure&nbsp;672.&nbsp;Highlight (select) all ROIs in order to be displayed in the graph.At this point in the experiment set up, notice what will be graphed during the Timelapse. Define Left and Right axes display.Select the display mode - ROIs mode vs. Channels mode. Graphing  ROIs will graph each cell/ROI individually over time. Graphing  Channels will graph the average intensity/ ratio of each cell/ ROI over time. These are graphing options for the during of the Timelapse, but can be modified after acquisition. Choices made at this point in the experimental set up do not affect the data values or limit graph choices post acquisition. In the 6D dialog window, press the Run Now button. A new image will be captured.You will be able to observe ratio graphs during the acquisition. This is critical to understand if there is a change in response to the solutions added to the specimen to create ratio responses - a change in the ratio of bound and free calcium in the cells. The live graph is an indication of whether or not there was a response (e.g. if the dose was toxic). This is basically a tool to allow the researcher how to plan the course of the experiment. Once the full dataset has been measured through the time measurements dialog, it is now possible to set the calcium calibration- normalizing the data off of the experimental Rmin and Rmax.Select the Define Calcium Calibration  icon.Pick Rmin value from graph - This is also the Fmax (maximum fluorescence intensity of 380. (Generally speaking this is after EGTA is added and near the start of the calibration routine of the ratio experiment before ionomycin is added). Figure&nbsp;673.&nbsp;Pick Rmax value from graph - This is also the Fmin (minimum fluorescence intensity of 380. (Generally speaking this is after ionomycin is added- highest levels of free calcium). Figure&nbsp;674.&nbsp;Click Finish. What becomes available after calcium calibrationDataset has a new channel tab named [Ca2+] with corresponding IMD display.Graph options for Left and Right now include Ca+2 as one of the drop down options. In the Time Measurement window, select the Data to Excel option from the Export pull-down menu.Press the Export button.See  . If MS Excel is not installed on your computer, the only way to export the data is Data to File and Graph to Raster File / Graph to Clipboard ",
     id: 7 }, 
   { title: "Supported cameras",
     xmlid: "id|ISS.supported.camerasx",
     content: " Single camera driversAndor EMCCD (Clara, iXon X3, iXon Ultra 897, ImageEM, Luca R)Andor sCMOS (Neo, Zyla 4.2 and 5.5)Hamamatsu Flash 4.0 (both CL and USB3 interfaces)Nikon A1Nikon DS-Qi2Nikon DS-Ri2Single camera drivers + Dual emission splittersPhotometricsCairnHamamatsuDual camera driversAndor EMCCD and sCMOSHamamatsu Flash 4.0Triple/Quad driversAndor EMCCD ",
     id: 8 }, 
   { title: "Supported illumination devices",
     xmlid: "id|ISS.supported.illuminationx",
     content: " AOTF and LED light sources with NIDAQ interface89 North Heliophor  ( ) Agilent MLC400 ( )Lumencor SPECTRA/AURA ( )Lumen Dynamics X-Cite XLED1 ( )LU-NV (via NIDAQ)NIDAQ Illumination device Piezo Z devices with NIDAQ interfaceNIDAQ Piezo ZPI E-712 DMD devices with NIDAQ interfaceNikon DMD ( )Andor Mosaic 3 ( )Mightex PolygonDual Polygon Other NIDAQ devicesFilter ChangerAnalog OutputCalibrated Analog OutputShutterSwitcherTTL Output ",
     id: 9 }, 
   { title: "Usage Examples",
     xmlid: "id|IS_examplesx",
     content: " Stroboscopic illumination sequence Figure&nbsp;936.&nbsp;Definition  Figure&nbsp;937.&nbsp;Cycle previewPhoto-activation during transition period Figure&nbsp;938.&nbsp;Definition  Figure&nbsp;939.&nbsp;Cycle preview  Figure&nbsp;940.&nbsp;Oscilloscope viewSingle color continuous photo-activation with single camera Figure&nbsp;941.&nbsp;Definition  Figure&nbsp;942.&nbsp;Oscilloscope viewSingle color photo-activation following exposure Figure&nbsp;943.&nbsp;Definition  Figure&nbsp;944.&nbsp;Oscilloscope viewTwo color photoactivation using dual Andor camera Figure&nbsp;945.&nbsp;Definition  Figure&nbsp;946.&nbsp;Oscilloscope viewPiezo Z - Z Offset Figure&nbsp;947.&nbsp;Definition  Figure&nbsp;948.&nbsp;Oscilloscope viewPiezo Z - Z stack, single color Figure&nbsp;949.&nbsp;Definition  Figure&nbsp;950.&nbsp;Oscilloscope viewPattern Illumination Figure&nbsp;951.&nbsp;Definition  Figure&nbsp;952.&nbsp;Live View with the Illumination patternFilter Wheel Figure&nbsp;953.&nbsp;Definition  Figure&nbsp;954.&nbsp;Oscilloscope view ",
     id: 10 }, 
   { title: "Data Panel",
     xmlid: "id|Mod_Metalo.Resultsx",
     content: " The    panel is placed at the bottom of the application screen and is divided into three sections. Figure&nbsp;1242.&nbsp;Data Table  The table includes results of all quantities measured. Only the fields/objects measured by the currently selected method appear in the table.Statistics and Results  The central portion of the window is occupied by statistics on all fields of the currently selected measurement method.Histogram  Optionally, histogram of the displayed data can be displayed. ",
     id: 11 }, 
   { title: "Nodes",
     xmlid: "id|action.referencex",
     content: "               ",
     id: 12 }, 
   { title: "Continue Training",
     xmlid: "id|ai.continue.trainingx",
     content: " Two main options on how to continue training the existing datasets are possible:First, train the AI on dataset A, then:Continue training on dataset B only.AI will slowly forget the dataset A, training should be faster than training from scratch on dataset B.Continue training on merged dataset A+B (image below).AI will be extended to work on both dataset A and dataset B. This is useful if you find out that in some cases your trained AI does not perform well.When using the Continue training on function, the whole AI is being retrained. If dataset A and dataset B are not similar, then this function is not recommended. Figure&nbsp;534.&nbsp;Continue training on merged dataset A+B. [Courtesy of Nikon Imaging Center, RIES at Hokkaido University]The left image (red binary) is with trained only 3 frames, the right image (green binary) is with adding 2 trained images (together 5 frames) after continuous training. After the continuous training we can see improvement in detection, where AI is more sensitive to the cell type then was before continuous training. ",
     id: 13 }, 
   { title: "Cell Motility",
     xmlid: "id|analysis.cell.motilityx",
     content: " (requires:  )This analysis measures cell motility. A simple timelapse-only ND2 file must be open for this command to be enabled. The results of the analysis are automatically loaded into the Tracking panel (see  ).Options Figure&nbsp;552.&nbsp;Recipe Select which recipe (a collection of settings) is used for the task from the pull down menu. You can also Load another recipe you have previously created or get. When you made some changes to the recipe, you can either Save them to the currently selected recipe, or Save As a new recipe.Channel Select which channel is analysed.Color Choose color of the binary layer.Threshold/Bright Spot Detection/Dark Spot Detection For Threshold, use the sliders to define the thresholded objects. You can also manually overwrite the values of the thresholding limits.For more information about Bright Spot Detection / Dark Spot Detection, please see  .Binary operations You can apply some of the binary operations to improve the thresholded binary image.Restrictions You can filter the detected binary objects using the Size or Circularity restriction filters.Run on Select which dimension is used for the analysis.Preview Check to display preview of the result.Run Now Starts the task and displays the    window and the    with the results.Cancel Cancels the task and closes the window. ",
     id: 14 }, 
   { title: "Cell Proliferation",
     xmlid: "id|analysis.cell.proliferationx",
     content: " This analysis measures cell growth in time. A simple timelapse-only ND2 file must be open for this command to be enabled.Options Figure&nbsp;553.&nbsp;Recipe Select which recipe (a collection of settings) is used for the task from the pull down menu. You can also Load another recipe you have previously created or get. When you made some changes to the recipe, you can either Save them to the currently selected recipe, or Save As a new recipe.Border Handling   Include objects touching image borders takes all objects touching the border of your image into the analysis. Exclude objects touching image borders excludes all objects touching the border of your image from the analysis.  Include objects touching top or right border includes all objects touching the top or right border (frame) and excludes all objects touching the bottom or left border (frame) of your image. The frame restricting your cell count area can be adjusted from the outside by entering a numeric value [µm, %] or by dragging the yellow lines in your live image.Channel Select which channel is analysed.Color Choose color of the binary layer.Threshold Use the sliders to define the thresholded objects. You can also overwrite manually the values of thresholding limits.Binary operations You can apply some of the binary operations to improve the thresholded binary image. Use the Preview check box to view results of the current settings in your active image.Restrictions You can filter the detected binary objects using the Size or Circularity restriction filters.Run on Select which dimension is used for the analysis.Preview Check to display preview of the result.Run Now Starts the task and displays the    window and the    with the results.Cancel Cancels the task and closes the window. ",
     id: 15 }, 
   { title: "Cell Count Analysis",
     xmlid: "id|analysis.countx",
     content: " This analysis is designed for counting organic cells. It operates on a single channel and produces one binary mask and a set of predefined features. It has similar functionality as the Object Count in NIS-Elements.Start by choosing your optical configuration, selecting a color that is well visible in the preview image (check the Preview check box) and thresholding. The threshold bar enables selecting low and high intensity thresholds. Only the pixels within this interval will be taken to make objects visualized as a binary mask.Under the main Threshold bar there are four post-processing controls to further tweak the objects (Smooth, Clean, Fill holes, Separate). If the objects are too rough it is a good idea to try some smoothing, but not too high values in order to avoid merging of nearby objects. Then use little bit of Clean to get rid of the smallest objects. Set Fill holes if it makes sense in your sample. And finally play with Morphological separate (higher numbers separate less – produce bigger objects). You can filter the objects based on their Size (EqDia – Equivalent Diameter: calculated from object area as if it was a circle) or Circularity. Then you can check each feature you want to measure. Following features are available: Area, Eq. Diameter, Circularity, Elongation, Length, Width, Max Feret, Min Feret, Mean Intensity, Sum Intensity, Perimeter. For their detailed description please see  . If your object size does not differ, you can Use object standard size. Object count field shows the number of objects detected on the current frame using the current analysis settings. Finally you can choose whether to Save features for each object and confirm the analysis setting by clicking OK. Figure&nbsp;551.&nbsp;Redefinition window for the “Cell count” capture type. Window OptionsThe top buttons which are common to all analysis windows are described in the   chapter.Options concerning Threshold are already described in the   chapter. Include objects touching image borders This function takes all objects touching the border of your image into the analysis. Exclude objects touching image borders This function excludes all objects touching the border of your image from the analysis. Include objects touching top or right border This function includes all objects touching the top or right border and excludes all objects touching the bottom or left border of the measurement frame. The frame size can be adjusted by entering a numeric value [µm, %] which defines width of the outside margin. Single point threshold tool (requires:  )This tool picks threshold from the image based on single pixels. Click in the image and define one or more points of interest. Then use the mouse right-click to confirm the selection. Threshold is calculated automatically based on your selection. 3 points circle threshold tool (requires:  )Click inside the image to define typical object areas - in this case threshold from the radius of 3 pixels is used. The system detects similar parts of the image and highlights the resulting thresholded area by a selected color. 6 points circle threshold tool (requires:  )This tool is the same as the previous one besides using 6 pixel radius.Object features check boxes Select features to be measured.Use object standard size In some circumstance it is not possible to consistently threshold individual objects. If it is priori known that the object size does not vary significantly, it is possible to use the Standard object size (EqDia) feature to determine consistent count. The count is then calculated as a ratio of total area under mask and the Standard object area (calculated from its size).Save features for each object If this check box is checked, features are saved for each object detected by the Cell Count analysis.After defining the Cell Count and clicking OK, you can use parameters resulting from this analysis to define an expression. The parameters are then available in the Define Expression window under the “name of your Capture task” / Analysis / “name of your Cell Count”.Cell Count analysis generates one binary mask accessible after opening acquired images. Following set of features is always saved into the database:NAMETYPEDESCRIPTIONCountFieldObject countAreaFieldArea of all objects under the maskAreaFractionFieldRatio of Area to the whole Field AreaObjectAreaObjectIndividual Object AreaObjectEqDiaObjectObjectEqDia Object Individual Object EqDiaObjectMeanIntensityObjectIndividual Object Mean Pixel IntensityObjectSumIntensityObjectIndividual Object Sum Pixel IntensityAll the object features (ObjectArea, ObjectEqDia, ObjectMeanIntensity and ObjectSumIntensity) produce following aggregated statistics: Mean, Min, Max, Sum, StDev (as member variables). In the expression they appear this way:Job.CaptureName.Analysis.CellCountName.FieldFeatureJob.CaptureName.Analysis.CellCountName.ObjectFeature.MeanJob.CaptureName.Analysis.CellCountName.ObjectFeature.MinJob.CaptureName.Analysis.CellCountName.ObjectFeature.MaxJob.CaptureName.Analysis.CellCountName.ObjectFeature.SumJob.CaptureName.Analysis.CellCountName.ObjectFeature.StDevWhere CaptureName is the name of the capture task, CellCountName is the name given to the CellCount analysis, FieldFeature is Count, Area or AreaFraction and ObjectFeature is ObjectArea, ObjectEqDia, ObjectMeanIntensity, ObjectSumIntensity.See also  . ",
     id: 16 }, 
   { title: "Analysis Explorer",
     xmlid: "id|analysis.explorerx",
     content: "      The   Analysis Explorer  control panel enables the user to manage and run analysis recipes. Choose    to display it.If Assay Launcher module was checked in Modules during NIS-Elements installation, three predefined GA3 recipes for the Nikon Ti2 and Nikon Ji microscopes become available. Also a shortcut to   is created in the shortcuts section of the    layout. For editing and using GA3 recipes, we suggest duplicating the GA3 recipe rather than overwriting it.First button of the control panel   Create New is a pull-down menu with all available analyses. Once you define your analysis and save it, a new analysis definition is created in the explorer. Each time the analysis is run, a new analysis record is added under the analysis definition name. It is linked to the image file to which the analysis was applied. Figure&nbsp;549.&nbsp; Dialog Window Options Create New Selects and defines a new analysis which can be directly executed on the current image or saved for later use. Herein, analyses can also be imported from an analysis file (  Import Analysis from File...) or from Jobs (  Import Recipe from JOBS). Edit Opens the analysis dialog window enabling to edit the analysis definition. Run Executes the selected analysis definition on the currently opened image. Batch Opens the Batch Analysis  window enabling to execute the selected analysis definition on multiple files (see:  ). Delete Deletes the selected analysis definition/analysis record. Context Menu over the Analysis definition Run Runs the selected analysis definition on the currently opened image. Edit Opens the analysis dialog window enabling to alter any parameters and overwrite the selected analysis definition with these new settings. Rename Renames the analysis definition. Duplicate Duplicates the selected analysis definition. Export Exports the current analysis definition into a file. Delete Deletes the selected analysis definition. Context Menu over multiple selected Analysis definitions Export Selected Items Exports each selected analysis definition into a file. Delete Selected Items Deletes the selected analysis definitions. Context Menu over the Analysis record Open File Opens the image linked to the selected analysis record. Double-clicking on the analysis record does the same. Open Containing Folder Opens the folder containing the image linked to the selected analysis record. Revert to this definition Uses the analysis definition of the selected analysis record to overwrite the parental analysis definition. Make new Analysis from this definition Creates a new analysis definition from the selected analysis record. Delete Deletes the selected analysis record. Creating an analysis Open an image to be analyzed.Select the analysis from the   Create new pull-down menu. The analysis definition window appears.Adjust the analysis settings - live preview of the segmentation is displayed in the image so you can play with the segmentation parameters. Save it using the   Save As button or click Run Now, enter its name with a description and confirm by clicking OK. Now the named analysis definition is listed in the Analysis Explorer and is ready to be run on other images.Running analysis on the current image Open an image.Select the analysis name within Analysis Explorer.Click the   Run button. The analysis will be applied to the current image.Batch analysis Select the analysis name within Analysis Explorer.Click the   Batch... button. A dialog window appears.In the Analysis Recipes portion of the window select the level of user interaction:Run the batch without user interaction The analysis will be applied to all images in the batch “as it is”.Show definition for each image After opening each image in the batch, the analysis definition window appears allowing you to adjust the settings for each image separately. This per image analysis adjustments do not influence the original analysis definition.Show definition once The definition window appears only once after opening the first image of the batch. Analysis adjustments made here are applied to all subsequent images in the batch and do not influence the original analysis definition.Select Files to be processed . Browse for the folder containing images to be analyzed and adjust the selection of files to be processed. Click Run.When the batch is done Close the progress dialog.Reviewing measurement results All measurement results of the last used analysis are saved to the global NIS-Elements results table which is opened after the analysis/batch is finished. Run the    command to display the appropriate control panel.  Each run or batch run overwrites the    or    panel with new results. Particular settings of your analysis may be saved as a recipe so that you can reuse the settings anytime in future. Moreover, each run of the analysis recipe saves the settings to the database so you can create a recipe out of the analysis run. Here are the options:Export to a file Right-click the analysis name and select  Export command. Browse for the location to save the analysis definition to and click Save. The file will be saved with an extension appropriate for the analysis type (such as *.cellcount)Import from a file Click the   Create new button and select  Import Recipe.... Locate the file containing the analysis definition (e.g.: “c:\\recipes\\definition.cellcount”) and click Open.Import from JOBS (requires:  )Click the   Create new button and select  Import Recipe from JOBS. A list of recipes saved from the JOB definition window appears (see  ). Select one and click OK.Applying local changes to a definition Local changes (e.g. different threshold value) may have been applied to each run of one analysis. You can replace the original analysis definition by one of these locally changed. Just right-click the analysis run and select  Revert to this definition.Editing, Renaming or Duplicating analysis definition All these actions are accessible from a context menu which appears when you right-click an existing analysis name. Once the image folder is selected, all contained images become listed in the table. By default, only images with the value Valid in the Status field are selected for processing. Check status of the other images and decide whether to include them in the batch as well. Figure&nbsp;550.&nbsp; Image Statuses: Valid The file is perfectly compatible with analysis definition. Not Matching The file is not perfectly compatible with analysis definition but it may be converted automatically. This is typically a single-channel image where the channel name does not match the name used in the analysis definition or calibrations do not match (uncalibrated definition used on calibrated image and vice versa). Conversion Dialog The file is not compatible with analysis definition, user interaction is needed in order to assign image channels to the channels used in the analysis definition. Typically channels in the image do not match channels in the analysis definition. Invalid The file cannot be processed. Typically, it has less channels then required by the analysis definition.  File list is arranged alphabetically by the file type in the following order from top to bottom: Valid, Not Matching, Conversion Dialog, Invalid. Other OptionsFilter Standard filter based on filename extensions. Only images matching the selected extension will be listed.Prefix Another filter based on filename prefix. If you type e.g. “i” to this field, only filenames beginning with the letter “i” will be displayed.Include All Subfolders Select this option to list images regardless of the folder structure. All images of the above-defined folder will be displayed.Select Only Valid Select only images having the status  Valid.Deselect All, Select All Select or remove selection of all images in the list.Do not modify original file If checked, only measurement is done without influencing the analyzed image and no binaries are saved to the image. Analyses which do not produce Automated Measurement Results such as Cell Motility always modify the original file (otherwise there would be no results) and therefore the check box is hidden for them.Hide Not Matching Display only images having the status  Valid. ",
     id: 17 }, 
   { title: "General Analysis",
     xmlid: "id|analysis.generalx",
     content: "       (requires:  ) With General Analysis, it is possible to make multiple binary masks on a single channel as well as to combine multiple masks by means of generic expression with powerful operators. It is also possible to calculate custom features. High interactivity enables viewing your result at any time. Check boxes enable viewing just the desired operations or analysis steps. Complicated sequences can be created in few minutes - e.g. color image processing, segmentation, binary image processing, object filtration and measurement, etc. Binary masks can also be mixed using a general expression. Any binary mask can act as a ROI (Region Of Interest) inside which measurement can be done (organelles, markers in cells, etc.).General Analysis exists in two forms - as a part of the Jobs module local option (results go to the Jobs database and are automatically saved as Job Run) and as a local option of NIS-Elements available from the Image menu (results are shown in the Automated Measurement Results tab and are automatically deleted after closing unless they are saved). Except small details, the graphical user interface is almost the same in both forms.See also:  .  Any changes made to the color image are temporary. All operations (thresholding, processing, measurements, calculations, etc.) are made on the original (unaffected) image data. The General Analysis dialog setting is saved as a Recipe (see:   ). Analysis results (for General Analysis from the Image menu) must be saved otherwise they are deleted after closing the results window. Results from General Analysis performed by Jobs are automatically saved in single Job Run.Top Toolbar Options&nbsp;The top buttons for General Analysis found in the Image menu are described in the    section whereas General Analysis for Jobs is described in the   chapter.Analysis Palette&nbsp;Info tab  Figure&nbsp;555.&nbsp;Info tabThis tab enables editing the General Analysis Description, viewing Channel Description and adding notes to each channel (3rd column in the bottom table). OK applies the current General Analysis setting and quits the window. Cancel closes the General Analysis window without using the current settings. Preview displays changes done in General Analysis inside the Preview window. These three last features can be found at the bottom of each general analysis tab. Add Channel tab  Figure&nbsp;556.&nbsp;Add Channel tabAdd and name your new channel tab defined on a specified color channel. These two features can later be changed at the top of each channel tab, where you can also set the channel's color label. The  View Binary icon opens the Binary Layers panel showing all binary layers available.   Move Layer Up/Move Layer Down icons moves the currently selected channel one step up / down in the Binary Layers panel. After defining parameters it may produce a binary mask as a result of channel segmentation and processing. When the analysis is executed it produces measurement on the mask (if made) and selected channel.If the    Wizard button is clicked, the current Channel is marked with a wizard symbol and Thresholding parameters will be shown in the wizard area after the job is executed. Custom description to the threshold linked with its channel can be entered after clicking the ... button. The number of channels is limited to 40. However this number may not always be achieved. It depends on the maximal allowed value of the User Objects in the system process. This value is set to 10000 by default and can be changed in the registry key:[HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\\Windows\\USERProcessHandleQuota] Add Combined tab  Figure&nbsp;557.&nbsp;Add Combined tabCombined tab is used to make binary images as a combination of existing (tabs above the current one) binaries (e.g. union, intersection of previously created masks). You can use the Binary Operations buttons to make an Expression which defines how existing binary masks are combined. The first row of buttons contains operands – the binary masks (from the tabs to left of the current tab). Below are the operators: AND – Intersection, OR – Union, NOT – Complement, SUB – Subtraction, XOR – Exclusive OR, HAVING and parentheses. AND, OR, SUB, XOR and HAVING are all binary operators: the are placed between operands. NOT is an unary operator placed before one operand. Parentheses should be used to dictate the order of operations. After defining the expression you can choose Binary Processing and Features to be calculated. Add ROI tab  Figure&nbsp;558.&nbsp;Add ROI tabThis tab measures features inside a selected binary. Choose the binary from the ROI combo box and select one of the methods handling objects touching ROI borders. Then add desired measured features below. You can also measure distances of objects to different objects and save these data together with the analysis results. Add Calculations tab  Figure&nbsp;559.&nbsp;Calculations tabCalculations&nbsp;Adds a new tab called Calculations. These functions enable mathematical operations on measured data. By adding a new calculation and clicking the ... button you enter the Expression Definition window where you can write your expression using supported functions and operators:FunctionDescriptionabs()absolute valueacos()arc cosineacosh()arc hyperbolic cosineasin()arc sinasinh()arc hyperbolic sinatan()arc tangentatanh()arc hyperbolic tangentavg()averagecos()cosinecosh()hyperbolic cosineexp()exponential powerif(;;)condition ifln()natural loglog()logarithmlog10()common logarithmlog2()binary logarithmmax()maximummin()minimumrint()round to integral valuesign()signsqrt()square rootsum()sumtan()tangenttanh()hyperbolic tangent OperatorDescription+addition-subtraction*multiplication/division^square&lt;less than&lt;=less than or equal to&gt;greater than&gt;=greater than or equal to==equal to&amp;&amp;logical AND||logical OR!=not equal toFor more information about writing expressions, please see  .Populate pointsets with objects coordinates&nbsp;Populate pointsets with objects coordinates finds centers of objects in a selected binary and fills your point set with their coordinates. To use this function, your job definition has to contain the    task. Be sure you set the Action correctly. Append should be used in cases where the point set is firstly filled with all points and then more operations are preformed on this point set. Replace is used when a point position is found and the operation is performed immediately on it. Then it is overwritten by the next xy position where measurement is performed. For better understanding this function please see our two use cases:  ,  .Define regions&nbsp;This function automatically generates regions if your job definition contains the    task. Choose your Region list, specify on which layer the regions will be detected, choose between Append / Replace (described above) and select the Region Division (Per Frame / Per Object).Analysis steps&nbsp; Remove Selected Tab Deletes the currently selected tab.  Move Up / Down Using these arrows you can move your tabs vertically in their inventory.Main area options&nbsp;Preprocessing Here you can choose one of the Preprocessing methods: Remove Average Background, Remove Dark Background, Remove Light Background, Subtract Background Using Contrast, Autocontrast, Local Contrast, General Convolution, Detect DIC Objects, Detect Edges, Detect Peaks, Detect Regional Maxima, Detect Regional Minima, Detect Valleys, Gabor Edge Detection, Gradient Morpho, Linear Close, Linear Dilate, Linear Erode, Linear Open, Close, Open, Low Pass Filter, Advanced Denoising, Fast Denoising, Median and Smooth. Kernel By clicking the kernel icon you can switch between different kernels. Each kernel icon represents a pixel neighborhood method, that is assigned to the preprocessing method selected. Every pixel of the image is then evaluated according to the selected kernel which influences the resulting image processed by the selected preprocessing method. Try to use different kernels to find out which one brings the best image results for your current capture setup.Count Adjusts the intensity of the preprocessing method selected.Threshold The threshold bar enables selecting low and high intensity thresholds. Only the pixels within this interval will be taken to make objects visualized as a binary mask. Threshold is optional in General Analysis. If it is skipped, only intensity features on the whole field of view are available. If the threshold is performed, it may be chosen not to save the binary image which is useful for temporary binary masks.More information about thresholding can be found here:  .Bright/Dark Spot Detection Spot Detection uses mainly circular objects with same intensity values as a threshold.For more information about setting up the Spot Detection parameters, please see:  .Tight Borders Detection For information about this detection, please see   .Smooth / Clean / Fill holes / Separate These tools function as filters. Use them to improve your image quality before processing. Use Smooth (to smooth rough edges) / Clean (removes small objects) / Fill Holes / Separate to get the objects. Reset This button resets the settings.Size Position the sliders to restrict the area of the sizes of detected objects.Circularity Position the sliders to restrict objects in your image with desired circularity.Save Binary Check if you want to save your binary layer together with your captured images.Binary processing Binary processing further alters the binary masks. The choice of operations is same as in NIS-Elements. You can choose some binary processing methods, set their kernel and intensity (Count).  Figure&nbsp;560.&nbsp;Feature When the Binary mask is fine-tuned it is time to set the features to be measured and saved into the database.See  . If you check Preview you can see the results of a currently defined analysis. Insert analysis infoYou start with an empty dialog with the Info tab selected. Here you can insert description of the analysis.Add relevant color channels to the analysisDecide on which image channels the segmentation will be performed. Add these channels to the Analysis steps tab by the  Add Channel button.Define analysis of each channelAdjust the preprocessing, segmentation, binary processing parameters and measurement features for the added channels. The resulting binary layers are displayed in the current image window. You can turn each layer on/off by the check box in front of the channel name.If preprocessing has been applied, you can always display the original image by selecting the View original check box. Any changes made to the color image are temporary (only for Thresholding and Spot Detection) and all measurements are made on the original (unaffected) image data as well as saving.Combine binary layers (optional)You can perform logical operations processing and measurements on multiple binary layers. To do so, create a “combined” channel by clicking the  Add Combined Layer button and specify the parameters.Define ROIBinary mask of one channel can be used as a ROI to restrict measurement on another channel. If this is the case, click the Add ROI button. In th ROI channel, again, specify the measurement features.Define calculationsClick the  Add Calculations button to add the calculation step in the analysis. In this step you can define any mathematical expression using the features measured in previous steps of the analysis.Run the analysisClick the Run Now button to apply the analysis to the current image. A new record will be appended to the list of performed analyses in the Analysis Explorer. The measured results are then accessible in the    window. The possibilities of the General Analysis are best shown on a real example. In this case we are trying to find out how much of a protein gephyrin (in red) is present in positively labeled globular bushy cells (in green) using General analysis in NIS-Elements. Figure&nbsp;561.&nbsp;The green channel “Alexa 488” shows globular bushy cells (GBC) in an anteroventral cochlear nucleus (AVCN) part of a rat brain stem. The red channel “Cy3” shows the protein gephyrin.We start by opening the General Analysis dialog window (  ) and adding the Alexa 488 as a new layer. This can be done clicking the  Add Channel icon. We enhance the image using Local Contrast and then we define the threshold values to restrict our cells from the background and other objects. Just for better visualisation we also added Count as a Feature to know, how many objects are detected using the current settings. Figure&nbsp;562.&nbsp;Parameters of the first channel.  Figure&nbsp;563.&nbsp;Resulting preview.Now we can add the second channel and do thresholding as we did with the first one. Figure&nbsp;564.&nbsp;Thresholding the second channel.  Figure&nbsp;565.&nbsp;Resulting preview.To find out which cells contain proteins we will combine both channels clicking the  Add Combined layer icon and defining the expression as follows: Alexa_488 HAVING Cy3. This way we can create ROIs only with cells containing proteins. Figure&nbsp;566.&nbsp;Combined tab settings.  Figure&nbsp;567.&nbsp;Resulting ROIs.We will now use the ROIs from the previous step for the final protein count. We click the  Add ROI Mask tool and select Inside which we want to measure (Combined). Now we define which features to measure (Count of Cy3 channel in our case). When done we click the Run Now button and view the results. Figure&nbsp;568.&nbsp;ROI Mask measurement.  Figure&nbsp;569.&nbsp;Final results.  Figure&nbsp;570.&nbsp;Result visualisation.  Figure&nbsp;571.&nbsp;Scheme: GA creates global point set The slide is scanned at 20x magnification.General Analysis is used to detect objects and create a new point set with only the objects of interest (their centers).The new point set is scanned at 40x magnification.Such a procedure can be carried out using the following tasks.Scan area Figure&nbsp;572.&nbsp;We are using a 75x25 mm slide with a labeled sticker. We will scan an area of 5x5 mm. The slide must be aligned before the scan begins.The Detection and the Capturing Point Sets Figure&nbsp;573.&nbsp;There are two point sets in the job. The first point set is to be used with the low magnification objective to detect cell nuclei. The other is an empty point set which will be filled with points based on the General Analysis results.Capture Definitions Figure&nbsp;574.&nbsp;The first capture definition is used for detection of cell nuclei while the second definition is used to capture two-channel images (nuclei and mitochondria).Detection Point Loop Figure&nbsp;575.&nbsp;In the detection point loop, general analysis task is defined, containing two channels (operations): Figure&nbsp;576.&nbsp;General Analysis Window Segmentation of the DAPI channel - if an object is detected, its center is appended to the new point set. Figure&nbsp;577.&nbsp;Settings of the Threshold_DAPI channelNew Point Set definition - defines where to store the detected points and that we want to append (not replace) each point. Figure&nbsp;578.&nbsp;Settings of the Calculations channel - the point set definitionCapturing Point Loop Figure&nbsp;579.&nbsp;The point loop responsible for high magnification image capturingThe resulting job should look like this: Figure&nbsp;580.&nbsp;Job DefinitionOnce the job is started, a progress window appears: Figure&nbsp;581.&nbsp;Low resolution capture on DAPI, detection of objects  Figure&nbsp;582.&nbsp;High resolution capture - both image channels  Figure&nbsp;583.&nbsp;Job Results  Figure&nbsp;584.&nbsp;Scheme: GA creates local point setIn this example, unlike in the previous one ( ), if General Analysis detects one or more objects, a local point set is created and scanned immediately (see  ). Such setup can be used to acquire for example live processes. All time-consuming actions such as objective changing shall be removed in such cases: Figure&nbsp;585.&nbsp;Tasks arrangementIf you compare this job with the previous one, the biggest difference is in the placement of the Detected_Points point loop which is placed INSIDE of the Detect_objects loop. Furthermore, a time loop has been added.Detection Point LoopIn the detection point loop, general analysis is defined, containing two channels (operations): Figure&nbsp;586.&nbsp;General Analysis Window Segmentation - if an object is detected, its center is appended to the local point set.Local Point Set definition - defines where to store the detected points and that we want to run in “replace” mode. Figure&nbsp;587.&nbsp;GA: Settings of the Calculation channel ",
     id: 18 }, 
   { title: "Image Intensity Analysis",
     xmlid: "id|analysis.intensityx",
     content: " The Image Intensity analysis evaluates the intensity of the whole Field of View. It is purposefully simple. The example of usage for this analysis is when deciding whether there is a signal in the field or not - i.e. if you want just to decide if to capture (or not to) the whole well. If the field is the whole well, there are some options to restrict the well outline (rectangular or circular) and center portion if necessary. It is also possible to restrict the range of intensities. Image Intensity analysis window enables defining the Image Intensity as shown on the following picture. If you click the first Preview icon you can see your changes directly on the live image.This analysis evaluates the image data intensity. It is purposefully simple. Image Intensity is useful if you want just to decide if to capture (or not to) the whole well. You can evaluate the whole image, circular / rectangular region of the image or the thresholded portion. Figure&nbsp;588.&nbsp; The top buttons which are common to all analysis windows are described in the   chapter.Analysis Name Name your image intensity analysis.Run Specify in which phase will the analysis be executed (During Acquisition or After Acquisition).Full frame Calculates full frame of the live image.Channel Select a optical configuration based on its name.Restricted Check if you want to restrict the image area.Exclude border Excludes the live image from the outside. Circular / Square shapes are available.Exclude center Excludes the live image from the center. Circular / Square shapes are available.Threshold Set the sliders to distinguish objects on which the intensity will be measured.Intensity Measure Shows currently measured values from the live image.Preview Check if you want to see a live preview of your current Image Intensity settings.After defining the Image Intensity and clicking OK, you can use parameters resulting from this analysis to define an expression. The parameters are then available in the Define Expression window under the “name of your Capture task” / Analysis / “name of your Image Intensity”. ",
     id: 19 }, 
   { title: "Live/Dead Analysis",
     xmlid: "id|analysis.livedeadx",
     content: " This analysis is used for counting live and dead elements in one sample. Two optical configuration channels are required - one channel is used for counting Live elements and one for Dead elements. Figure&nbsp;589.&nbsp;Live/Dead analysis dialog window Window optionsThe top buttons which are common to all analysis windows are described in the   chapter.Analysis Name Name of this analysis.Run Select when will the analysis be performed - During Acquisition or After Acquisition. Include objects touching image borders This function takes all objects touching the border of your image into the analysis. Exclude objects touching image borders This function excludes all objects touching the border of your image from the analysis.Channel Select a channel used for counting Live/Dead elements.Auto Detect Click this button to go into image thresholding mode to mark an object whose intensity values will be used for identifying all other objects present in the image.Threshold The threshold bar enables selecting low and high intensity thresholds. Only the pixels within this interval will be taken to make objects visualized as a binary mask.More information about thresholding can be found here:  .Smooth / Clean / Fill holes / Separate These tools function as filters. Use them to improve your image quality before processing. Use Smooth (to smooth rough edges) / Clean (removes small objects) / Fill Holes / Separate to get the objects.Size Position the sliders to restrict the area of the sizes of detected objects.Circularity Position the sliders to restrict objects in your image with desired circularity.Preview If checked, preview of the settings is shown on the Live image. ",
     id: 20 }, 
   { title: "Bio Analysis Modules",
     xmlid: "id|analysis.modulesx",
     content: "                     ",
     id: 21 }, 
   { title: "Sperm Motility",
     xmlid: "id|analysis.sperm.motilityx",
     content: " This analysis tracks sperms and classifies them by speed. Figure&nbsp;590.&nbsp;OptionsAnalysis Name Name of your analysis.Threshold The threshold bar enables selecting low and high intensity thresholds. Only the pixels within this interval will be taken to make objects visualized as a binary mask.More information about thresholding can be found here:  .Smooth / Clean / Fill holes / Separate These tools function as filters. Use them to improve your image quality before processing. Use Smooth (to smooth rough edges) / Clean (removes small objects) / Fill Holes / Separate to get the objects.Size (px) Position the sliders to restrict the area of the sizes of the detected sperms.Circularity Position the sliders to restrict sperms in your image with desired circularity.Elongation Please see   .Max Object Speed Select if a speed limit is required. Only trajectories of sperms moving within the speed limit will be considered. Use this option when you are sure the sperm cannot move faster than the set limit.StDev Multiplication Factor Please see Standard Deviation Multiplication Factor  ( ).Join tracks up to “n” frames apart Tracks which are not directly connected in time can be joined together after specifying the frame gap.VAP Averaging Specifies the average path velocity parameter.Remove track shorter than n segments Tracks shorter than a specified number of segments can be removed from the analysis.Preview If checked, preview of the settings is shown on the Live image. ",
     id: 22 }, 
   { title: "Single Particle Tracking",
     xmlid: "id|analysis.sptx",
     content: " and (requires:  )Single Particle Tracking (SPT) module combines image analysis and trajectory statistics to understand the motion of functional molecules in the cytoplasm or in the nucleus. It tracks single fluorescent molecules at the molecule level and analyzes parameters of these tracks, such as the mean square distance value (MSD).Start by opening the image for tracking (containing only t-dimension). To open the SPT analysis, run    and click on the first combo box to select   Single Particle Tracking and then click on its button. Figure&nbsp;1427.&nbsp;Dialog window options Save Saves the new SPT settings into the Analysis Explorer or overwrites the previous settings of already saved analyses. Save As Enables the user to enter the name of the analysis and enter a text description. Clear Resets the settings of the currently opened SPT dialog window. Save Recipe Saves a recipe of the currently opened SPT settings into the database. Load Recipe Opens the Load Recipe dialog window enabling to select and load previously saved recipes. Export Exports the current SPT settings into a Recipe Settings File (*.spt). Import Imports the Recipe Settings File (*.spt).Analysis Name Name of your SPT.Maximum Particle Displacement This value represents movement of molecules between two consecutive frames. Suggested value is shown in the Cumulative Displacement Histogram.Add MaxIP Channel Removes the mean intensity value from all frames to better visualize areas where particles may be present.Advanced Settings Reveals advanced setting options.Percentile Threshold Thresholds the specified percentage value of the brightest pixels on each frame.Motion Blur Size Single molecules are beyond optical microscopy visibility, therefore their movement is observed as a motion blur. Size of this blur can be specified to restrict the size of the molecules.Remove objects with SNR lower than Removes objects with signal-to-noise ratio lower than a specified value.Tracking Results Preview tab Displays preview result values of the first 500, middle 500 and last 500 frames.Displacement tab Click Calculate Histogram to view information in the Cumulative Displacement Histogram. Its legend describes the color lines. Vertical green line suggests the best numeric value for the Maximum Particle Displacement. Protein Halflife represents a time interval for how long the molecule emits light and therefore is detectable. Enter this value based on your tracked sample.Run Now Executes the SPT process.Close Closes the SPT dialog window.Once you set up the SPT dialog window and execute it clicking on Run Now, detected particles are shown in the image and tracking results are shown in the Tracking tab (  ). Visualization of detected binaries can be tuned inside the Display tab inside Advanced Tracking ( ). Next to the source ND2 file, two new .CSV files are created after the tracking is finished. First file contains information about each detected object (X[µm], Y[µm], FRAME NUMBER, TRACK NUMBER). Second file contains the overall result information (FILE NAME, NUMBER OF DETECTIONS, RELATIVE DENSITY [1/µm/s], NUMBER OF TRACKS, NUMBER OF CUT TRACES, MEAN SNR, MSD1 [µm], MSD2 [µm], MSD3 [µm], ... ). ",
     id: 23 }, 
   { title: "Wound Healing",
     xmlid: "id|analysis.wound.healingx",
     content: " This analysis measures changes of cell distribution in time. A simple timelapse-only ND2 file must be open for this command to be enabled.Options Figure&nbsp;554.&nbsp;Recipe Select which recipe (a collection of settings) is used for the task from the pull down menu. You can also Load another recipe you have previously created. When you make some changes to the recipe, you can either Save them to the currently selected recipe or Save As a new recipe.Close Holes - Count This command fills holes in detected objects. Specify how many times the command will be run.Delete Small Objects This command deletes the smallest objects. Set how many times the command will be run. Use the Preview button to check your actual image settings.Do Postprocessing Check if you want to use postprocessing.Run on Select which frames are used for the analysis. You can select All frames or just a frame Selection.Preview Check this box to view results of the current settings in the active image.Run Now Starts the task and displays the    window and the    with the results.Cancel Cancels the task and closes the window. ",
     id: 24 }, 
   { title: "Concrete",
     xmlid: "id|app.concretex",
     content: "       ",
     id: 25 }, 
   { title: "Deconvolution",
     xmlid: "id|app.deconvolutionx",
     content: "                   (requires:  ) (requires:  ) ",
     id: 26 }, 
   { title: "Extended Depth of Focus",
     xmlid: "id|app.edfx",
     content: "     (requires:  )EDF allows you to combine an existing Z-stack of images into one focused image by picking the focused regions from each frame and the pieces together. ",
     id: 27 }, 
   { title: "Filter Particle Analysis",
     xmlid: "id|app.filtersx",
     content: "                    (requires:  ) ",
     id: 28 }, 
   { title: "FRET",
     xmlid: "id|app.fretx",
     content: "                        (requires:  ) ",
     id: 29 }, 
   { title: "General Analysis 3",
     xmlid: "id|app.ga3x",
     content: "                      ",
     id: 30 }, 
   { title: "HDR",
     xmlid: "id|app.hdrx",
     content: "   (requires:  ) ",
     id: 31 }, 
   { title: "Layer Thickness Measurement",
     xmlid: "id|app.layersx",
     content: "   (requires:  ) Figure&nbsp;1219.&nbsp;Layer Thickness Measurement tab in the Linear mode. ",
     id: 32 }, 
   { title: "Metalo - Cast Iron Analysis",
     xmlid: "id|app.metalo.castironx",
     content: "         (requires:  ) ",
     id: 33 }, 
   { title: "Metalo - Grain Size Analysis",
     xmlid: "id|app.metalo.grainsizex",
     content: "                                            (requires:  ) ",
     id: 34 }, 
   { title: "N-STORM",
     xmlid: "id|app.nstormx",
     content: "               ",
     id: 35 }, 
   { title: "Object Classifier",
     xmlid: "id|app.objectclassifierx",
     content: "  (requires:  )Objects can be classified according to selected features using this control window. Run the    command to open it: Figure&nbsp;1402.&nbsp; ",
     id: 36 }, 
   { title: "Smart Experiment",
     xmlid: "id|app.sex",
     content: "      (requires:  ) ",
     id: 37 }, 
   { title: "Measurement Sequencer",
     xmlid: "id|app.sequencerx",
     content: "        (requires:  )(requires:  ) ",
     id: 38 }, 
   { title: "Weld Measurement",
     xmlid: "id|app.weldsx",
     content: "  (requires:  )The    module can be used for analysing images of the following types of welds:Fillet WeldLap WeldSpot Weld Display Measurement SequencerRun the    command to display the panel for running measurement sequences.Select a Measurement SequenceThere will be different measurement sequences in the list for different weld shapes (Lap, Fillet, etc.). The default templates are displayed in red color. Double-click the template name - e.g. Fillet Weld. The Measurement Sequencer - Run panel appears.Select parametersSelect check boxes next to the parameter names for those parameters to be measured.Start measurementMake sure the   Show Drawing button is pressed so the scheme of the current weld shape containing dimensions is displayed. Open the image for measurement. Click the   Measure button.Draw the required lines and dimensions to the imageDimension lines appear in the drawing to give you a hint of how to measure the current parameter correctly. A textual hint may be displayed below the drawing. Click inside the image to measure the first parameter. When finished, the sequence will proceed to the second parameter, etc.Save resultsAfter the measurement is finished, the   Export to Report gets enabled. Click it to save the result.Proceed to another imageTo measure another image using the same settings, open it and click the   Restart button. ",
     id: 39 }, 
   { title: "Application of the Standards in the Planimetric Method",
     xmlid: "id|application.of.standards.planimetricx",
     content: " The planimetric method in “NIS-Elements Grain Size Module” (NGS) measures Grain Size values of a single phase material structure in accordance with the following standards as described:ISO 643 NGS applies the paragraph C.3.3 that is an exact copy of the ASTM E112 – 13 procedure described below.JIS-G-0551 (2013) NGS applies the paragraph B.3.3 „Count Method“ that is an exact copy of the ASTM E112 – 13 procedure described below.ASTM E112-13 NGS applies the paragraphs 11.1, 11.2 and 11.4According to 11.1 for the planimetric procedure, NGS generates a circle of known area while the optical configuration should be set to reach at least 50 grains in the field and count the number of grains within this area. NGS does not block measurement if the amount of grains is lower. NGS keeps the value of the area as a MaskArea in [mm2].According to 11.2 NGS counts separatelly the number of the grains completely inside within the known area and the number of grains intersected by the circumference of the area.According to 11.4 NGS calculates:Number of equivalent whole grains N = 1 x (num fully inside) + 0.5 x (num intersected)Actual number per square mm NA = N / MaskAreaGrain Size G = (3.321928 log10 NA ) – 2.954NGS displays Grain Size G as the nearest norm value (0, 0.5, 1, 1.5…14.0)E1382-97 (2015) NGS applies the paragraphs 13.9, 14.2, 14.3According to 13.9.1 NGS detects grains inside a measurement field, deletes all grains intersecting the test area border and measures the total area of all of the grains Agi within a field and counts the number of grains Ni.According to 13.9.2 NGS calculatesThe average area of the grains in the field Ai = Agi / NiAccording to 13.9.3 This measurement should be repeated for n fields, at least 5x (NGS does not block measurement if the number of fields is lower).NGS calculates the mean grain area: Ā = mean value of {A1, A2 ….} in mm2According to 14.3 NGS calculatesGrain Size G = (-3.3223 log10 Ā) – 2.955NGS rounds off the value of G to the nearest tenth unit (not displayed nor reported).According 14.2 NGS displays Grain Size G as the nearest norm value (0, 0.5, 1, 1.5…14.0) ",
     id: 40 }, 
   { title: "Automatic Deconvolution",
     xmlid: "id|automatic.deconvolutionx",
     content: " Click  Automatic Deconvolution on the top of the image toolbar to display the following window.See also  . Figure&nbsp;607.&nbsp;Dialog window settings Numerical Aperture Enter the numerical aperture of the objective. Immersion Refractive Index Enter the refraction index of the immersion medium used. Predefined refraction indexes of different media can be selected from the pull-down menu. Calibration Enter the image calibration in μm/px. Channels Image channels produced by your camera are listed within this table. You can decide which channel(s) shall be processed by checking the check boxes next to the channel names. The emission wavelength value may be edited (except the Live De-Blur method). Brightfield channels are omitted automatically. Use Spherical Aberration Correction If turned on, spherical aberration of the PSF is calculated and used. Enter the Acquisition Depth (depth defining the center of the sample where the PSF is generated). Default value of the Acquisition Depth is taken from image metadata as a distance from the Z-Stack edge to the Z-Stack Home position. This distance is in most cases 1/2 of the Z-Stack depth. The only exception is the Asymmetric Z-Stack mode where the home position is explicitly defined by the experiment setting.Adjust the number of Layers if you want to use the depth variant deconvolution. If you set Layers to a number bigger than one, the stack is internally split in the Z dimension into several sub-stacks (layers) and an aberrated PSF is computed separately for each one. For example a Z stack having a total range of 10 µm, 5 layers, and acquisition depth (set in the deconvolution dialog) of 10 µm will generate PSFs for the sub-volumes at 6 µm, 8 µm, 10 µm, 12 µm, and 14 µm.Finally set the Sample Refractive Index value (refractive index of the medium used for sample acquisition). Default Sample Refractive Index is 1.35 (for water).If Use Spherical Aberration Correction is turned off, PSF without any aberration is used. Create new document New document is created once the deconvolution process is finished. Do not show this dialog for images with valid metadata If this option is checked, the settings dialog is not shown when deconvolving images with valid deconvolution metadata. Figure&nbsp;608.&nbsp;Acquisition depth and refractive index for different acquisition setups: left image - sample (red) with its surrounding solution; right image - sample with a coverslip and an immersion medium.A Water/oil/air objective with a sample (red) in a medium under the coverslip having a different refractive index that the objective was designed for.B Water objective with the sample matching the objective design.C Sample with its surrounding solution having a different refractive index than the objective was designed for. In this case acquisition depth = objective working distance. If using a correction collar on your objective, the space between the adjusted plane and the capturing plane defines the acquisition depth. Be sure to de-select the Use Spherical Aberration Correction option. On the contrary, when running deconvolution from the top image toolbar, 0 has to be filled into the Acquisition Depth field. ",
     id: 41 }, 
   { title: "Comparison Charts Workflow",
     xmlid: "id|comparison.charts.workflowx",
     content: " Start the application by   .Click the    button in the main tool bar.A dialog window appears. Select the Comparison Charts measurement method.Specify settings for the selected Image data source - see  .Click the Start Measurement button. A new full-screen window appears displaying the first image of the data source and a set of (ISO 643) comparison charts.The image is now displayed in one of the supported total magnifications (physical zoom). Its value is displayed in the tool bar next to the zooming buttons. The    button zooms the image to the nearest supported magnification which fits to the screen. The    button sets the magnification to 100x. Figure&nbsp;1280.&nbsp;See  One or more standard charts appear next to the image based on which comparison mode is selected in the tool bar. The    is used in the following example. Figure&nbsp;1281.&nbsp;The middle chart with the red border is the one whose value will be appended to the result table once you confirm the current measurement. Compare the chart with the image visually. If found useful, click on the    button to display chart contours over the image. You can move the contours around the image by mouse.Use mouse wheel or the up/down arrow keys to display charts with lower or higher grain size values. Once you find a chart with matching grain size, click the   .The grain size value is appended to the result table on the right, a corresponding frequency bin in the top right histogram table is increased by 1 and measurement of the next field is started. Should the current image for whatever reason be excluded from the measurement, click on the    button, no data will be added to the result table.  In case you realize a mistake was done, you can always use the    button to delete the last one or more measurements. If the Image data source is a folder or an ND2 file, the button also loads the corresponding image so that it could be re-measured.The measurement will be finished if: the specified Field count is reached,all images in the selected folder are measured,the user clicks the    button.Having measured all the fields or having terminated the measurement, you will be prompt to create a report or to exit the measurement: Figure&nbsp;1282.&nbsp;Manual finish Figure&nbsp;1283.&nbsp;Automatic finishYes The comparison window will close and the current data erased!Yes with report, OK &amp; Report All data will be passed to the reporting engine and the comparison window will close.No The measurement will continue with all previously measured data available.Select to create a report.Fill in the Report Info window and click on the Report button to create the report. Figure&nbsp;1284.&nbsp; Figure&nbsp;1285.&nbsp;Example reportSee  .Save the report to a *.rpt file readable by NIS-Elements or export it to *.pdf or *.rtf. ",
     id: 42 }, 
   { title: "Data output and Reports",
     xmlid: "id|data.output.and.reports.metx",
     content: "    ",
     id: 43 }, 
   { title: "Data Panel",
     xmlid: "id|data.panelx",
     content: "    Data 🔗,    View &gt; Analysis Controls &gt; Data  Data Table Object Data, Field Data Select what data to display in the left part of the window from the pull-down menu. Object data describe each detected grain, field data describe the field (one measurement action) statistics. Reset Data Erases all data of the currently selected method.   Show Data De-selecting this button can hide the object / field data table.   Show Statistics Displays an additional table where overall statistics (Mean, Standard Deviation, Minimum, Maximum) are displayed.   Show Histogram Expands the control panel by a histogram of the currently displayed data.Export Exports the data to a file.Please, see the   chapter of the core system manual for further details about this pull-down menu.Options This pull-down menu consists of additional commands. It is possible to load/save complete measurement settings to/from an external (*.meas) file.Statistics and Results Grain size G The result of the measurement rounded to 0.5.Histogram Apart from common appearance settings - like color, line width, or default text descriptions of the histogram items - there are the following options in the window:Pull-down menu serving for selecting a measurement feature, the distribution of which shall be displayed.Export  The minimized version of the standard Export pull-down menu.Please, see the   chapter for further details.Options  Mode In the Mode pull-down menu, you can select the way of displaying the Y axis (number, number in %, cumulative, cumulative in %).Line histogram The histogram can be switched to be displayed as a continuous line instead of a number of bars.Bins Select the Bins tab in order to adjust the way bins of the histogram are created. If the histogram bins shall be equidistant, bin width, minimum and maximum values can be set. If non-equidistant, each bin limit value should be put inside the definition table.Graph, Table you can switch between the graph and the table view via these buttons. The histogram source data are displayed in the table view.All Data the resulting data can be grouped by any of the table values (dragging the column caption to the gray area above the data table will create the groups). Records with matching values are put to the same group. This arrangement enables you to compare statistics of these groups in the histogram:Group the results according to one or more columns.Click the All Data button. A pop-up window appears displaying the list of groups or their combinations.Holding the Ctrl key down, select the groups you would like to display in the histogram. A color will be assigned to each group.To discard the grouped display, select the All Data option on the top of the list. ",
     id: 44 }, 
   { title: "Database",
     xmlid: "id|databasex",
     content: "        (requires:  )After you install the    module, the Database menu appears in the main toolbar. ",
     id: 45 }, 
   { title: "Algorithms",
     xmlid: "id|deconv.algorithmsx",
     content: "       Our model representing observed image is:b = Ax + wIn this equation, (b) is the observed image, (A) is a general transformation matrix and (w) is additional noise. We define the following problem to solveminimize ||Ax-b||2 + λ||x||1where (λ) is parameter of the regularization term – L1 norm of x. We can restrict matrix (A) from its generality to represent only the space invariant convolution to solve the problem efficiently. The minimization leads us to a two step algorithm.Repeat until the convergence is reached:Wavelet regularization of the intermediate result – wavelet soft thresholdingLandweber iterationThis algorithm is called ISTA and converges quite slowly. There exist many improvements of the algorithm with faster convergency. Here are some of them: TwISTA, SISTA, FISTA, FWISTA. Our Landweber method uses FWISTA. This method is an implementation of the Accelerated Lucy-Richardson algorithm with noise regularization. It is assumes that the input PSF parameters are known accurately. The problem which we try to solve can be described by the following equation:C = A * B +NWe know the captured image (C) and the PSF (B). We look for the true image (A) and we know there it is some additional noise (N) present. The * operator stands for convolution. So the equation can be rewritten as:The Lucy-Richardson (LR) algorithm belongs to the class of so-called Expectation Maximization(EM) algorithms where we assume the noise(N) has a Poisson distribution. We don't need any other characteristics of N. This algorithm estimates A from given B and C iteratively. The algorithm always converges to the result but not so quickly. Because of this, we need to let the algorithm do at least 10 iterations, optimally 30-60 iterations. Performing more than 60 iterations usually has no purpose. The number of iterations can be set within the dialog window. This method is an implementation of the Iterative Asymmetric Blind Deconvolution algorithm. It is called “blind” because PSF is not known exactly. However, at least rough estimation of the PSF parameters is needed. The algorithm improves the PSF/source image estimations iteratively during the deconvolution process. The problem can be described by the following equationC = A * B +NWe know the captured image (C), and we look for the true image (A), PSF (B) and additional noise (N). The “*” operator stands for convolution. The algorithm then works like this:It takes the first estimation of B (B1 - from given parameters) and a spectral characteristic of noise (N1) computed from C and estimates the true image (A1).C = A1 * B1+ N1In the second step, it uses the C, A1, and N1 values to re-estimate the PSF(B2)C = A1 * B2+ N1In other words this is a sequence of standard deconvolution operations where we reestimate - for odd steps: An from C, An-1; for even steps: An from C, Bn.This re-estimations continue until the defined number of iterations is reached or until the estimated values stop changing. The quality of results of the blind deconvolution depends on the number of iterations. The determination of this number depends on visual inspection of the resulting image. This method is an implementation of the Wiener filter algorithm. It is assumed that the input PSF parameters are known accurately. The Wiener filter gives us explicit solution of the following equation:C = A * B + NIf we know spectral characteristic of original image(A) and additive noise(N). B means the PSF and C is observed image.The result is reached by direct division in Fourier spectrum using the Wiener inverse filter that can be obtained as:where FBInv is a fourier transform of inverse filter, FB* is a complex conjugate of fourier transform of PSF, FB is fourier transform of PSF, PSN is power spectrum of noise, PSA is power spectrum of original image (can be substituted by power spectrum of observed image).When we obtain FBInv the reconstructed image can be obtained as follows:A' = C*BInv Articles concerning the algorithms were published by IEEE PAMI. Online access to the articles is charged and therefore cannot be shared here. Follow the links below to gain some more information.Landweber deconvolution   Richardson-Lucy deconvolution  Blind deconvolution   Fast Deconvolution   ",
     id: 46 }, 
   { title: "2D Deconvolution",
     xmlid: "id|deconv.settings.2dx",
     content: " Run the    command to display the following window. Figure&nbsp;611.&nbsp;Dialog window settings  Undo/Redo These buttons implement a standard undo/redo functionality. Deconvolution Method Select which deconvolution method is applied. See   for further details. Modality To handle the out-of-focus planes correctly, it is important to know how exactly the image sequence has been acquired. Select the proper microscopic modality from the combo box. Pinhole size Depending on the Modality setting, set the pinhole/slit size value and choose the proper units. Slit orientation If SFC/Slit mode is selected in the Modality field, define the slit orientation (horizontal, vertical). Magnification Specify magnification of the objective used to capture the image sequence. Numerical Aperture Enter the numerical aperture of the objective. Refraction Index Enter refraction index of the immersion medium used. There are some predefined refraction indexes of different media in the nearby pull down menu. Calibration Enter the image calibration in μm/px. Output Specifies whether a new document showing the deconvolved image is created (Create New Document) or the current image is overwritten with the result of the deconvolution process (Keep Current Document). Convert to Float option enables the user to convert 3D deconvolved images into floating point image. When Create New Document is checked, this conversion is done automatically. Z-Step Set the step size of the sequence. Noise Level Specify an estimation of the amount of noise present in the image. For the Richardson-Lucy deconvolution method a precise SNR (signal to noise ratio) value can be entered into the edit box when Custom noise level is selected. This value ranges from 1 (noisy image) to 50 (image almost without noise).For the Richardson-Lucy, Landweber and Blind deconvolution methods an Automatic noise level recognition can be set. Iterations Blind, Landweber and Richardson-Lucy deconvolution are iterative methods. More iterations (repetitions) take more computation time but usually improve quality of the result. Specify the number of iterations. For the Richardson-Lucy and Landweber deconvolution methods an automatic recognition of number of iterations can be turned on (Auto Stopping). Specimen Thickness Estimate thickness of the specimen: Flat, Thin, Normal, or Thick. This parameter influences the PSF shape. Import PSF Enables the user to import the Point Spread Function image. If this is done, the deconvolution parameters will be disabled.Importing PSF can be of great help especially if you have determined the PSF image by capturing a nano-bead. It is reasonable to import the PSF for fast, blind and Richardson-Lucy deconvolution either. See  .Multiple point spread functions can be imported at once if previously extracted from different layers by the    command. The deconvolved Z dimension is then divided into the appropriate number of segments. Each PSF is applied to the corresponding segment (e.g. 3 layers = 3 unique PSF – for the top, middle and bottom part of the deconvolved Z-stack). Channels Image channels produced by your camera are listed within this table. You can decide which channel(s) shall be processed by checking the check boxes next to the channel names. The emission wavelength value may be edited (except the Live De-Blur method). Brightfield channels are omitted automatically. Preprocessing This function can be used to subtract the background value from the image, deconvolution process is performed on the image without the background and once the process is finished, the background is placed back into the deconvolved image to its original place. By default the function is turned off (Do not subtract). Average background value can be estimated automatically (Subtract automatically) or defined manually (Subtract manually). When defining background manually, the Background Picker dialog window appears after clicking Define... (see:  ). This option works on the sample, not on the PSF. Remove spurious high intensity pixels If dark artifacts occur in the deconvolved image, it could be caused by single high intensity pixels which are sometimes produced by a confocal scanner. This option removes these pixels from the image before deconvolution.Points Spread Function  Use Spherical Aberration Correction If turned on, spherical aberration of the PSF is calculated and used. Enter the Acquisition Depth (depth defining the center of the sample where the PSF is generated). Default value of the Acquisition Depth is taken from image metadata as a distance from the Z-Stack edge to the Z-Stack Home position. This distance is in most cases 1/2 of the Z-Stack depth. The only exception is the Asymmetric Z-Stack mode where the home position is explicitly defined by the experiment setting.Adjust the number of Layers if you want to use the depth variant deconvolution. If you set Layers to a number bigger than one, the stack is internally split in the Z dimension into several sub-stacks (layers) and an aberrated PSF is computed separately for each one. For example a Z stack having a total range of 10 µm, 5 layers, and acquisition depth (set in the deconvolution dialog) of 10 µm will generate PSFs for the sub-volumes at 6 µm, 8 µm, 10 µm, 12 µm, and 14 µm.Finally set the Sample Refractive Index value (refractive index of the medium used for sample acquisition). Default Sample Refractive Index is 1.35 (for water).If Use Spherical Aberration Correction is turned off, PSF without any aberration is used.Compute PFS Automatically Automatically computes the PSF.Use Imported PSF:  Import PSF Enables the user to import the Point Spread Function image. If this is done, the deconvolution parameters will be disabled.Importing PSF can be of great help especially if you have determined the PSF image by capturing a nano-bead. It is reasonable to import the PSF for fast, blind and Richardson-Lucy deconvolution either. See  .Multiple point spread functions can be imported at once if previously extracted from different layers by the    command. The deconvolved Z dimension is then divided into the appropriate number of segments. Each PSF is applied to the corresponding segment (e.g. 3 layers = 3 unique PSF – for the top, middle and bottom part of the deconvolved Z-stack). Run on ROI Deconvolution can be performed just on a portion of the image - region of interest. Check Run on ROI and define the ROI parameters. Click Define to draw the ROI directly into your image and confirm it by clicking the secondary mouse click. Or fill in the rectangle values manually into the edit boxes. Max enters the maximal Z stack range. Deconvolve Executes the deconvolution process using the parameters defined in the dialog window.Preview Displays a preview image of the current settings. When computing the preview image, deconvolution is applied to the current view. For example, having a mono timelapse ND file, the deconvolution is applied to the currently displayed frame. Total time needed for deconvolution of the whole file can be then estimated as Preview Time x Number of Time Frames. If you process a single-frame image, the preview time equals the deconvolution time. ",
     id: 47 }, 
   { title: "3D Deconvolution",
     xmlid: "id|deconv.settings.3dx",
     content: "  Run the    command to display the following window. Figure&nbsp;609.&nbsp;Dialog window settings  Undo/Redo These buttons implement a standard undo/redo functionality. Deconvolution Method Select which deconvolution method is applied. See   for further details. Modality To handle the out-of-focus planes correctly, it is important to know how exactly the image sequence has been acquired. Select the proper microscopic modality from the combo box. Pinhole size Depending on the Modality setting, set the pinhole/slit size value and choose the proper units. Slit orientation If SFC/Slit mode is selected in the Modality field, define the slit orientation (horizontal, vertical). Magnification Specify magnification of the objective used to capture the image sequence. Numerical Aperture Enter the numerical aperture of the objective. Immersion Refractive Index Enter the refraction index of the immersion medium used. Predefined refraction indexes of different media can be selected from the pull-down menu. Calibration Enter the image calibration in μm/px. Z-Step Set the step size of the sequence. Noise Level Specify an estimation of the amount of noise present in the image. For the Richardson-Lucy deconvolution method a precise SNR (signal to noise ratio) value can be entered into the edit box when Custom noise level is selected. This value ranges from 1 (noisy image) to 50 (image almost without noise).For the Richardson-Lucy, Landweber and Blind deconvolution methods an Automatic noise level recognition can be set. Iterations Blind, Landweber and Richardson-Lucy deconvolution are iterative methods. More iterations (repetitions) take more computation time but usually improve quality of the result. Specify the number of iterations. For the Richardson-Lucy and Landweber deconvolution methods an automatic recognition of number of iterations can be turned on (Auto Stopping). Import PSF Enables the user to import the Point Spread Function image. If this is done, the deconvolution parameters will be disabled.Importing PSF can be of great help especially if you have determined the PSF image by capturing a nano-bead. It is reasonable to import the PSF for fast, blind and Richardson-Lucy deconvolution either. See  .Multiple point spread functions can be imported at once if previously extracted from different layers by the    command. The deconvolved Z dimension is then divided into the appropriate number of segments. Each PSF is applied to the corresponding segment (e.g. 3 layers = 3 unique PSF – for the top, middle and bottom part of the deconvolved Z-stack). Use Spherical Aberration Correction If turned on, spherical aberration of the PSF is calculated and used. Enter the Acquisition Depth (depth defining the center of the sample where the PSF is generated). Default value of the Acquisition Depth is taken from image metadata as a distance from the Z-Stack edge to the Z-Stack Home position. This distance is in most cases 1/2 of the Z-Stack depth. The only exception is the Asymmetric Z-Stack mode where the home position is explicitly defined by the experiment setting.Adjust the number of Layers if you want to use the depth variant deconvolution. If you set Layers to a number bigger than one, the stack is internally split in the Z dimension into several sub-stacks (layers) and an aberrated PSF is computed separately for each one. For example a Z stack having a total range of 10 µm, 5 layers, and acquisition depth (set in the deconvolution dialog) of 10 µm will generate PSFs for the sub-volumes at 6 µm, 8 µm, 10 µm, 12 µm, and 14 µm.Finally set the Sample Refractive Index value (refractive index of the medium used for sample acquisition). Default Sample Refractive Index is 1.35 (for water).If Use Spherical Aberration Correction is turned off, PSF without any aberration is used. Output Specifies whether a new document showing the deconvolved image is created (Create New Document) or the current image is overwritten with the result of the deconvolution process (Keep Current Document). Convert to Float option enables the user to convert 3D deconvolved images into floating point image. When Create New Document is checked, this conversion is done automatically. Channels Image channels produced by your camera are listed within this table. You can decide which channel(s) shall be processed by checking the check boxes next to the channel names. The emission wavelength value may be edited (except the Live De-Blur method). Brightfield channels are omitted automatically. Preprocessing This function can be used to subtract the background value from the image, deconvolution process is performed on the image without the background and once the process is finished, the background is placed back into the deconvolved image to its original place. By default the function is turned off (Do not subtract). Average background value can be estimated automatically (Subtract automatically) or defined manually (Subtract manually). When defining background manually, the Background Picker dialog window appears after clicking Define... (see:  ). This option works on the sample, not on the PSF. Remove spurious high intensity pixels If dark artifacts occur in the deconvolved image, it could be caused by single high intensity pixels which are sometimes produced by a confocal scanner. This option removes these pixels from the image before deconvolution.Points Spread Function  Use Spherical Aberration Correction If turned on, spherical aberration of the PSF is calculated and used. Enter the Acquisition Depth (depth defining the center of the sample where the PSF is generated). Default value of the Acquisition Depth is taken from image metadata as a distance from the Z-Stack edge to the Z-Stack Home position. This distance is in most cases 1/2 of the Z-Stack depth. The only exception is the Asymmetric Z-Stack mode where the home position is explicitly defined by the experiment setting.Adjust the number of Layers if you want to use the depth variant deconvolution. If you set Layers to a number bigger than one, the stack is internally split in the Z dimension into several sub-stacks (layers) and an aberrated PSF is computed separately for each one. For example a Z stack having a total range of 10 µm, 5 layers, and acquisition depth (set in the deconvolution dialog) of 10 µm will generate PSFs for the sub-volumes at 6 µm, 8 µm, 10 µm, 12 µm, and 14 µm.Finally set the Sample Refractive Index value (refractive index of the medium used for sample acquisition). Default Sample Refractive Index is 1.35 (for water).If Use Spherical Aberration Correction is turned off, PSF without any aberration is used.Compute PFS Automatically Automatically computes the PSF.Use Imported PSF:  Import PSF Enables the user to import the Point Spread Function image. If this is done, the deconvolution parameters will be disabled.Importing PSF can be of great help especially if you have determined the PSF image by capturing a nano-bead. It is reasonable to import the PSF for fast, blind and Richardson-Lucy deconvolution either. See  .Multiple point spread functions can be imported at once if previously extracted from different layers by the    command. The deconvolved Z dimension is then divided into the appropriate number of segments. Each PSF is applied to the corresponding segment (e.g. 3 layers = 3 unique PSF – for the top, middle and bottom part of the deconvolved Z-stack). Run on ROI Deconvolution can be performed just on a portion of the image - region of interest. Check Run on ROI and define the ROI parameters. Click Define to draw the ROI directly into your image and confirm it by clicking the secondary mouse click. Or fill in the rectangle values manually into the edit boxes. Max enters the maximal Z stack range. Deconvolve Executes the deconvolution process using the parameters defined in the dialog window.Preview Displays a preview image of the current settings. When computing the preview image, deconvolution is applied to the current view. For example, having a mono TZ (timelapse+Z stack) ND file, the deconvolution is applied to the currently displayed Z-stack only. Total time needed for deconvolution of the whole file can be then estimated as Preview Time x Number of Time Loops. If you process a single-Z-stack, the preview time equals the deconvolution time.  Figure&nbsp;610.&nbsp; Dialog window settingsCurrent Background Can be used to get the selected (Min/Mean/Median intensity) background values for each channel from the selection (current frame, all frames, selection or a rectangle). Use the &lt;- buttons to gain the appropriate values.Get Background Value As Specifies how the background value is calculated. It can represent a Min, Mean or Median of the rectangle.Get Value from Values can be gained from the Current Frame, All Frames or a Selection.MainView Default viewing mode.Max IP Maximal intensity projection is displayed in your image.Min IP Minimal intensity projection is displayed in your image.Define Rectangle Draws a rectangle restricting the background detection. ",
     id: 48 }, 
   { title: "Live De-Blur Setup",
     xmlid: "id|deconv.settings.livedeblurx",
     content: " Run the    command to display the following window. Figure&nbsp;612.&nbsp;Live De-Blur OptionsUse Live De-Blur Check this option to enable live image de-blurring. Specimen Thickness Estimate thickness of the specimen: Flat, Thin, Normal, or Thick. This parameter influences the PSF shape.Image Noise Level Specify an estimation of the amount of noise present in the image. Contrast enhancement This option specifies whether contrast post-processing shall be performed after deconvolution. Choose its strength from the combo box. If you are not sure about its strength, select None. After the deconvolution is finished, you can use the    command instead.Subtract background before deconvolution Move the XY stage so that no signal (just background) is present in the live image and press the Get Background button. The background intensity will be computed from the acquired data.Insert deblurred channels to Live If this option is selected, the channels selected for de-blurring will be appended to the original channels coming from the camera. A multi-channel image will be produced containing either the original channels and the processed channels.Region of Interest Specify the ROI area. If you check the Entire image option, the ROI extends to the whole image. If you want to select just a part of the image, specify position of the top left corner and dimensions of the ROI.Press the Define button to activate image window and you have to define a rectangular area of interest. Finish area definition using right mouse click.Show on toolbar Adds the  Live de-blur button into the main toolbar which enables the user to directly control the Use Live De-Blur option. ",
     id: 49 }, 
   { title: "Detection Panel",
     xmlid: "id|detection.panelx",
     content: " This window adjusts settings for grain or boundary detection. A binary layer is created containing separate areas each representing a single grain. Figure&nbsp;1234.&nbsp; Select detection type.Adjust detection parameters - the most important of which is the threshold limit(s).Save all the settings as a detection preset for further use. Name it and assign an image to it for better orientation.See  ,  . Grain Size measurement belongs to the statistical methods, so it is not essential to set up the extremely precise thresholding to reach binary image with all ultimate details that might be complicated and time consuming. The manual corrections during measurement are available as well. ",
     id: 50 }, 
   { title: "Detection Panel",
     xmlid: "id|detection.panel.refx",
     content: "     Detection ,    View &gt; Analysis Controls &gt; Detection   Figure&nbsp;1292.&nbsp;Detection panelDetection Presets ,  Selects number of detected phases. Select a particular preset from the nearby pull-down menu. This selection influences the set of available detection tools.Save As This button can save the current preset under a different name.Properties... You can modify the name and the sample image of the current preset by selecting this button.Delete This button deletes the current preset.Detection type Light grains with dark boundaries, Dark grains with light boundaries The most frequent and typical detection types. The first is for bright field microscopy, the second is for dark field or polarized light microscopy. Both are based on the intensity threshold.Contrast grains, Contrast grains with boundaries Designed for color etched, tinted or for samples analysed in polarized light (like e.g. aluminium); uses special image transformations and may be very sensitive to sample look.Simple A predefined procedure for detection of two phases. The user defines just the threshold value.Advanced Enables an advanced user to define a custom sequence of preprocessing commands to prepare the image for detection. Other custom processing functions can be then applied to the binary image which resulted from the detection. See  .Macro Loads an external macro which would perform the whole detection procedure including threshold.Ai (requires:  )Uses the artificial intelligence Segment Objects.ai detection method  ( ) trained by    . Only the “.oai” files present in the NIS-Elements installed GrainSize folder (e.g. C:\\Program Files\\NIS-Elements\\nisai_networks\\GrainSize) are shown in the Ai Detection list. Click on the arrow on the right to select the file. To load a file from a different location, use Browse....Preprocessing Adjusts the image in order to enhance the important image features (object borders, etc.) for easier thresholding.Correct background inhomogenity  This function corrects illumination inhomogeneities of the color image.Threshold Specifying correct threshold limits is a crucial task of image analysis. The point is to determine, which pixels will and which will not be included in the binary layer, and thereby distinguish analyzed objects from the background. The Grain Size application uses the thresholding tool of NIS-Elements. There are only the following buttons added: Display Original Image - changes are made to the image throughout the detection procedure. This button enables you to display the original image before any changes were applied. Auto Threshold - pressing this button modifies the threshold settings automatically according to what detection preset is selected.See also  .Postprocessing Adjusts the detected (segmented) objects to minimize false detections.Separate grains  An automatic procedure which separates the grains connected accidentally by the thresholding procedure.Delete grains smaller than  Detected objects smaller than the defined limit are excluded from the binary layer.Save, Cancel Whenever the settings are changed, these buttons becomes enabled. You can always use the Save button to store the changes to the current preset, or the Cancel button to discard them.  Figure&nbsp;1293.&nbsp;The preprocessing and postprocessing portions of the detection procedure may be customized. Select the Detection type &gt; Advanced option. An editable table where single processing functions can be inserted appears within the Detection window.The following tools are available:   Add function Opens a list of available functions.   Remove function Removes the selected function from the procedure. Preview Performs the so-far-defined procedure from the beginning to the selected function, and displays results.   Edit function Adjusts parameters of the selected function. Original image Overrides all image changes and displays the original image.Image PreprocessingThe first step of a successful detection is preprocessing. Prepare the image for the detection:Press the    button. The list of available functions appears. Figure&nbsp;1294.&nbsp;Select functions from the list one by one and press the Add button to include each one in the preprocessing. Or double click the function names. Close the list.Within the selected functions, pick the ones you would like to change the parameters of. When a function has some editable parameters, the    button becomes active. Press it to modify the parameters.Any function can be removed from the procedure by pressing the    button.Any function can be moved within the list by the arrow buttons in the top right corner of the window.Object Detection - Threshold&nbsp;Specifying correct threshold limits is a crucial task of image analysis. The point is to determine, which pixels will and which will not be included in the binary layer, and thereby to distinguish analyzed objects from the background.Binary Layer Postprocessing&nbsp;It is usually needed to process the detected binary objects using some morphology functions. Proceed in the same way as with preprocessing. Please refer to the electronic documentation for further information on the Binary Menu commands. ",
     id: 51 }, 
   { title: "Direct Mode",
     xmlid: "id|direct.modex",
     content: " The workflow to measure one image in a direct mode is:Start the applicationOpen or capture an imageSet or select a detection preset Figure&nbsp;1243.&nbsp;Preset Selection - DetectionCheck the detectionUse the    button.Set or select a measurement preset Figure&nbsp;1244.&nbsp;Preset Selection - MeasurementDisplay the mask and check it.Use the    buttonRun the measurementUse the    button.After detection, the software continuously measures all necessary parameters, calculates the Grain Size value and displays it in the    panel. Figure&nbsp;1245.&nbsp;Make manual correctionsAfter the software applies the measurement mask to the detected grains, manual corrections can be made. If you make manual corrections either in the   or an  , the GS value gets recalculated with every change. See  ,  .Review dataClick on the    button to display detailed data, you can switch between Object Data and Field Data. Figure&nbsp;1246.&nbsp; Figure&nbsp;1247.&nbsp;Click the    button to display a histogram. You can select the measured feature displayed in the histogram in a pull-down menu. Object data can be displayed also as a table. Figure&nbsp;1248.&nbsp;Histogram and feature selection Figure&nbsp;1249.&nbsp;Histogram dataSee reference:  .Confirm or cancel the current fieldUse one of the buttons on the left tool bar:   Stores measured data of the current field under a Field ID, clears overlay layers and prepares for the next field.   Discards measured data of the current field, clears overlay layers and prepares for the next field. When you finish the measurement with the Field Data option selected in the    panel, the last field gets selected. Data of this field are displayed in the statistics and the histogram. If you select the Select All option, the statistics will be always calculated from all fields. Figure&nbsp;1250.&nbsp;Current field dataRepeat the procedureRepeat the procedure on multiple fields. Once finished, export the results to a report or elsewhere. See  . ",
     id: 52 }, 
   { title: "Edit Area in the Focused Image",
     xmlid: "id|edf.edit.focused.areax",
     content: " This advanced option enables you to draw an area inside the image and select the Z slice to be used in the resulting focused image.Display the focused image.Select the    command. The cursor changes.Draw an area inside the focused image which should be affected. Finish drawing by right-click. The defined selection can be inverted by pressing G.Use the mouse wheel or arrow keys to browse Z slices within the selected area.Finish the procedure by a right-click or Enter. ",
     id: 53 }, 
   { title: "Real Time EDF",
     xmlid: "id|edf.realtimex",
     content: " (requires:  )Having a motorized Z drive, you can use the following procedure to capture an EDF image:Run the    command. The Real Time EDF window appears with the Align images check box added.Select whether to Align images before creating the EDF image or not.If you want to use high dynamic range, check Use HDR and set its parameters (see:   ).Choose whether to create a Z-Map (Balanced, Correlation or Original) and choose the Z-Profile coordinates (Zero Based or Real Coordinates). If the Z-map calculation is not necessary for your task, select None in the drop-down menu to speed up the EDF image acquisition.Set the parameters of the Z Series Acquisition.See:  .Click Run.  Either before or after the capture you can execute a Command (from the Command List) or run a Macro (from a file). ",
     id: 54 }, 
   { title: "Real Time EDF Manually (without motorized Z)",
     xmlid: "id|edf.realtime.manualx",
     content: " (requires:  )Even without a motorized Z drive you can capture an EDF image easily:Run the    command.A simple window appears. Select whether to Align Images and Create Z-Map (Balanced or Original). If the Z-map calculation is not necessary for your task, select None to speed up the EDF image acquisition. Figure&nbsp;615.&nbsp;Once you click the Start button, the system starts to capture images and pick the most-in-focus parts from the live image while you should move the manual Z drive so that you include the entire depth of focus.Click Finish to create the focused image. ",
     id: 55 }, 
   { title: "EDF Step by Step",
     xmlid: "id|edf.step.by.stepx",
     content: "  When you start NIS-Elements with the EDF module installed, a new section containing several commands appears in the Applications menu. Open (or acquire) an ND2 file that includes the Z dimension. If you use the    command to get the Z stack of images, a new ND2 file is created automatically.Align SequenceSmall shifts of the images can occur as a result of hardware inaccuracies during the Z series acquisition. To get reliable results out of the EDF module, the sequence that is to be processed should be aligned first. Use the    command to automatically correct possible shifts.Create the focused imageWhen the method is selected and the sequence is aligned, the only thing to do is to run the    command. Select the Processing Range and adjust the Z-Map Setting - choose whether to create a Z-Map (Balanced, Correlation or Original) and choose the coordinates for the Z-Profile (Zero Based or Real Coordinates). If the Z-map calculation is not necessary for your task, select None in the drop-down menu to speed up the EDF image acquisition.The focused image will be created and appended to the ND2 file.View the result Figure&nbsp;613.&nbsp;After the focused image is created, all buttons of the EDF toolbar (located within the image toolbar and in the context menu over the focused image) become enabled. There are three ways to observe the resulting focused image, as:A common Color/Gray image.A 3D model, utilizing so called 3D Surface View.A 3D anaglyph - to be viewed using 3D glasses.  These buttons display the focused image, the first one as a view of the existing ND image, the second one as a new separate 2D document.Creating anaglyph&nbsp; Using this button, you can create a three-dimensional stereo image (anaglyph). This image shall be observed with special glasses having red and green or blue filters which provides the 3D impression. Exploring the surface view&nbsp; This button runs the built in surface viewer. The surface viewer displays the surface of a captured object three-dimensionally. You can rotate the view by mouse or shift it by right-mouse. The mouse wheel serves for zooming. Figure&nbsp;614.&nbsp; Special OptionsDefault View Displays the object in default rotation and zoom.Z-zoom Adjusts the sequence height for more credible look. The first frame is lowest,  The first frame is highest Sets whether the first frame in the sequence is the lowest or the highest one. Clear Measurement If the   Length 3D  tool is used for measuring this button clears all the measurements made.See  . Select background Color Chooses the color of the background. Show Box Toggles the model outlines visibility. Surface Grid Shows a grid copying the surface topography. Adjust the line colors and density of the grid in Options found in the drop-down menu. Show Crop Plane Enables you to virtually cut the surface by a plane. Press Ctrl to display the cropping plane and the axis of rotation. Hold the Ctrl key down and drag the cropping plane or the axis of rotation with primary mouse button to a new position. Press Ctrl and drag the plane by right mouse button to rotate the cropping plane. Toggle Scene Lighting Switches on/off the lighting of surface view. Export to VRML Exports the surface to VRML (Virtual reality modeling language). Two files will be saved. One that contains the 3D information (WRL) and the other is a 2D texture (JPG). Mind, that both files must be distributed (e.g. placed on the web) together! To view files in VRML format some VRML viewer is needed. The viewer is a plugin to your web browser. There are several viewers available for free:Cosmo Player -  Cortona -   (please use Internet Explorer since Google Chrome and Mozilla Firefox do not support this plugin) Export to STL Exports the surface to STL ( ) importable by CAD softwares for modeling, 3D printing, etc. Movie Maker Runs the Movie Maker.See  . Switch to full screen mode rendering Press this button to display the current view in fullscreen mode. Exit the fullscreen mode by pressing Escape. Show help Opens the help describing the Extended Depth of Focus. ",
     id: 56 }, 
   { title: "Export to Report Template",
     xmlid: "id|export.to.report.template.gsx",
     content: " The most requested output of a grain size measurement is a Grain Size Report containing the measured values and some additional information about the sample, equipment, laboratory identification, date etc.Measure grain sizePerform the grain size measurement. Use a measurement method and mode of your selection.Initiate exportIf you used Comparison Charts, the export to a report template can be initiated in the last step of the procedure. See  .Otherwise, use the Export button placed in the    panel. Select Report Template from the pull-down menu and click on the button again. Figure&nbsp;1287.&nbsp;  Object and Field data are stored and also exported separately while the Grain Size number and the statistics are global data and are always exported. Select the data type in the    panel. Figure&nbsp;1288.&nbsp;Fill in export informationFill in the form, most of the fields are optional. In the last field, specify the path to the correct report template file (*.rtt). Figure&nbsp;1289.&nbsp;Some report templates are installed with the module to a directory named “GrainSize”, e.g.: C:\\Program Files\\NIS-Elements\\GrainSize\\Export\\ReportTemplate1.rtt A new template can be created or any template can be duplicated and modified in the NIS-Elements Report Generator. You can then export the same data multiple-times by repeating the procedure each time selecting a different template. This way, you could for example create a short version of the report for printing and a full version for archiving as a PDF.See  . The filled data will be remembered for the next measurement.Create the reportAfter you filled the form, click the Report button. A report editor will open with the data filled in the assigned fields. Figure&nbsp;1290.&nbsp;Example: grains size reportSave the report, it can be:stored in internal NIS-Elements format *.rpt;exported to *.pdf;exported to *.rtf (readable for text editors e.g. MS Word);sent to local or net printer;sent via MS Outlook (fully functional Outlook and e-mail client required) ",
     id: 57 }, 
   { title: "Advanced particle detection",
     xmlid: "id|filters.advanced.detectionx",
     content: " In the Detection control tab, aside from Thresholding, it is possible to add a processing function by the  button in the Preprocessing or Postprocessing part of the control panel. Figure&nbsp;636.&nbsp;Add a Processing Function (Preprocessing).The window contains a list of possible processing functions. The added function appears in the list below the Original Image item. Double click on the function or select it and press the  (preview) button to run the detection procedure from the beginning (original image) to the selected function. This way you can watch the influence of the processing functions on the image or the binary layer. The more complicated the detection is, the more time-demanding the analysis will be. When a filter contains thousands of fields, which is very usual, the analysis can take up to several hours. Nevertheless, usual filters are typically white (or light) and impurities dark, thus, the thresholding-only method will be sufficient in most cases.Once the preview of the detection procedure is satisfying, save it by pressing Save As.. in the top part of the control panel. The following window appears: Figure&nbsp;637.&nbsp;New Detection PresetList of all already defined presets is in the left portion of the window. Enter the Name of the preset and choose its Icon. You can use a saved image (From file...) or use the currently opened document (From document). Save the preset by clicking OK. ",
     id: 58 }, 
   { title: "Autofocus during scanning",
     xmlid: "id|filters.autofocusx",
     content: "  Figure&nbsp;633.&nbsp;Focusing ParametersDuring the scanning the objective has to be focused on the filter surface all the time. Choose a focusing method in the Focusing tab in   Options.None No focusing is performed during the scanning.Use manual focus The scanning is interrupted automatically and the user is asked to focus manually so that the scanning can resume.Use focus plane Enables the user to use a focus plane for focusing. If Autofocus with focus plane is checked, the autofocusing is turned on and the plane parameters can be set. Set the Focus every distance, Range and Step to define the plane. Use the button on the right next to the arrow to insert the recommended Step size.Use autofocus Turns the continuous fast autofocus on. Before use, the system must learn the properties of the objective and stage to focus quickly - start this process using Learn autofocus.The following window appears. Make sure the image is focused on a distinct object of a filter and that the optimal exposure time is set. Then click Yes. Figure&nbsp;634.&nbsp;Learn autofocus dialog windowThe system starts the autofocus calibration process which takes approximately less than one minute. Figure&nbsp;635.&nbsp;Learning processAfter the learning is done, you can test the autofocus by clicking Test Autofocus. ",
     id: 59 }, 
   { title: "Manual Editing of Particles",
     xmlid: "id|filters.manualobjecteditx",
     content: " In the analysis results panel, click the   Inspect Objects button. The Inspect Objects dialog window appears where you can inspect the detected objects one by one and correct the detection manually if needed. Figure&nbsp;642.&nbsp;Inspect ObjectsSelect the Type of material, its Class and how to sort the objects (Sort by).Use the arrow buttons at the bottom to browse the detected objects. If a particle needs to be modified, click Manual Edit. The original image frame will be opened and a simple drawing toolbar appears: Figure&nbsp;643.&nbsp;Simple Object EditorSelect one object which needs to be edited by a single click. To select more objects you can either hold the Ctrl key while clicking or use the   Select objects tool to draw a selection rectangle.Choose what to do with the selected object(s): Delete objects The selected vector object(s) will be deleted. Edit objects The selected objects will be displayed in a binary editor. You can re-draw them by several tools. Draw new Regardless of the object selection, the binary editor appears and you can draw new objects.Close the binary editor after adjusting the objects using the   Exit Editor or by hitting the Tab key. Changes made to the objects will be saved to the filter.   Close the Simple Object Editor.Changes to the objects are shown in the Edit History (Show Edit History...).Optional steps:If you need to change the Material Type, click on one of the material buttons (Particle/Metallic/Fiber) or hit the associated key (F1, F2, F3).If you need to display the current object in the main picture window, click Open Picture.If you want to remove the currently viewed particle from all classes, click Delete.If you want to see the binary layer in the preview, use the Show binary check box.If you want to have the current object in the report gallery, check Export this picture to report. Reset the list of exported images using Reset exports selection. To export the current objects and directly see the report, click on Export picture to report.Proceed to another particle or Close the Inspect Objects window. ",
     id: 60 }, 
   { title: "System settings",
     xmlid: "id|filters.systemsettingsx",
     content: " Turn on the system and all the devices, insert a typical filter.Check the camera settings and objective calibration. Follow the instructions:Turn the live image on using   Live.Move the camera over the sample area.Click  and open the Camera Settings control window. Here you can check the camera exposure. You can also set the Gain, Offset, Gamma and Saturation parameters, and use the Auto Exposure and Auto White options. In the histogram graph, while using the color camera, you can display either all the channels in one graph, or separately. All the changes are immediately visible in the live image. Confirm the setting by clicking OK. Figure&nbsp;629.&nbsp;Input Setup windowOpen the Objectives window by clicking   Objectives from the main toolbar. Figure&nbsp;630.&nbsp;Use New... in the right part of the window to add a new objective and follow the instructions to calibrate it.Choose a Calibration Method and click OK. Figure&nbsp;631.&nbsp;Calibration Method dialog windowAfter finishing the calibration select the newly calibrated objective as active (in the context menu over the objective choose Set as Active. Current objective is shown in a blue color in the bottom status bar.Check the camera orientation – click   Objective Info. Objective Info dialog will display information about the camera angle. Indicated angle should be as close as possible to 0 or 180 degrees. If the angle is way off from these values, you should recalibrate the objective. ",
     id: 61 }, 
   { title: "Automatic particle measurement",
     xmlid: "id|flt.measurex",
     content: " Click   Measure Filter to start the analysis of a scanned filter. This process can take up to several minutes according to the number of analysed fields and the detection preset complexity. You will be informed about the progress of the analysis in the following window: Figure&nbsp;625.&nbsp;Analysis Progress BarYou can pause or quit the analysis by the  or  buttons in the bottom right corner of the window.If you are opening the filter image from file, it already contains all the data, which can be recalculated. If you change the standard, which is used for particles measurement, the measured data will be recalculated. ",
     id: 62 }, 
   { title: "Filter Scanning",
     xmlid: "id|flt.scanningx",
     content: " During the scanning process you can follow the sequence of icons in the left tool bar.Before you start the scanning, please make sure the camera and objective settings are properly calibrated - see  . Optimal illumination should be set before analyzing any filter. Move the XY stage to the position, where the analyzed part of the filter is well visible. Software can assist you in checking and setting the optimal illumination. Click   Adjust Light to show the Check Illumination dialog window. Mean Intensity and the Overexposure Ratio characterize the scene and show recommended values. In case you decide to use this routine, you need to setup the recommended intensity and over-illumination tolerance to be compliant with your system. This is done in the Scanning tab of the   Options dialog (see  ). Figure&nbsp;617.&nbsp;Click   Define Position to start defining the filter position. Follow the instructions shown in the window. Enter the diameter of the scanned filter. Current objective calibration is shown below. Figure&nbsp;618.&nbsp;Move the XY stage until the crosshair touches the left filter boundary. Then click OK. Figure&nbsp;619.&nbsp;Click   Check Positions to check the filter edges. XY stage will reach the left filter edge and the following window will show up. Also a crosshair is shown in the camera image. It should cross the edge of the filter. If it does, click Next. If not, move the stage or drag the crosshair by mouse, so that the crosshair crosses the filter edge and click Next - the coordinates will be redefined to fit the point. Follow the instructions to check the filter edges. Figure&nbsp;620.&nbsp;Now move on to Scanning itself. By default, no autofocus is used during the scanning. If needed, go to the Focusing tab of the   Options (see  ).Click   Scan Filter to start scanning the filter. If the calibration in use is different from the recommended one, a warning window appears. You can set the calibration again (by selecting the correct active objective) or just click Continue if you are satisfied with it and the recommended calibration will be set to the current value. If there is an opened file in the filter preview window, the  button will be disabled. In such case, click  Close Filter to close this file. It is possible to scan the filter and immediately analyze it by clicking   Scan and Measure.The following window appears: Figure&nbsp;621.&nbsp;Scanning Progress dialog windowThis dialog displays the scanning progress, information about the ratio of overexposed pixels and mean intensity. Pause the scanning by the  button or quit the procedure by the  button at any time. If a continuous automatic exposure function of your camera is turned on, it will be automatically turned off until the scanning is finished. This is to prevent single tiles of the scanned filter to differ in brightness.The filter is saved to the temporary folder by default, but it is possible to select a permanent storage to a defined folder (see point number  ). Files are saved as a .flt. file format which contains the image data of all the fields, scanning date, pixel calibration, real filter dimensions and measurement circle, number of fields, images, dimensions, and other morphological data of all measured and edited particles. ",
     id: 63 }, 
   { title: "Capturing FRET Image, FRET Calibration",
     xmlid: "id|fret.capturex",
     content: "    Run the    command to display the main FRET control panel. Select the appropriate settings and capture the FRET image by the Capture FRET button. The FRET control panel consists of several sections. In this section, optical configurations which will be used during the acquisition should be assigned to FRET channels. Figure&nbsp;646.&nbsp;Optical Configuration In this pull-down menu, select an optical configuration that contains Camera settings which shall be used during the FRET image acquisition. If None is selected, the current camera settings will be used.Channels The FRET image can contain 2-4 channels. Here you can assign an optical configuration to each of them. It is recommended that the optical configuration contains only the filter changer settings. If the Dual View device is available, it can be used for channel splitting. Then, only one optical configuration for each excitation wavelength will be used. The Dual View device splits the emitted light to two beams filtering different wavelengths, and each of them is captured by only a half of the camera chip.Dual camera setup is also possible, splitting the different wavelengths between two cameras.Load/Save The channels settings may be saved to the system by the Save button. Once saved under some user defined name, this name appears in the Load pull-down menu ready to be loaded. The settings can also be loaded from an external FRET image. Separate images of the D-labeled and the A-labeled samples must be acquired in order to calibrate the FRET method correctly. Figure&nbsp;647.&nbsp;Insert the D-labeled sample. Press the Capture Donor Only Sample button.A new image will be captured and named “Donor”.Insert the A-labeled sample. Press the Capture Acceptor Only Sample button.A new image will be captured and named “Acceptor”.Press the Calibrate button to display the calibration dialog.Select the “Donor” image and the “Acceptor” image in the appropriate pull-down menus. Figure&nbsp;648.&nbsp;The Define ROI and the Define Background ROI buttons distinguish the foreground/background parts of the images.Draw the background/foreground ROIs.Confirm the calibration by OK.  Any image file can be used for the calibration. The pull down menu displays the currently opened images. Other images can be reached via the Browse button. If a non-FRET image is used, a window appears where you should map the image channels to FRET channels and select the FRET method. In this section, select one of the two available FRET methods, Sensitized Emission or Specified bleed through. Figure&nbsp;649.&nbsp;Sentisized Emission Figure&nbsp;650.&nbsp;Specified bleed throughXx FRET Average intensities of assigned FRET image components.Xx DONOR/ACCEPTOR Average intensities of the donor/acceptor-only images.FRETEFFICIENCY is calculated by the following formula: EFFICIENCY [%] = (FRETCORRECTED / DdFRET ) * 100FRET coefficientsThere can be 2 or 4 coefficients used according to the method. Their values are results of the calibration. If the calibration was not performed, you can type the coefficient values manually. The coefficients settings can be saved/loaded.Acceptor in FRET (CoA) DaACCEPTOR / AaACCEPTORDonor in FRET (CoB) DaDONOR / DdDONORDonor in Acceptor DdACCEPTOR / AaACCEPTORAcceptor in Donor AaDONOR / DdDONOR ",
     id: 64 }, 
   { title: "Creating FRET Image",
     xmlid: "id|fret.create.imagex",
     content: " The FRET image can be created from images saved on hard disk too. Run the    command. A window appears. The FRET image creation principles are the same as if capturing the FRET image except that you provide the module with mono images from hard disk instead of capturing them via a camera. Figure&nbsp;651.&nbsp;Select a mono image from the pull-down menus for each FRET channel. Their previews appear.Select a method, calibrate the module, modify the coefficients (similarly as if capturing the FRET image - see  ). Confirm the settings with OK.A new FRET image is created and opened within the application screen. ",
     id: 65 }, 
   { title: "Creating FRET View",
     xmlid: "id|fret.create.viewx",
     content: " Any image which contains at least 2 channels can be converted to a FRET image. Run the    command. The following window appears:  Figure&nbsp;652.&nbsp;Select FRET Method (see  )Assign one emission type (Da, Dd, Aa, Ad) to one image channel.Calibrate the module or enter the FRET coefficients by hand. See  . ",
     id: 66 }, 
   { title: "Introduction",
     xmlid: "id|ga3.introductionx",
     content: " General Analysis 3 (GA3) is a versatile instrument designed to construct image analysis procedures in a modular manner. These procedures, here referred to as “recipes”, are created in the GA3 Editor  (  ) and organized through the Analysis Explorer  ( ). A GA3 recipe contains a series of actions, such as denoising or thresholding, executed on specific inputs like image color channels or binary layers, ultimately generating the desired outcome. The recipe is graphically represented as an oriented graph, providing a visual representation of the analysis workflow process. Figure&nbsp;683.&nbsp;GA3 EditorMain toolbar3D nodes switchSearch boxCategoriesGroups with nodesGraph areaSaving nodesComments/zooming toolbarNodes are arranged within categories and groups, and they can be moved into the Graph area and connected in a sequential manner to form a analysis recipe. The nodes and their links determine the sequence in which they are executed. By connecting the nodes, the output of one node becomes the input for the next node in the chain. The graph consists of action nodes that produce specific intermediate results. Each step within the chain, which influences the final result, can be previewed or modified in real-time. Figure&nbsp;684.&nbsp;Node anatomyNode types Nodes are listed in groups which are organized in categories. Click on one of the category icons and browse through the node list below or use the search box to find the exact node. When working with 3D binaries, turn the 3D node switch on to automatically update the node list with the 3D nodes.Multiple instances of each node can be dragged and dropped into the graph area. Each node needs to be connected to its appropriate source, for example: Example&nbsp;8.&nbsp;ExampleThreshold node should be connected to a source color image on which the threshold is performed.Fill Holes node should be connected to a binary layer.Filter Records  node should be connected to Records.There are three distinct types of nodes available:Binary (circle shape input)Color (diamond shape input)Results (square shape input) ",
     id: 67 }, 
   { title: "GA3 Processing",
     xmlid: "id|ga3.tasks.allx",
     content: "              GA3 Processing &gt;  General Analysis 3🔗 To utilize more powerful analyses in the    module on captured images, use the advantage of the    task. The presence of resizing tools and optimized preprocessing features speeds up the detection and improves the overall data analysis. GA3 can also be run on an already acquired job run.Requires:Module:&nbsp;   Importing of tasks containing a GA3 recipe automatically exports the included GA3 definition into the   .Main AdvantagesGA3 can be created and edited directly from Jobs.One frame with relevant dimensions can be captured to serve as a preview.   task can be shown in the wizard mode both before the jobrun or during runtime.Wizard in Jobs shows only the GA3 wizard part of the dialog. What will be visible in each wizard page is defined by the Visible Channels/Binaries/Tables nodes in the GA3 Results tab.Result binaries can be exported to generate point sets.Result binaries can be exported to generate region lists.Result color images or binaries can be used as a stimulation pattern.Result data tables can be exported as JOB parameters.  Figure&nbsp;1212.&nbsp;General Analysis 3 task.Results&nbsp;Result tables are internally stored in a HDF5 format (accessible via free external viewers). Figure&nbsp;1213.&nbsp;HDF View.If the result table produced by the GA3 task can be classified as a frame table (contains all required loop index columns) or object table (contains all required loop index columns, binary object ID column and entity column), it is shown together with other jobrun features in the jobrun result window view elements (grid, wellplate view, ...). Figure&nbsp;1214.&nbsp;Jobrun Result window.Other tables are accessible in the Analysis Results window element invoked from the jobrun result window toolbar. Figure&nbsp;1215.&nbsp;Show Custom Results button. Figure&nbsp;1216.&nbsp;Analysis Results window.Runtime Wizard&nbsp;When the    task is set to be Shown at Runtime, user can adjust or define GA3 analysis settings on captured samples during the acquisition. The Show at Runtime property can be conditional, the activation/deactivation can be controlled by an   ,    or    tasks.  GA3 Processing &gt;  Average🔗 Processes an image sequence by the following processing node of the    module:   .Requires:Module:&nbsp;  Capture Task Select a    placed inside a loop.  GA3 Processing &gt;  Best Focus Plane🔗 Processes an image sequence by the following processing node of the    module:   .Requires:Module:&nbsp;  Capture Task Select a    placed inside a loop.  GA3 Processing &gt;  Integrate🔗 Processes an image sequence by the following processing node of the    module:   .Requires:Module:&nbsp;  Capture Task Select a    placed inside a loop.  GA3 Processing &gt;  Max IP🔗 Processes an image sequence by the following processing node of the    module:   .Requires:Module:&nbsp;  Capture Task Select a    placed inside a loop.  GA3 Processing &gt;  Median🔗 Processes an image sequence by the following processing node of the    module:   .Requires:Module:&nbsp;  Capture Task Select a    placed inside a loop.  GA3 Processing &gt;  Min IP🔗 Processes an image sequence by the following processing node of the    module:   .Requires:Module:&nbsp;  Capture Task Select a    placed inside a loop.  GA3 Processing &gt;  Align🔗 Processes an image sequence by the following processing node of the    module:   .Requires:Module:&nbsp;  Capture Task Select a    placed inside a loop.  GA3 Processing &gt;  Equalize Intensity🔗 Processes an image sequence by the following processing node of the    module:   .Requires:Module:&nbsp;  Capture Task Select a    placed inside a loop.  GA3 Processing &gt;  Stitch🔗 Processes an image sequence by the following processing node of the    module:   .Requires:Module:&nbsp;  Capture Task Select a    placed inside a loop.  GA3 Processing &gt;  Clear Temporary Data🔗 This task clears temporary data shared by several instances of General Analysis 3 tasks if they contain recipes with specific GA3 nodes supporting such sharing (e.g. Tracking).It enables the user to share such data inside an inner loop (e.g Timelapse) and clear them when outer loop (e.g. Multipoint) moves to the next position.(requires:  ) ",
     id: 68 }, 
   { title: "Toolbars and Menus",
     xmlid: "id|ga3.toolbarsx",
     content: " Main Toolbar Reset recipe Creates a new empty analysis. Save Saves the current analysis. Save As Saves the current analysis into a recipe with a given name and notes.  Undo/Redo These buttons implement a standard undo/redo functionality. Import Imports an analysis from a .ga3 file. Export Exports the current analysis into a .ga3 file. HTML export Exports the current analysis into a .html file. Activate document Brings the image being processed forward. Crop document Opens the Crop preview document where the size and dimensions of the current image can be cropped. The user can enter numeral values of the crop or directly resize and move the red rectangle in the image and select a loop range of the document. Use the Crop dimensions slider to set the dimension range or use Ctrl + (Shift) + click to select the document frames for the preview (highlighted green) and click the secondary mouse button to confirm the selection. Once the crop is confirmed by clicking OK, a new cropped document is opened. Edit wizard Opens the Edit Wizard dialog window used for adjusting and activating nodes being marked Show in wizard. After clicking the   Edit Task Description button, the description of each node and any inserted thumbnails can be adjusted using the top toolbar tools or using the HTML View (HTML 4 markup). Show help Opens the help file. Define preview ROI Preview ROI is used for previewing purposes to speed up the analysis definition. It defines a region of interest in the Define preview ROI dialog window or directly by resizing and moving the red rectangle in the image and selecting a loop range of the document. Use the ND dimensions slider to set the dimension range or use Ctrl + (Shift) + click to select the document frames for the preview (highlighted green) and click the secondary mouse button to confirm the selection. Information about the ROI selection is shown next to the button. To reset the ROI to the full size/full frames of the original image, click on  Delete preview ROI.3D Switches the node list from 2D to 3D nodes.Search This search box can be used to quickly find a node. Type in the name of a node, select it from the list of found results and click on it. It is then highlighted in the list below. Show task panel Shows/hides the task panel. Hiding the panel is useful for observing more complex analyses.Preview If turned on, a preview of the analysis is shown in the processed image. Progress bar is shown until the preview is generated. Run Performs the defined analysis on the linked image. Modify run options Reveals two run options: Add comment Any custom comment can be created using the Rich text editor  and added to the graph area. Adjusts the zoom factor to view the graph in its original size. Fit zoom Zooms the graph to fit in the window. Context menu over a node Enable/Disable Turns the node on/off. Disabled nodes are not used in the analysis. Delete Deletes the selected node. Open help Opens the help of the specific node. Locate Highlights the selected node in the node list. Rename Renames the selected action.Make Function If two and more nodes are selected, they can be merged into a single function using this command. The inner nodes can be edited (Edit Function) or restored back (Decompose Function). Please see   for details.Make Group Groups all selected nodes together. To ungroup the actions, click Decompose Group. Show/Hide in wizard Marks the selected action with a  symbol and puts it into the GA3 Wizard which is shown just after clicking Run. Once you go through all steps of the wizard and adjust parameters of all the actions, click Run to execute the analysis. Duplicate Duplicates the selected action. Dynamic parameters Adds more table inputs which can be used to refine the action and filter its output. For example find    on a channel, add a dynamic parameter “P1: Spots.Diameter” and connect it to records from a different measurement containing EqDiameter data. The    action is refined and the outcome respects the EqDiameter information.To add a dynamic parameter check the first check box in the Dynamic Parameters table and a new input with its name is shown above the node. Context menu over a result Store this result /  Do not store this result Defines whether the selected result is saved or not after the analysis is run. Set description Can be used to add custom description to the selected result or use the default one.Select All Selects all nodes in the layout.Color Can be used for changing color of the selected result.Move Layer Can be used to move the selected result above or below other results so that the preview and outcome can be properly seen.Make New Channel A new channel is added to the source image. Rename Renames the selected result. Make new layer Creates a new binary layer which can be viewed in the Binary Layers (  ) panel. Change layer to Takes the binary layer of the current node and moves it into the selected layer. Move layer Changes the order of the binary layer of the current node. Record OptionsEach record is represented by a complex table. The following options are available: Tab Results Results are shown as tabs (only one result is visible at once). Split Results Results are shown at once splitting the Analysis Results window. Split Horizontally Results are splitted horizontally (e.g. table data are shown next to their graph interpretation). Split Vertically Results are splitted vertically. Show Statistics Summary statistics for all records in the table are shown at the bottom. Export Exports the current records to MS Excel or Clipboard. Choose an option from the drop-down menu. Select binary objects The binary object selected in the table is highlighted in the image after clicking this button. Modify Table Columns Enables the user to select which columns are shown in the records table, change their name, format, and precision. Display Tracking Turns on all the tracking features set in the tracking Settings... found under the drop-down arrow. These features help visualize objects tracked using Tracking nodes.For more information about tracking capabilities, please see  . ",
     id: 69 }, 
   { title: "Data Integrity Report",
     xmlid: "id|gpa.cfr.reportx",
     content: " Data Integrity Report in the GPA application is in compliance with   defining the criteria under which electronic records and electronic signatures are considered trustworthy, reliable, and equivalent to paper records.To generate the Data Integrity Report, reveal the context menu over a locked GPA run, select   Data Integrity Report, select the path where the pdf file will be saved, name the file, click Save and choose a certificate signature. Figure&nbsp;496.&nbsp;Data Integrity Report example.Data Integrity Report typically contains:General information - author, recipe name, recipe locking date, computer name, application version, date and time, path to the image.Data integrity check - image data, binary data, acquisition settings, optics, calibration, camera information, recipe data and authentication.Log - start and finish times for acquisition, segmentation, measurement and inspection.User adjustable settingsManual inspection - number of edited objects and time of the report creation. ",
     id: 70 }, 
   { title: "Creating a recipe",
     xmlid: "id|gpa.definition.windowx",
     content: " The following workflow shows how to create a GPA recipe.Creating a new recipeClick on   Create New in the GPA Explorer, name your new recipe and optionally add some notes, then click OK.GPA Definition window opens. The four main tabs (described below) guide you through the recipe definition. Figure&nbsp;492.&nbsp;GPA Definition dialog window.AcquisitionAcquisition Type - choose how the sample area will be captured.Large Image - captures the frames across the defined area and stitches them into a large image.Covering - captures the frames across the defined area based on the specified maximum particle size and saves them as a multi-point image.Regular Pattern - captures the frames across the defined area based on the Coverage Definition parameters.Optical Configuration - choose an optical configuration used for image acquisition.Require image intensity level - optionally set the threshold values for the intensity of the image to be captured. Light source should be adjusted to fit into this interval during acquisition.Sample Shape - choose the shape that will define the sample area.Sample Size and Position - for a circular sample shape select:Fixed size and position to define the circle area just by the diameter of the circle.Fixed size to define the circle by its diameter and Diameter Tolerance which is the extent to which the newly specified diameter (specified during the GPA run) may differ from the one in the recipe definition.Custom to define the circle only by three points (successively click on buttons 1. Point, 2. Point, 3. Point on three different positions).For a rectangular sample shape select:Fixed size and position to define the sample area by the specified width, height and the coordinates of its   Top Left corner.Fixed size to define the sample in the same way as in the Fixed method but its top-left corner can be adjusted during the GPA run.Custom to define the rectangular area simply by setting its   Top Left and   Bottom Right corner. Click on  to reveal buttons used for moving the stage position.Coverage definition - set the image frames covering the sample area for the Pattern acquisition type.For the Covering acquisition type a maximal particle size can be defined (Maximum particle size for frames overlap counted as Max. Feret,   ). Particles smaller than this size are measured and the frames are scanned with the appropriate covering (max. is 1/4 of the frame width).For the Regular Pattern it can be set either as an exact number of frames (frame count) or calculated automatically by setting just the distance between the frame centers (frame distance).Measurement Mask - sets the excluded (not measured) area for the Large Image acquisition type as a distance from the sample border.Auto Shading - automatic image shading correction ( ) can be turned on.Focusing Options - set how the sample focusing will be performed.No focus turns the auto focus function off.Focus plane focuses on a focusing plane defined by three points during the GPA run.Auto focus can be used to specify the XY-Distance, Z-Range and Z-Step which will be used with the auto focus.Focus plane and auto focus combination of the latter two options can be set as well.The recommended range value is shown in the button with an  arrow. Click on it to fill in the recommended Z-Step value. AF Test runs live and performs auto focus with the current settings on the current stage position.Access - choose whether the current definition can be accessed only by its creator (Private) or by all users (Shared).MeasurementChoose between a Standard measurement (described below) or a Custom measurement using a previously defined General Analysis 3 ( ) loaded from a GA3 recipe file.Minimum particle size for measurement - particles smaller than this size measured as an equivalent diameter (Eq. diameter,   ) will not be measured during the GPA run. All the following measurement options are optional and can be defined (added to the list) after clicking on .Exclamation mark next to a measurement option indicates that the measurement is not set properly or some definition information is missing.Segmentation - select a segmentation method used for finding the particles in the captured image. Enter the name of the particles in the first edit box and choose its color. Optionally add another (different) segmentation(s). Click Segmentation Test to pre-adjust the segmentation on the live image. To close the segmentation window, click on Segmentation Test again.Simple Intensity Threshold (  )Intensity Threshold, RGB Threshold (  )Segment Objects.ai ( ) - click ... and browse for the .oai file.Measurements - check the object or field   to be measured.Custom Measurements - create a custom measurement (simple expression or JavaScript function) with the use of features checked in the previous step. Click on a feature in the Features list to insert it into the equation area below and use the Operators on the right to build the equation. To change the name of the custom measurement, edit the name in the Feature edit box.For example “Elongation = Max. Feret / Min. Feret”.Classification - create particle classes. Each class with its unique name and color is based on an existing segmentation (or all of them) and an equation. Click on a feature in the Measurements list to insert it into the equation area below and use the Operators on the right to build the equation.For example you have a thresholded binary layer. You can define a new classified binary and define a JavaScript which is called by the GPA for each object in the first thresholded binary layer. If the JavaScript returns “TRUE” then the object is moved to the new binary layer. E.g. “Max. Feret / Min. Feret &gt; 3”.Histograms - add histogram(s) to the results. Click on    and select one feature used for drawing the histogram. Then choose the segmentation (for which binary layer the histogram is drawn) and histogram type from the drop-down menus. Additional histogram options are available after clicking on   - see    for details.Result Tables - add table(s) to the results. Click on    and select the features used in the results table. Select the segmentation, click  to select a data sorting feature. Then choose whether the sorting will be ascending (Asc) or descending (Desc) and choose how many first rows of the table are shown first. Then click on the   Show result data and/or   Show statistics buttons to highlight them ensuring that they are shown after the measurement.Limits - create histogram or table feature limits evaluating the sample whether it is correct or not. Use the Equation... button to create the limits. Click on a feature in the Features list to insert it into the equation area below and use the Operators on the right to build the equation. Optionally change the naming for the values that passed or failed the set limits. To change the name of the limit, edit the name in the Limit edit box.For example in the histogram biggest particles class if there is less than 6 particles, then the sample is all right.InspectionYou can optionally add Inspection step(s) to verify the proper object segmentation when the measurement is done. Click on  to create a new inspection step.Select a segmentation layer with objects to be inspected.Choose an object feature to sort.Select the number of objects which will be manually inspected.Choose whether these objects are smallest or biggest.ReportResults acquired by the measurement can be organized and presented in a report. Click on  to create a new report template.Give the report template a name in the first field.Click   Edit.... A blank page is shown.Drag and drop report elements from the left tool bar into the report preview (white page area) to create a report layout.For more information about creating a report please see  .To open the report pdf file right after it is generated, highlight the   Print on Finish button by clicking on it. Digital signature can be added to each report to sign the pdf file. Highlight the   Use Digital Signatures button by clicking on it. See also  .Once all the GPA Definition tabs are set-up, click Save and Close, and you can run the definition - see  .Editing an existing recipe&nbsp;A new entry with the given name is created in the GPA Explorer list when the GPA definition is saved. Select a recipe name in the GPA Explorer list and click   Edit. If there is no recipe present, create a new one as described above. ",
     id: 71 }, 
   { title: "Executing a recipe",
     xmlid: "id|gpa.executing.recipex",
     content: " To run an existing GPA recipe, select a GPA definition from the GPA Explorer list ( ) and click   Run or double-click on it. GPA run tabs will appear in the middle section of the application. Click and adjust all the tabs settings step-by-step as described below. Figure&nbsp;493.&nbsp;GPA Run.LightEnter the Sample ID in the field below the stage preview and adjust the light source (fit into the limits if Require image intensity level was set for the current recipe).Click Next.SampleMove the green cross in the live image to precisely position the sample corner/edge and use the buttons below the stage preview to correct this position.Click Next.FocusIf Focus Plane or Focus Plane and Auto Focus were set in the Focusing Options of the current recipe, then click on a point inside your sample area, precisely focus on the sample and click Set and Next.Repeat this procedure for the second point not too close to first one. Click Set and Next.Repeat this procedure for the third point not too close to the previous two points and click Set and Finish.Move to a random point and click Move to Focus Plane to verify that it is really in focus or use the Move to Focus Plane and Auto Focus to do the focus automatically.Click Next.CaptureAutomatically scans the image. No user interaction is required.DetectAdjust the segmentation levels so that your particles are highlighted in the live image with the chosen segmentation color. If the segmentation is disabled, click   Check Detection.Click Next.MeasureMeasures / captures the sample and finishes the GPA run with a new record in the GPA Explorer.Click Next when the measurement is done.InspectIf an Inspection step was added in the GPA Definition then the GPA Inspection interface opens. Figure&nbsp;494.&nbsp;GPA Inspection.Here you can check and correct the detected objects using the Draw and Erase tools in the left toolbar to further improve the ongoing segmentation.Choose a binary layer which will be modified by drawing (select the rectangle of the binary layer below the image).Select a tool, adjust the line thickness (Size) and then draw directly into the binary layer in the image to correct the first object.Once your object is corrected, click   OK &amp; Next.Correct another object and click   OK &amp; Next. Repeat this procedure until all your objects are correctly segmented. To skip correcting objects at any time, click   Finish. To correct a class of objects or all objects again, click   Re-inspect. Click   Show Contour to easily see the edge of the binary layer of the object. Use Ctrl+up/down arrow keys to increase/decrease the binary object transparency.After the last object is inspected the measurement is repeated utilizing the segmentation improved by the manual inspection and the results are shown in the bottom part of the application. To repeat the measurement procedure on the same GPA definition, simply click on New. ",
     id: 72 }, 
   { title: "GPA Explorer",
     xmlid: "id|gpa.explorer.windowx",
     content: "  Figure&nbsp;495.&nbsp;GPA Explorer. GPA Explorer lists all GPA definitions (bold) and its runs (records shown below the definition on which it was executed). Definitions highlighted red are factory definitions which are present in the C:\\ProgramData\\Laboratory Imaging\\Platform\\GPA\\factoryDefinitions folder and cannot be edited.The following GPA Explorer tools are available: Create New Creates a new GPA definition after giving it a name and optional notes. Run Executes the selected GPA definition. Edit Opens the selected definition in the GPA Definition window ( ) so that it can be edited. Open Result Opens the image on which the selected GPA measurement was performed together with its results. Open Document Opens the image of the selected GPA run. Show Info Pane Reveals an information pane below the GPA Explorer with information about the selected GPA definition. Sort by Date Sorts all GPA results by their date and groups them by month. Sort by Sample ID Groups and lists all GPA results by their Sample ID. Sort by Type Groups and lists all GPA results according the their last execution time. Date Filter Turns the date filter on/off. To set the date filter, click on the black arrow next to the icon and specify the filtering interval in the left and right calendar (). Define Database Path Opens the Define database path window where you can create a new database (New) or set the path to an existing database file (Open). Import/Export from File Imports/exports a GPA definition from/to a .gpa file or imports all the definitions and explorer settings from an .esf file. Remove Deletes the selected GPA definition or run.Context menu over GPA definition Edit Opens the selected definition in the GPA Definition window ( ) so that it can be edited.Locked Check this item to lock the GPA definition. Once the Recipe locked dialog is confirmed by clicking OK, the definition is locked and cannot be changed. Definitions which are locked are indicated using the  icon in the GPA Explorer list. Although locked definitions cannot be edited, they can be duplicated and then edited while remaining unlocked. Run Executes the selected GPA definition. Rename Renames the GPA definition. Duplicate Duplicates the selected GPA definition. Remove Deletes the selected GPA definition. Export Definition Exports the selected GPA definition as a file into the specified folder. Import Definition Imports a GPA definition from a file. Export Segmentation Recipe Creates a new GA3 recipe from the settings in the Measurement tab (Minimum particle size for measurement and Segmentation). Export Measurement Recipe Creates a new GA3 recipe from the settings in the Measurement tab (Measurements, Custom Measurements, Classification, Histograms, Result Tables, Limits) and the Report tab.Context menu over GPA run Rename Renames the GPA run. Remove Deletes the selected GPA record. Open Result Opens the image on which the selected GPA measurement was performed together with its results. Open Document Opens the image of the selected GPA run. Export Definition Exports the selected GPA definition as a file into the specified folder. Import Definition Imports a GPA definition from a file. Open Report Opens the report file folder in windows explorer. Data Integrity Report Creates a CFR pdf report ( ) for a locked GPA run. ",
     id: 73 }, 
   { title: "GPA",
     xmlid: "id|gpax",
     content: "       General Particle Analysis (GPA) is a task oriented tool used for automated image acquisition and particle measurement. Step-by-step recipe creation and execution utilizing the motorized stage is the fastest and most convenient way to detect and measure particles.Main Features of GPAHighly optimized large image scanning.Wizard style procedure.Administrator and Operator mode (for creating and running recipes).Multiple segmentations including AI segmentation.Particle classification using JavaScript.Custom feature measurement.Manual particle inspection.User data output.Report including a digital signature.CFR compliance. ",
     id: 74 }, 
   { title: "Left Tool Bar",
     xmlid: "id|grainsize.left.toolbarx",
     content: " If the direct mode measurement is run ( ) the following tools appear in the left tool bar (based on the selected measurement preset).   Pointing Tool The main and default tool used for clicking on objects and selecting graphical interface elements.   Magnifier Glass Tool for magnifying portions of an image. Select this tool and click and hold the primary mouse button inside your image. To define the zooming parameters run   .See also:  .   Grain Size Wizard Opens the Grain Size Wizard window ( ).   Grain Size Comparison Opens the Grain Size Comparison window ( ).  Separate Grains (planimetric method)Use this tool to draw grain borders manually if some have been detected incorrectly. It is recommended to increase transparency of the binary layer [Ctrl + Down], the object borders become enhanced.   Connect Grains (planimetric method)Creates a connection between two neighboring graphites. Draw the connection by mouse.   Smart Delete Grain (planimetric method)Click the area to be deleted, and the system automatically unites it with one of the neighboring areas. This tool preserves the size of the whole detected area.   Delete Grain (planimetric method)Deletes continuous areas of the binary layer after they are clicked on.   Add/Remove Intersection (intersection methods)Adds an intersection point on the measurement mask, or removes an existing one.   Delete Segment (intersection methods)Point the mouse cursor between two intersection points, and the line segment will be removed from the measurement when clicked.   Show Mask Displays/hides the vector test mask. If the mask is moved before the measurement results are accepted, the image is re-measured according to the new placement of the mask.   Display in Colors Displays the detected line segments or grains in different colors.   Highlight Grain Borders (planimetric method)Displays the grain borders in the image.   Show Labels Displays labels within the image.   Show Detection Runs the currently selected detection settings on the current image, but no measurement is performed. The user can modify the detection preset and save it.   Start Measurement Starts direct measurement of the current image. See  .   Finish Measurement Saves the current measurement results to the Data table. The system becomes ready to measure the next field (image).   Skip Measurement When measuring a sequence of images, this button discards the results of a single measurement.   Cancel Measurement Discards results of the current measurement. ",
     id: 75 }, 
   { title: "Automatic Capture",
     xmlid: "id|gs.automatic.capturex",
     content: " This mode measures Grain Size on a requested number of fields inside a defined area. When selected, the following window appears: The scanning stage must be initialized beforehand. Figure&nbsp;1254.&nbsp;If you started the    and selected the option Automatic Capture as the image data source, continue as follows.Live camera image appears overlaid by a red cross. Use the arrow buttons or a joystick to move the sample so that the red marks the top left corner of the scanned area. Figure&nbsp;1255.&nbsp;Stage Area PreviewSpecify the Sample size and click the Check Bounds button. The XY stage will move the red cross to the bottom-right corner of the sample. Check whether the red cross marks is placed as expected, otherwise modify the specified sample size accordingly.Specify the number of fields to be scanned (Field Count). XY positions are calculated automatically by the software.See descriptions of all options in  .Click the Start Measurement button. The first field will be captured and a window for defining the detection parameters appears.Select a Detection Preset and click one of the buttons,  or , to check the detection. If needed - adjust options in the    panel - especially the threshold. Figure&nbsp;1256.&nbsp;If the preset is modified and not saved under a new name, the changes in detection are stored automatically to the actual preset.See also  .Click the Measure Field button. A window appears where you can select a Measurement Preset. Figure&nbsp;1257.&nbsp;See  If needed, adjust settings in the    panel, the changes will be saved to the preset in the next step.If needed, click the Back to Detection button to return to the detection adjustment. Then continue with the Measure Field button again.Click the Done &amp; Next Field button to finish measuring the first image.It is recommended to repeat this procedure (capture image, check the detection, check the measurement) at least twice. If you are confident that the settings are fine, click the Done &amp; Measure All Fields button. The rest of fields will be measured using the most recently saved presets.View the resulting data and export them in the required format. See  . ",
     id: 76 }, 
   { title: "Tools of the Comparison Window",
     xmlid: "id|gs.comparison.charts.referencex",
     content: "  Figure&nbsp;1286.&nbsp; 1:1 Comparison mode,  1:3 horizontal comparison mode,    1:3 vertical comparison mode, 1:4 comparison mode Arrangement of the image vs the comparison charts. Select the one which suits you. Increase grain size,  Decrease grain size Displays the next/previous comparison mask. You can use a mouse wheel instead.   Measure and new frame Confirms the currently selected comparison mask and writes its value to the result table. The next image is opened.   Skip and new frame Skips the current image and excludes it from the measurement.   Finish measurement Finishes the measurement. Create the report, otherwise the measured data will be lost.   Undo last measurement Deletes the last measurement from the table of results and displays the previous image.  There is no “previous image” in the manual capture mode so it just deletes the data in this mode.   Mask overlay Displays the selected mask over the image. Measurement sound alert If turned on, a sound is played after each field is measured. Show histogram window Displays histogram of results of the current measurement sequence in a separate window.   Zoom 100x Zooms the image so that it is displayed in the 100x magnification. This option assumes correct monitor (size) calibration.See  .   Optimal Zoom Images which are displayed too small/big in the 100x zoom can be zoomed automatically to reasonable size. The computed zoom factor is displayed next to this button and the comparison masks are changed accordingly in order to match the zoom. This option assumes correct monitor (size) calibration.See  . Reset data Deletes data from the table of results. Export data Saves the currently measured data to a CSV file. Report Creates a new report containing the current results. ",
     id: 77 }, 
   { title: "Measurement Mask",
     xmlid: "id|gs.measurement.maskx",
     content: " A measurement mask can be displayed by the    tool bar button. It can be moved and resized by mouse, or you can specify its size via the    panel. Figure&nbsp;1236.&nbsp; ",
     id: 78 }, 
   { title: "Sequential Measurement",
     xmlid: "id|gs.quick.sequentialx",
     content: " Before introducing the Grain Size Wizard, this was the basic method for measuring a predefined set of images.Place the images you want to analyze sequentially to one folder.Select the   Measure &gt; Measure Sequence  command.Browse for the folder containing the images.Run the batch process by clicking the Measure button.Confirm each measurement the same way as if measuring a single image. Another image will open automatically, until all images are measured.See  . ",
     id: 79 }, 
   { title: "Grain Size Wizard - Start",
     xmlid: "id|gs.w1x",
     content: " The first window of the grain size wizard () contains the following options:Measurement method Planimetric, Linear, Circular, and Abrams measurement methods determine shape of the test mask. Each method has some default settings, which can be adjusted during the measurement.The Comparison charts method lets the user to compare the sample images to standard comparison charts.Industrial standard Select the standard to be used. ISO 643 is used for the Comparison charts method regardless of what is selected.Phase count Specifies the number of phases being measured.Image data source Select the source of your images. See the description in chapter  .Sample size Specifies the size of a square for the Automatic Capture procedure. The specified number of fields are automatically arranged within this area.Field Count Define number of fields sufficient for successful measurement. After the specified number of fields is measured, the user will be prompt to finish or to continue with the measurement .Auto focus, Auto Focus Setup (Automatic Capture only, requires a motorized Z drive) Select this option to perform auto focus according to the Auto Focus Setup. The Steps in Range focusing method is used.  Figure&nbsp;1295.&nbsp;EDF, EDF Setup (requires:  ) (Automatic Capture only, requires a motorized Z drive) Select this option to use the Extended Depth of Focus capturing method where the resulting all-in-focus image is computed from a Z-stack. Set the range and step in the EDF Setup window. Figure&nbsp;1296.&nbsp;See   for more information., , ... (Automatic Capture only) Use these buttons to move the stage so that the red cross displayed in the live image is in the top left corner of the measured area of the sample.  Figure&nbsp;1297.&nbsp;Stage Area PreviewCheck Bounds (Automatic Capture only) After clicking this button, the motorized stage moves to the bottom right corner of the sample. If the red cross in the live image is not where expected, you can adjust the Sample size. The stage will move accordingly. Figure&nbsp;1298.&nbsp;Stage Area PreviewStart Measurement Proceeds to the next step of the wizard, which is grain detection. See  . ",
     id: 80 }, 
   { title: "Grain Size Wizard - Select Detection Preset",
     xmlid: "id|gs.w2x",
     content: "  Figure&nbsp;1299.&nbsp;Detection Preset Select a detection preset for your sample. Detections presets can be created and modified within the Detection control panel. If you modify detection options and proceed with the wizard by Measure Field or Measure All Fields, the changes are saved automatically to the selected preset. Show Original Color Image Displays only the sample without any overlay layer. Show Detection Displays a binary layer over the sample image which represents the detected grains. This layer changes as you adjust the options within the    panel. Show Borders Displays only colorized borders of the detected grains so that you can see the underlying sample.Measure Field Saves the current detection settings to the preset and proceeds to measurement of the current image. Opens the   window.Measure All Fields Saves the current detection settings to the preset and measures all images specified as Image Data Source in the first step of the wizard. The first measurement preset of the selected type (e.g.: Planimetric) is used without prompting the user to modify it. This button shall be used after several images were measured (via the Measure button) and the user is sure the current measurement preset works well.Skip Field You can skip the current image and exclude it from grain size measurement.Finish Finishes measurement of the current sample. The user is expected to export the data to a report or a table before measuring another sample. Otherwise, the current data will be lost upon starting the wizard again. See  .Cancel Cancels measurement of the current sample and no data are written to the table of results. ",
     id: 81 }, 
   { title: "Grain Size Wizard - Select Measurement Preset",
     xmlid: "id|gs.w3x",
     content: "  Figure&nbsp;1300.&nbsp;Measurement Preset List of measurement presets of the selected method (e.g. Planimetric). The method is selected in the first step of the wizard. New measurement presets can be created outside of the wizard within the    panel.Back to Detection Returns to the previous step, no data are measured.Done &amp; Next Field Saves the current measurement preset, measures the current field, opens the next field and runs the detection on it.Done &amp; Measure All Fields Saves the current measurement preset, measures the current field and applies the previously saved detection preset and the measurement preset to the rest of images specified as Image Data Source in the first step of the wizard.Skip Field You can skip the current image and exclude it from grain size measurement.Finish Finishes measurement of the current sample. The user is expected to export the data to a report or a table before measuring another sample. Otherwise, the current data will be lost upon starting the wizard again. See  .Cancel Cancels measurement of the current sample and no data are written to the table of results. ",
     id: 82 }, 
   { title: "Job Database",
     xmlid: "id|hca.dbx",
     content: "        (requires:  ) or (requires:  ) Next to the database path in the    panel, there is a pull down menu marked with  containing the following options.Create New Empty... Click this button to create a new database. Specify where it will be stored and enter its name.Create New Based on Current... Selects which items will be copied into the newly created database.Open Existing... Opens an existing database from a .dat file.Rename... Renames the current database file.Quick Open This option is used for fast switching between the databases checked in the Manage Quick Databases window. To edit the list of quick open databases, click Manage and add/remove/move up/move down your databases, check which of them will be shown and specify the maximal number of the recently used databases. Refresh This function enables refreshing the database.Cleanup Frequent database inserts, updates, and deletes can cause the database file to become fragmented. This function rebuilds the entire database to improve the speed when working with the database.Backup... Specify the folder where you want to backup your current database and select whether to include its images.Restore... Click ... and search for the .bin file to load your backed up database. Specify whether you want to restore images (Include Images check box) and click Restore.Upload... This window opens the Upload dialog. Please see:  . Click ... and specify where the Remote database .dat file is present. Use the check boxes next to each job to include/exclude them into/from the uploading process. The Uploading Rules table below shows the compared data status of both the local and the remote databases. Adjust the Action to perform combo boxes to set the merging rules and then click Upload to start the uploading process.Download... This window opens the Download dialog. Please see:  .Click ... and specify where the Remote database .dat file is present. Use the check boxes next to each job to include/exclude them into/from the downloading process. The Downloading Rules table below shows the compared data status of both the local and the remote databases. Adjust the Action to perform combo boxes to set the merging rules and then click Download to start the downloading process.Import Files... Opens the Import Files into Current Project dialog window enabling to import selected .ND2, .tif and .tiff files into the current job.Advanced Import Files... Opens the Advanced Import Files into Current Project dialog window used for advanced image importing into the current job. Dimension, its parsing and loop can be defined before the import. For more information please see  .Import BioStation CT Files... Opens the Import Biostation CT files into Current Project dialog window enabling to import selected biostation .CT files into the current job. Properties... Enables setting the database root folder, sharing properties together with image storage properties. For more information about database properties, please see:  .  Always use the    command when doing a backup. Copying the “jobsdb.dat” file manually could corrupt the database structure and the backup will be inaccessible.Run the    command, a dialog box appears. The backup routine copies the database file to a specified folder. If image files are also backed up (recommended) they are copied as well. The backup folder is a regular database bundle and can be accessed from NIS-Elements. If the backup folder already contains a previous backup, it is merged in the following way:The previous database file is renamed (current date is appended) and a new file is created.New image files are added to the backup folder.User can choose what will happen with the ND2 image files. Figure&nbsp;1024.&nbsp;Please see   chapter on how to set the database. This operation replaces the content of the current database folder with the content of the backup folder. Restoring the database destroys the original content of the current database. Figure&nbsp;1025.&nbsp;Press the ... button to browse to your database folder. After selecting it, click OK and Restore to finish the restoration process. You can uncheck the Include Images check box in case you don't want to restore image files. Projects, jobs and job results are stored in a database. This database contains all metadata except images acquired by jobs. Images are not stored in the database file. This is for two reasons: efficiency and data security. As images are ordinary ND2 files they are independent of the database and can be opened by NIS-Elements directly. Images are marked as read-only and should not be modified in NIS-Elements. On the other hand, the database can be opened and accessed without the images. Therefore images are stored in folders and the database contains relative paths to them. Images and database files exist in a folder and make a bundle. This bundle can be copied as a whole. However it is not safe to copy the folder while it is accessed by NIS-Elements. Use the   /   functions instead. You can adjust the following database settings after you click the  Database Properties button: Figure&nbsp;1026.&nbsp;Database root folder Define the path to your database files and select the type of access to the data. You can choose between: Local database - recommended, fast, safe.Shared database (synchronous) - opt this if access from multiple PC's is required. This setup is safe and sufficient for long-lasting experiments (e.g. 1 frame per minute).Shared database (asynchronous) - if access from multiple PC's is required along with the speed of access. Regular back-ups are absolutely necessary with this setup. If an unexpected error such as power failure occurs while the database is being accessed, the database may get corrupted (lost).Image files are stored in Choose in which form you want to save your image files. The following options are available:&lt;JobName&gt;\\YYYYMMDD_HHMMSS_mmm\\&lt;files: A1.nd2, A2.nd2 ...&gt; Figure&nbsp;1027.&nbsp;YYYYMMDD_HHMMSS_mmm__&lt;JobName&gt;\\&lt;files: A1.nd2, A2.nd2 ...&gt; Figure&nbsp;1028.&nbsp;Y = Year, M = month, D = day, H =hour, M = minute, S = second, m = millisecond The jobs database is contained in the jobsdb.dat file in the database folder. It contains the project and the job hierarchy, all the definitions (jobs, analysis recipes, labels, tags, etc.), acquisition data like plate or well information, loop indexes, time analysis results, links to the image files and much more. All this information is stored in a single file. The database file is usually much smaller than the size of an image file.Other temporary files may also appear in this folder (typically jobsdb.dat-shm, jobsdb.dat-wal or jobsdb.dat-journal). These files indicate that the database is in use. All temporary files should disappear after all NIS-Elements applications accessing that particular database are closed. If NIS-Elements did not finish properly, these files may remain. They must not be deleted as they contain a valid database state. In this case NIS-Elements should be launched, database connected and NIS-Elements closed again. Note that it is not safe to copy the .db file at that moment. NIS-Elements is connected to a single Jobs database at any given time – it is the current database. The current database can be changed in Database Properties (see:  ). NIS-Elements stores the current database folder on per user basis which means that every user can have his own database.Database size&nbsp;To avoid deposit and upgrade difficulties caused by huge databases, please follow the basic database rules.Make smaller, more manageable databases (use   ).Measure features per object only where it makes sense - e.g. in many HCS analyses on a well-plate, statistics per frame (not per object) may be enough.Cut off irrelevant data before merging databases.If the database is still huge, it makes sense to put the .db file on an SSD disk (must be big enough) and mount a normal drive to “Projects”. Navigate to the folder containing your database file and execute the following command (from the command line): mklink /J Projects \"C:\\Temp\", where “C:\\Temp” is the folder of your choice. If you experience any other problems with the database (slow performance, crashing, ...), try to clean up the local and remote database (  ). With new versions of NIS-Elements the database file format may change to support new features. An upgrade of the database is required before it can be used by a new version of NIS-Elements. However, the upgrade renders the database inaccessible from any previous version of NIS-Elements. When the database is accessed by multiple NIS-Elements instances, they all must be working with the same database version.It is a good practice to make a backup of the database before an upgrade of NIS-Elements is made, although &lt;name&gt;_backup_&lt;date&gt;_&lt;time&gt;.dat backup file is created automatically before the upgrade is performed. This file is automatically renamed to e.g. _v520.dat if the backup succeeds. Settings of the old NIS-Elements 5.20 - 5.21 (builds 1400 - 1499) are inspected and the settings pointing to the upgraded database are replaced to show the new _v520 database.A warning message is displayed before the upgrade:The database atF:\\JobsDb\\jobsdb.datmust be upgraded in order to be used with this version of NIS-Elements.Note:(1) Upgraded database cannot be accessed by earlier versions of NIS-Elements!(2) This version of database will be preserved as jobsdb_v520.dat in the same location for older versions of NIS-Elements.Do you want to proceed with the upgrade? (requires:  ) Figure&nbsp;1029.&nbsp;Downloading a remote database or uploading a local database can result in database merging. Parameters and rules of database merging are specified in the Upload/Download dialog window.Dialog window optionsLocal Database If necessary, click ... and specify where the Local database .dat file is present.Remote Database Click ... and specify where the Remote database .dat file is present.Project Use the check boxes next to each job to include/exclude them into/from the uploading/downloading process.Content Displays the number of job runs and acquired images per each job definition. If the content displays this  symbol, some files are missing in the source database.Uploading Rules This table shows the compared data status of both the local and the remote databases. Adjust the Action to perform combo boxes to set the merging rules and then click Upload / Download to start the uploading/downloading process.  New data of runs/analyses/tags are transferred only if Overwrite is selected. New projects and job definitions are transferred always (selected only). Invalid actionsdownloading/uploading into the same databasedownloading/uploading into a database created using a different version of NIS-Elementsdownloading/uploading into a local database on a network drivedownloading/uploading into a network database folder on a network drive with read-only permission ",
     id: 83 }, 
   { title: "Running a Job, Viewing Results and Graphs",
     xmlid: "id|hca.viewresultsx",
     content: "       Once the job is run (after clicking the  Run Job button) a window is displayed showing the progress. Similarly to ND2 acquisition blue rectangles representing dimensions or frames turn green when captured. The window shows the analysis results immediately during the job after the analysis is done. Some tasks require user interaction during the job progress (e.g.   ) and other tasks can be forced to do it as well if you check Show at runtime ( ). Once the task definition dialog shows up, the Job execution is paused until user confirms settings modification by pressing Next.The following picture shows a well plate selection of 6x10 wells being scanned. Green color indicates which wells have been already captured, yellow indicates the well being scanned at the moment (C9). Figure&nbsp;988.&nbsp;Job Progress Visualisation Auto Contrast Automatically adjusts contrast value in the Preview image. Reset Contrast Resets the contrast value. View Binary Displays the binary layer in the preview. View Color Displays the color layer in the preview. View Overlay Displays the color layer and the binary layer in overlay. Hide Histogram Hides/Shows the histogram below the preview. Show Well Plate Displays the Well plate view. Show Heat Map Displays the Heat map view. Average or Last values of the Feature selected from the first combo box can be shown above the heat map. (Choose a feature that is displayed during the job progress and select the last or average value - it displays a number of the Runtime steps.) Results Opens the Job Results dialog window (see:  ). Pause Enables the user to pause the experiment - then the button changes to   Continue. For example: if your job does not perform auto-focus before each frame but you see that the preview image is getting blurry, you may pause the acquisition, re-focus and then start it again by clicking   Continue. Finish When clicked, only images captured so-far will be saved, the experiment will be ended. Abort Click this button to end the experiment without saving any data.  Right-clicking on the image preview enables the user to move the stage to keep an object in view manually using the   Move this Point to Center command. Context menu over the well plate can be used to display a Schematic View of the well plate, show a scale bar (Show Scale) or move the stage to the selected well, to a cursor position or to the stage center. Context menu over the well preview can be used to display a scale bar. (requires:  )Once the job run is finished, results are saved automatically and the Job results window opens. To open some other saved job results, select the particular job/job-run in the   and click   Results. The Job result window shows captured images and all associated captured data of a single job run. Figure&nbsp;989.&nbsp;Job Results windowThis picture shows a result of a well-plate experiment displaying preview of the captured images in each well. If multiple images are captured in a single well, they are automatically stitched into a single large image.Basic dialog areas&nbsp;Basic navigation is done using the ND navigation bars in the bottom part of the dialog window. Right side shows a preview of the selected well/frame with heat-map controls and acquired data browser.The Job results window views the results from “various angles”. There are three basic views:Thumbnail viewWellplate view (if wellplate was used during acquisition)Grid viewIn Thumbnail and Plate view you can view Images, Heat-map of selected data values (Quantities, Acquisition metadata and Analysis results) and labels (in case of wellplates). Grid view displays all results in a tabular form.ND2 Interconnection&nbsp;Double-clicking an image in the result view opens the underlying ND2 file with that particular image frame displayed. If the job consists of multiple ND files, just a single image window stays open. To display each ND2 file in a separate window, use alt + double click. Both options are available also on the context menu.Reusing a Job Definition&nbsp;ND2 files created in Jobs are saved together with their Job Definition. To display or reuse this definition, simply right-click on the opened image and choose Reuse Job Definition.Top Toolbar options Huge Thumbnails,  Large Thumbnails, etc. Thumbnail view displays all acquired images in the order of acquisition. Channel displayed in the thumbnails can be selected in the tabs below the thumbnail view. Wellplate Real layout of the plate is displayed. XY View Any captured multipoint frames are visualized as a single image. Grid Images along with metadata are displayed in a grid (see below). Filtering and grouping is supported (you can sort and group by any column). Statistics can be turned on. See  . Show Images Displays all acquired results as images. Show Labeling In the wellplate or thumbnail view, this button displays color labels for each well if the labels were defined. Show Heatmap Independently on the current view (thumbnails, well plate, grid), you can select a heat-map which represents one of the meta-data values (e.g. analysis results, gas concentration, temperature, etc.). In the heat-map view you see the selected colored squares instead of images. The color is based on the value and can be tuned. If multiple data are acquired in one well/frame (e.g. when multi points were captured in each well of the well plate), it is possible to aggregate these data (calculate min, max, mean, sum, SD, etc.) in order to get a single value per well. If one of the aggregating features is selected from the combo box, entire multi point bar is selected and the results show the aggregated values. If one multi point is selected in the multi point control bar, Aggregate combo box is automatically switched to None, displaying only non-aggregated results of the currently selected multi point.An example of a heatmap is displayed in the following picture. Displayed as thumbnails, each image is overlaid by a color and a value both of which carry information about the meta-data selected in the bottom-right corner of the window. By default the color range of the selected feature uses minimal and maximal values found in the entire job run. The color scale range and colors can be modified by the user. Figure&nbsp;990.&nbsp;Job Result: Thumbnail View (Heatmap) Select the meta-data to be displayed in the heat-map in the Quantities panel (bottom-right area of the window).Displaying two features in a heatmap&nbsp;Heatmap mode can be used to view two features at once. Open the Quantities panel and select a feature to be displayed from the first tab. Then check the second tab and select a feature. Both features are now displayed in the heatmap. Use the Properties Compare Mode button to switch to the  circle diameter mode or back to the  divided mode. Figure&nbsp;991.&nbsp;Circle diameter mode Figure&nbsp;992.&nbsp;Divided mode Show Histogram Data of each image are represented by a histogram.Aggregate Aggregates data by the selected feature. Time Trend Time trend represented by arrows for each image can be displayed using this button. Show Feature Values Displays a numeric value of the selected feature over each well. Use Filter Activate this button to filter the results. The filter can be defined by clicking the Define Filter button. Define Filter A filter may be applied to images displayed in the result view. Click the Define Filter button to display a table with the filter definition. Toggle the Use Filter button to turn the filter ON and OFF. Figure&nbsp;993.&nbsp;Job Result: Grid View Auto Scale Intensity Performs automatic setting of LUTs on the image thumbnails. Click on the pull-down menu to reveal LUTs options. Scaling can be based on a selected image (Scale From Selection) or all images (Scale From All Loaded). To adjust the LUTs settings, select Modify LUTs.... Export View Exports the well plate overview.  Show Selected Well/Jump to Wellplate Switches between single well view and well plate view. Import Metadata To assign prepared labels and metadata to the wells displayed in the result view, copy the metadata inside your spreadsheet application and click this button. Please see   for importing prerequisites. Run General Analysis 3 Opens the Run General Analysis 3 on Jobrun dialog window where it is possible to choose an analysis which is executed on the selected jobrun. Run Analysis This button runs an analysis on images captured using a job definition. See  . View Graph Click the black arrow next to this button to choose a graph view from the list. Then click the button to open the selected graph view. For more information about viewing results in different graph modes, please see:  . View Definition Click this button to display the job definition window. See  . Grid Settings Shows a table with all acquired metadata. Each feature can be displayed or hidden in the Grid View using the check boxes. Show Preview Display or hide a small image preview window. Show Labels Display or hide the labels toolbar. Labels of the wells selected in the wellplate layout are highlighted here. Any label can be temporarily turned off by unchecking the check box next to its name. Show Quantities Display or hide the Quantities panel showing image metadata. Refresh Contents Hit this button to refresh the currently displayed results.Quantities toolbarRainbow Gradient This button indicates the currently selected color gradient which is applied to the data results view. Click on the button and select a predefined gradient or choose Manage Custom Colors... to define a custom color gradient. Please see    describing the gradient definition on a similar dialog. Figure&nbsp;994.&nbsp;% Feature values are represented by their percentage. 100% is assigned to the maximal and 0% to the minimal value. Set range from current view This button assigns the left color to the lowest value and the right color to the highest value present in the current data results view. Set manual range Applies the custom minimal/maximal scale range. Click the ... button and enter the minimum and maximum values to be used instead of the default ones. Figure&nbsp;995.&nbsp;Click  to apply the manual scale range. Set maximum range Any manual scale changes are turned off and the default (widest) range is applied. (requires:  )Clicking on the  View Graph  icon in the result view opens the graph view. To display different graphs of your results, click the graph button to expand a combo box containing more options. Figure&nbsp;996.&nbsp;Histogram graph viewTop toolbar options Graph selection The first button is used for selecting the graph type.   of Frames/of Objects/of Tracks Select whether your graph results are related to image Frames, binary Objects or Tracks. Graph sub-type Specifies the graph sub-type. Following types are available:  Histogram,  Scatterplot,  Barchart,  XY Line,  Timechart. Options Opens the Graph Options dialog window. In the Appearance tab, for each color scheme, the colors can be adjusted as well as the data series position, trendline style and position, graph size in tiling and spacing between the graphs. X Axis and Y Axis Left tabs group options for modifying the axes. Save Opens the Save Graph Settings window enabling to save the current graph settings into the database. Load Opens the Load Graph Settings window used for loading graphs settings from the database. Import Imports the graph settings from a .graph file. Export Exports the current graph settings into a .graph file. Export This button can be used to export the resulting graphs into raster files or Clipboard. Data can be exported into MS Excel or copied to Clipboard as well. Clicking the black arrow next to this button reveals a menu which specifies further export settings (  Settings) and which graph features will be exported.  Zoom in/out Zoom in/out inside the graph area.Tabs on the left represent steps of setting-up your graph visualization. Each tab has a check box, which determines whether the tab parameters are influencing the resulting graph or not.Definition tab&nbsp;In this tab, the user defines main properties of each graph type. Settings are not the same for each graph type. Generally features shown on the X and Y axis are selected and their range, bins trend lines and other variables can be adjusted.Filter tab&nbsp;Filter tab enables filtering results based on selected features. These can be selected from the combo box in the Column column. After choosing the feature for filtering, select the Comparison type. Click the combo box in the Value column to reveal a histogram suitable for quick value estimation or enter the numeric value directly into the edit box. If you add multiple filtering features, you can move them up/down in the list using these   arrows. You can also delete each filter clicking on this  icon. Figure&nbsp;997.&nbsp;Filter tab Only filters with the check box marked as checked are used for result filtering.Data Series tab&nbsp;Series enable the user to view different groups of data in the same graph. Each data series can be displayed in a different color. Select one feature from the combo box (label, quantity, class or any measurement result) and adjust its starting and ending color. Bin types, their ranges and limits, legend and other parameters can be adjusted. Figure&nbsp;998.&nbsp;Data Series graphPrimary/Secondary Tiling tabs&nbsp;Both define another two groupings enabling the user to see more instances of the same graph populated with different group data. Primary Tiling defines the groups to be displayed in graph tiles horizontally whereas Secondary Tiling defines them to be displayed vertically. Figure&nbsp;999.&nbsp;Primary Tiling tabClassification&nbsp;This tab is used to create classes over the resulting graphs. These classes are easily defined by selecting the class color, entering its name and specifying its range (by editing from and to columns or by moving the rhomb symbol directly inside the graph). Figure&nbsp;1000.&nbsp;Histogram and Scatterplot allow for manual classification. Scatterplot can be classified in two ways:2 Perpendicular Lines Select this option from the drop-down menu and give it a Classification Name. Then move the blue diamond directly in the graph (or the central horizontal and vertical line) to define the area of the quarters. Adjust the color and name for each class (Class Name column) and click Apply. A new column with the given name is created under the Custom Metadata column in the results view and the data values correspond to the defined classes. Figure&nbsp;1001.&nbsp;Four group scatterplot classification based on the VAP and STR featuresGating A gate is a graphical boundary that determines which characteristics of data are suitable for further analysis. Select Gating in the drop-down menu and give it a Classification Name. Click directly in the scatterplot graph to define a polygon representing a boundary for your selected data, then click the secondary-mouse button to confirm the gate. Multiple gates can be added simply by adding a new class (). Adjust the color and name for each class (Class Name column) and click Apply. A new column with the given name is created under the Custom Metadata column in the results view and the data values correspond to the defined classes. Figure&nbsp;1002.&nbsp;Two classes, named Cycle1 and Cycle2, set for gating. Figure&nbsp;1003.&nbsp;Two gates drawn directly into the scatterplot.. Figure&nbsp;1004.&nbsp;Resulting table with classes corresponding to the defined gates. Right-click a job name in the    panel to display the following commands:Run Job/Wizard Runs the selected job definition in normal/wizard mode.Continue Job Continues the selected job.Edit Job Opens the   containing the selected job.View Job Results... Opens the Job Results window (see  ).Run Analysis Runs analysis ( ) on the job run results contained in the selected job.Rename Job Definition Enables renaming your job definition.Duplicate Job Definition Duplicates the selected job.Delete Job Definition Deletes the selected job definition.Lock Job Definition Locks your job definition so that it cannot be changed in the  .Copy Job into another Project... Copies the selected job into the selected destination project.Run General Analysis 3 on Job Opens the Run General Analysis 3 on Job dialog window where it is possible to choose an analysis which is executed on the selected job.Export Job Definition... Saves the definition of the selected job as a .bin file into the specified folder under a specified filename. Multiple definitions can be selected at once and exported to the specified folder as well.Export to TIFF Files... Can be used to export job results to TIFF.Copy filenames to clipboard Copies the full paths to all images from all job runs inside the selected job definition to clipboard. Path to each image is checked so that only the existing paths are copied.Show in JOBS Toolbar control Places the selected job on the   as a button. Right-click a job-run in the    panel to display the following commands:View Job Results... Opens the Job Results window (see  ).View Job Results in a New Window... Opens the Job Results in a new window.Run Analysis... Runs analysis ( ) on the selected job results.Open Containing Folder Opens a folder on your hard drive containing your result image data.Export to TIFF Files... Can be used to export job results to TIFF.Copy filenames to clipboard Copies the full paths to all images from the selected job runs to clipboard. Job runs can be selected across multiple job definitions (use Ctrl + mouse click). Path to each image is checked so that only the existing paths are copied.Copy/Move Selected Jobrun... Enables the user to copy/move the selected job runs into a new or already existing job.Delete Job Run Deletes the selected job run.Re-assign wellplate Opens the Select well plate window where the user can choose a different wellplate for the current jobrun.Stitch Points into Large Image Opens the    dialog window enabling to stitch captured images into a large image. This function works on any Job or HCA with any Point loop if image overlapping exists. Resulting new (and modified) Job runs are put under the same job into a separate group called Stitched Large Images. These stitched images are shown stitched in the Preview area inside the View Job Results window.Run General Analysis 3 on Jobrun Opens the Run General Analysis 3 on Jobrun dialog window where it is possible to choose an analysis which is executed on the selected jobrun.Maximum Intensity Projection Performs maximum intensity projection on the selected dimension (Time or Z-stack) of the selected job run(s).Import Job Definition This button imports a previously saved job definition into your current project.Save this Job Definition As... Saves your job as a new Job Definition (see  ).Revert to this Job Definition Assigns the job settings of the selected run to its Job Definition. By default the Job Definition uses the settings of the latest run.Job Run Properties... Enables assigning short text notes to each of your job runs. ",
     id: 84 }, 
   { title: "HCA Analysis",
     xmlid: "id|hcajobs.analysisx",
     content: " (requires:  )NIS-Elements provides the following analysis modules:         You can include one or more of these analysis modules to be a part of your HCA job or you can just use them to process the resulting images after running the acquisition part of the job. Analysis performed on job resultsGo to the Job Results window e.g. by double-clicking the job run record.In the result view  ( ) , click the  Run Analysis button.The Run analyses dialog window opens. Figure&nbsp;917.&nbsp;Click   Add New and choose the type of your analysis from the pull down menu. You can change its name by double-clicking in the Name column. Select which portion of the data will be used for the analysis run (Run on column). A selection of frames can be made prior opening this dialog. Click Define... and continue setting up your analysis (described in separate analysis chapters). Once the analysis is set up correctly a green icon  is shown. To import analysis definition from a recipe use   Load.   Duplicate duplicates the selected analysis whereas   Delete removes it from the list. Define as many analyses you like and once you are done click Run. Your analyses are then processed and the resulting variables are added to the Show Quantities  panel. Hide your preview by clicking the  Show preview button to see more of your analysis results. ",
     id: 85 }, 
   { title: "Custom Equation",
     xmlid: "id|howto.custom.equationx",
     content: "      Custom equations can be effectively used to enhance your captured / live images. To add a custom equation to a live image, run   . The following sections show some common and useful custom equations. The equation Current is just a simple copy of the current image / channel. Applying custom LUTs only to this image can help with the visualization. Figure&nbsp;675.&nbsp; Equation -Current simply inverts the current image / channel. Often it is easier to see small details in the inverted mode. Figure&nbsp;676.&nbsp; Equation pow(Current,0.5) can create a “gamma” image. Figure&nbsp;677.&nbsp; Figure&nbsp;678.&nbsp; Equation Current-Previous displays only moving objects in black and white colors. The distance moved per frame can be easily seen - white shows the current position and black is the previous position. Static objects are removed from the image. Figure&nbsp;679.&nbsp;This equation can be modified to see only moving objects in white color: Average[+4..+6]-Average[+0..+3]. Here average is used for image smoothing. Figure&nbsp;680.&nbsp;Another way to hide static objects and display only the moving ones is to subtract average of all frames from the current frame. Use this equation: Current-Average. This can also shade-correct the image in some cases. Figure&nbsp;681.&nbsp; Maximum intensity projection that is increasing frame by frame can be used to visualize the movement of objects. Try the following equation: Maximum[-90..+0]. Range should be set to Relative. Figure&nbsp;682.&nbsp; ",
     id: 86 }, 
   { title: "Choosing the Deconvolution Method",
     xmlid: "id|howto.deconv.choose.methodx",
     content: "  Figure&nbsp;597.&nbsp;3D volume data before and after deconvolutionThere are different ways to sort deconvolution methods, either by the image data to be processed (2D/3D) or by the method of PSF determination (blind, Richardson-Lucy), or by expected quality of the result (fast). There are several commands available within the Deconvolution pull-down menu. Read the following overview and decide which command best suits your requirements:Deconvolution Commands   (requires:  ) This command is designed to work strictly with volume data-sets i.e. ND2 files containing Z-stack.   (requires:  ) Standard 2D single-frame images or ND2-files (Timelapse, multipoint, Z-stack...) can be processed by this command. Frame-by-frame deconvolution is performed in this case. Z-stack ND2 file can be processed by this deconvolution method as well, but the result will not be as good as you get from the 3D method.   Deconvolution of the live image may be performed by this command. A control panel appears where its parameters can be set and the processing on live image can be turned on/off. Deconvolution methodsIt is possible to decide between the following deconvolution methods. Each method uses a different algorithm to achieve the same goal (see  ).Automatic This method automatically deconvolves the opened image based on its metadata. If proper metadata are not available, deconvolution parameters can be specified before running the deconvolution process.Richardson-Lucy Using Richardson-Lucy deconvolution assumes we know the PSF exactly, so the system does not have to estimate it (see  ). This type of deconvolution usually leads to reliable results.Advantages It is stable method which produces good results.It is faster than blind deconvolution.It reduces image noise efficiently.It produces decent results even if you do not import PSF.Disadvantages The results may have lower contrast compared to the Blind deconvolution.Some image details may remain blurry.Landweber Landweber deconvolution is a current state-of-the-art method based on wavelet deconvolution. It can deal with noise and deliver crystal clear results.Advantages It is the fastest method (compared with Blind and Richardson-Lucy methods).It is stable (little less than the Richardson-Lucy method).It produces excellent results (not as bright as Blind in most cases, but with significantly less noise, which makes sample structures more clear).Disadvantages Needs approximately the same memory as Richardson-Lucy method.Rarely diverges.Blind This deconvolution method estimates the shape of the PSF from the parameters specified within the deconvolution dialog window. However, if the known PSF is used, it further improves the final result. Concerning RAM usage, it is the most demanding of all the methods.Advantages It produces top quality results.It makes very precise estimation of the PSF shape.It enhances even the smallest image details.Disadvantages The strength of this method may amplify image noise.In some cases, the method might diverge and give unsatisfactory results.Fast This method is optimized for speed.Advantages It is super fastDespite its swiftness, it can produce good results.Disadvantages The results are not as good as from the other methods.It can add artifacts to the image.Live De-Blur This method uses deconvolution algorithm similar to fast deconvolution but can be used on Live image (stream).Advantages It can handle frame-rates up to 10 fps.It can be applied to a restricted area of the image (ROI).Disadvantages Speed is the priority at the expense of image quality. Figure&nbsp;598.&nbsp;Comparison of deconvolution methods, from left: original image, fast, Richardson-Lucy, blind  Figure&nbsp;599.&nbsp;Comparison of deconvolution methods, from left: original image, fast, Richardson-Lucy, blind  Figure&nbsp;600.&nbsp;Comparison of deconvolution methods, from left: original image, fast, Richardson-Lucy, blind ",
     id: 87 }, 
   { title: "Introduction to Deconvolution",
     xmlid: "id|howto.deconv.introx",
     content: "  Deconvolution is an image-processing method for suppressing image blur caused by light/optical microscope systems. Deconvolution can be used to create sharper images with better resolution.Let's answer couple of basic questions before we start using the Deconvolution module:  Q:What is deconvolution for?A:Every microscope optical path (or optics in general) makes the resulting images blurry. Deconvolution is for de-blurring images captured by a digital camera. With NIS-Elements it can be applied to 2D images, 3D volume data, or Live image.  Q:Which deconvolution method is best for my image sample?A:Each method of the deconvolution menu is optimized for different purpose. To choose the right method you have to know especially:Do I need to process live image on the fly or a captured image file?Does the image contain 3D volume data (Z stack) or is it a standard 2D image?Do I need a high-quality result?Do I know exact parameters of the optical path - do I know the “point spread function” (PSF)?See  .  Q:What is PSF?A:Every optical path can be described by a so called “point spread function (PSF)” which specifies how a single point source will look like when captured by a camera. The PSF for each channel is usually represented by a grayscale image. A 2D PSF can look like this: Figure&nbsp;596.&nbsp;See  .  Q:Can I run the deconvolution module on my PC?A:It is important to realize that deconvolution methods demand appropriate computer performance. It is required to have at least 4GB of RAM, the faster CPU, the better. The need of RAM depends on the size of the processed files, 8GB is a standard, 16GB or more - highly recommended. Crucial acceleration of the deconvolution process can be achieved using a high performance GPU. Multiple GPU graphics cards are supported however it is recommended to use an even number of cards all of the same type. Deconvolution automatically calculates how much memory is needed for the deconvolution and what can be stored in the memory and what cannot, depending on free RAM memory and the sample dimensions. This optimization makes deconvolution much faster, because the deconvolution can avoid unnecessary accessing HDD. Implemented deconvolution methods are not suitable for confocal line scan documents. ",
     id: 88 }, 
   { title: "Determining the Point Spread Function (PSF)",
     xmlid: "id|howto.deconv.psfx",
     content: " There are two ways to determine the Point Spread Function:Capturing image of a nano-bead This way is the best way to determine the real PSF of your system. Even two equal microscope systems can produce different point spread functions.Insert a size-standard (a nano-bead) instead of your specimen. Size-standards are available commercially. The size of the bead should be as small as possible - i.e. ideally match the size of one subpixel (smaller than a pixel) in the resulting image. The same nano-bead can be used with different objectives (20x, 40x, etc.) as long as the sub-pixel condition is satisfied.Capture an image (or a Z-stack) of a single nano-bead. If it is difficult to set just one bead into the field of view, you can crop the image after acquisition. Make sure the background of your image is black (0). It is likely that the image of the bead will be blurry - this is the actual PSF of your microscope. If you plan to deconvolve multichannel images, you should capture a nano-bead with multi emission wavelength.Save the image to a file. For 3D deconvolution, the image must be a Z-stack ND2 file, for 2D deconvolution, the image must be in TIFF format.Use these images within the deconvolution dialog windows after pressing the Import PSF button. It is important that ALL(!!!) capturing settings used during PSF acquisition (for example Z-step, objective, scan zoom settings for confocal, etc...) match the settings used for capturing the specimen. Deconvolution results where a non-matching PSF was used can not be satisfactory!Calculating PSF from Properties of the Optical Path The PSF can be mathematically calculated from the following parameters:Numerical aperture of the objectiveRefractive Index of the immersion medium usedMicroscope modalityPinhole size (confocal microscopes)Objective magnification (confocal microscopes)Objective calibrationZ-stepSpherical aberrationExcitation wavelength (confocal microscopes)Emission wavelengthRefractive index of the sampleSet these parameters within the deconvolution dialog window. The PSF image will be created and used upon starting of the command. ",
     id: 89 }, 
   { title: "Tracking",
     xmlid: "id|howto.trackingx",
     content: "                                      (requires:  ) ",
     id: 90 }, 
   { title: "Triggered Acquisition",
     xmlid: "id|howto.triggeredacquisitionx",
     content: "                (requires:  ) ",
     id: 91 }, 
   { title: "Import Molecule List",
     xmlid: "id|import.molecule.listx",
     content: " Output of the STORM method results in a list of molecules (seeds) which can be imported (from a *.roi file) and displayed in NIS-Elements. The *.roi file contains 6 tab-separated columns for each molecule (row) in the following order: Number, X [nm], Y [nm], Z [nm], Frame number, Intensity.Roi file example11403.324607-82.87394319740721661.722094-264.3294859130332519.221886-193.1694819085142929.623406-81.9859467904495153725617-427.35942690449 ",
     id: 92 }, 
   { title: "Intercept Methods",
     xmlid: "id|intercept.methodsx",
     content: "  There are three intercept methods which count and measure the intercepts of a reference line with the detected grain boundaries. The Grain Size value is then calculated according to the expression defined in the standard. Each method uses different definition of the reference line (measurement mask).Linear Method The reference lines may be horizontal, vertical or both. The number of lines in each direction or the distance between the lines must be set. Clipping Rectangle defines the area of interest that reference lines don´t exceed. Figure&nbsp;1268.&nbsp;Linear method options Figure&nbsp;1269.&nbsp;Circular Method The following parameters must be set:Count Number of concentric circlesMin. diameter, Max. diameter Diameters of the inner and the outer circle. Figure&nbsp;1270.&nbsp;Options - Circular Method Figure&nbsp;1271.&nbsp;Abrams Method The following parameters must be set:Vertical and horizontal lines lengthDiagonal lines lengthSmall circle diameterMedium circle diameterLarge circle diameter Figure&nbsp;1272.&nbsp;Options - Abrams Method Figure&nbsp;1273.&nbsp;Other - Manual Method Special measurement is possible by the intercept methods, when the operator (for any reason) decides to skip detection and to make all intersections completely manually. Even in this case the detection preset must exist and must contain “full range thresholding histogram” covering the whole field. When the measurement starts while the    button is pressed, the mask overlaps the intercepts so the coloring is not visible properly. Switch off the button to see the intercepts only, not the mask.See also  . ",
     id: 93 }, 
   { title: "Advanced I-SPT",
     xmlid: "id|ispt.advancedx",
     content: "    Figure&nbsp;1455.&nbsp; Tick Advanced Settings.The Percentile Threshold is the rescaled brightest pixel value that can be considered as a detection. Set to a value around 10-20%. Figure&nbsp;1456.&nbsp;The Motion Blur size gives the maximum and minimum size for a detection to be accepted. A low percentile threshold shall be compensated by a high motion blur size. Set to a value between 0.1 and 2 µm². Figure&nbsp;1457.&nbsp;SNR ratio is the overall signal to noise ratio estimated of the particle. This is defined by the formula: Figure&nbsp;1458.&nbsp;The sum is computed over the pixels of the detection. The noise and variance are respectively the time median and variance of the one dimensional time series pixel without detections. SNR ratio shall be higher than 0.1.The tracking result preview shows the density of detections in the first 500, middle 500, last 500 frames of the movie. The density shall not vary significantly unless the final statistics will be altered. The problem of error and miss-assignment. Figure&nbsp;1459.&nbsp; ",
     id: 94 }, 
   { title: "Intra-Nuclear Single Particle Tracking (I-SPT)",
     xmlid: "id|isptx",
     content: "           I-SPT technical documentation with sample data can be downloaded  .Author: Vincent Récamier, 9.10.2014.Valid for NIS-Elements Ar, version 4.30.02. ",
     id: 95 }, 
   { title: "Advanced Topics",
     xmlid: "id|jobs.advanced.featuresx",
     content: "     ",
     id: 96 }, 
   { title: "Advanced Files Import",
     xmlid: "id|jobs.advanced.importx",
     content: "        Advanced Import makes it possible to create any job (and import it into the database) from individual files.JOBS experiments have a complex structure which typically cannot be entirely inferred from the files being imported. Therefore the import consists of telling the program the missing information.Especially the order of experiment (loop sequence and their nesting) must be defined in such a way that each frame from every file has its defined place in the sequence.Furthermore, not all important metadata can be derived from the image files (such as well-plate name, barcode, XY positions, wavelengths, etc.). As such metadata is typically related to the loops (plates, wells, Z-stacks, time-lapses, etc.) and repeat as a result of looping it is convenient to define them together with relevant loops.Advanced Import extends ordinary Import which is limited to experiments contained inside the single ND2 file, where all data is present. Figure&nbsp;1031.&nbsp;Invoking the Advanced Import Files into Current Project dialog ",
     id: 97 }, 
   { title: "Basic Job Rules",
     xmlid: "id|jobs.basic.rulesx",
     content: " Introduction&nbsp;Jobs are automated procedures for image acquisition and analysis. When a job is executed, images and analysis data are produced. The images are stored in ND2 files while metadata and analysis results are stored in a database including links to the corresponding ND2 files. A complete acquisition history is stored in the database as well.Order of execution&nbsp;A job is composed of tasks. The order of tasks in a job is essential because they are executed from top to bottom. There are two kinds of tasks: simple tasks and tasks which have blocks of other tasks nested inside them (e.g. loops and conditions). If there were only simple tasks, the job would be a mere chain of operations, very easy to assemble. On the other hand, it would be too difficult to create a simple time-lapse with 100 repetitions - having to copy the Capture task 100 times in a row. In order to avoid such copying, JOBS provide the concept of loops. Loops can embed other tasks. If a loop is run, it repeatedly executes the whole inner block of tasks until the defined number of repetitions is achieved.Another task which embeds other tasks is the Condition (  ,   ). Conditions are used for branching the job (deciding within the job flow). The embedded tasks are executed only if the specified condition is satisfied. For example: a condition in a job which scans a well plate can go like this: “capture a multi-point sequence on a well only if there is a fluorescence signal detected, otherwise proceed to the next well”. Figure&nbsp;958.&nbsp;A simple job capturing an image on each well center of the well plate selection RulesTasks are executed in order (from top to bottom).If a task contains child tasks, these child tasks are executed in order (conditions) or repeatedly (loops).Tasks inside a condition are executed only if this condition is fulfilled.Task position&nbsp;Position of each task in the job definition is substantial. In the following example,    and    tasks are swapped, bringing completely different results. Figure&nbsp;959.&nbsp;Herein, points on the sample are created manually. Time Lapse loop is then started for 10 minutes, executing embedded tasks every 20 seconds. Therefore a    (containing   ) is executed. Hence, an image is captured for each predefined point one after another. This capturing takes place every 20 seconds during the 10 minute Time Lapse interval. Figure&nbsp;960.&nbsp;If the Time Lapsetask is placed inside the Loop over Points task, the job works differently. Images are captured (for the duration defined in the Time Lapse task - 2 min in our example) on the first point, then for 2 minutes on the second point, etc. When all 2 min time lapses of all points we are looping over are captured, the job is finished.Task dependency&nbsp;Some of the tasks require other tasks to be present in the job - they depend on them. For instance, a well-loop which goes over wells requires a selection of wells on a well plate in order to loop over it (otherwise the job wouldn't know which wells to loop through). Moreover, the well selection task requires a well plate definition in order to know the dimensions of the well plate, how many wells are present and their exact position. Figure&nbsp;961.&nbsp;  If a task is not set-up properly (e.g.: missing dependent task, wrong task order in the definition, wrong task setting, ...) a red exclamation mark  appears in the caption of the concerned task. The job cannot be executed until all such errors are resolved.Definition-Execution type tasks&nbsp;Some operations consist of two separate tasks - one for definition  (e.g.   ) and one for execution (e.g.:   ). This is often useful in situations where a single definition is to be executed in different places of the job. If you wish to use such a task, always place the definition task before the execution task. Figure&nbsp;962.&nbsp;In this example, DAPI optical configuration is used in the    task and    parameters are defined. Points are generated on a dish and a loop over these points is created. On every point an image is captured (not saved) and used for the   . Information about the cell quantity at each point of the dish is used in a condition. If Cell Count is bigger than 0, all the tasks inside this    task are executed (   containing   ). Hence, image on each plane of the Z-Stack is captured (and saved). Notice that    uses the same definition in both places. If the user decides to switch the Optical Configuration used (e.g. from DAPI to FITC), he can simply change it in the    and all    tasks linked with this definition will use the new setting.RulesThe Definition task (usually containing the word Define, Definition, ...) has to be placed above its Execution task.Loops&nbsp;We can consider a well plate as a set of many wells. To sequentially pass through all the wells in Jobs we use Loops. Each processes (tasks) which we want to execute on the well level have to be placed inside this loop.    does not move between the wells on a well plate by itself. It is therefore impossible to expect pictures of single wells by placing just    inside the   . To do so, you have to place the    task (  ) inside the loop right before capture. This task tells the loop to move to the appropriate well.  Figure&nbsp;963.&nbsp;Loop over Wells and Loop over PointsIn this job, stage moves to the center of the first well of the well selection, auto focus is performed and random points are generated (first level loop). Then the job encounters the embedded second level loop. Still on the same well these random points are looped through and image is captured for each point. First level loop was now completed for the first well. Stage moves to the second well and the inside of the loops is executed repeatedly until all the wells of the well selection are run through.RulesTasks contained in a loop task are executed repeatedly and from top to bottom.Adjusting the Task Settings&nbsp;Tasks have different parameters which need to be set. These parameters are usually set at job-design time by double-clicking the caption of the task. However, crucial tasks can be selected and displayed within a wizard out of the main job body, so that the person running the job is not distracted by the whole job structure (see  ). Similarly, the same tasks can be displayed at runtime which enables the user to interact with the job or make decisions. Complex jobs can be parametrized ( ) to be more user friendly.Effective job building&nbsp;To avoid errors when building a job, all of the rules mentioned above have to be followed. The best practice is to think about the results we want to acquire and then go backwards. Which devices are necessary for our procedure? Is the procedure done only once or shall it repeat? Do I need to change sample holders (slides, well plates) during the job? Are there any decision-making points? What images exactly do we want to capture? Which device operations need to be done before an image is captured? Also think about the user interaction. Will the job be fully automatic or do you want to adjust the parameters during the job run? Try to answer all of these questions to make the job as simple and effective as possible. After that, you can start creating the job (see  ). ",
     id: 98 }, 
   { title: "Creating a Job",
     xmlid: "id|jobs.creatingx",
     content: "         Easiest way to start using Jobs is to choose from pre-existing Job templates created for most laboratory tasks. By clicking the   Add New Job from Template button in   you can access the Job Templates. A template is a predefined set of tasks (see  ) which you simply insert into your current job definition. The template can be used either “out of the box” or you can select a template closest to your objective and edit some of its tasks or add other tasks into it. Double-click on the desired template or press the Import button. The Job Wizard window appears. You can now set up the parameters step by step (see:  ). To see the job definition and edit the imported template, click Close and then click   Edit inside  . Figure&nbsp;964.&nbsp;Job Templates Go to    window.Here you can select an existing project or create a new one by clicking the   Add New Project button.After choosing a project click on the   Add New Job button.The Job Definition window opens. Here you can combine all different tasks to create a new job fulfilling your research needs together with all available devices compatible with NIS-Elements. Just drag and drop tasks from the left into the main area of the Job Definition window. Figure&nbsp;965.&nbsp;The tasks inside a job definition can be reorganized by dragging them up or down. Tasks which are not needed can be deleted by dragging the task out of the job window.To set the task parameters, double click the caption of the appropriate task. Name field (if available) can be used to alter the default name of the task, however some tasks do not allow numbers at the beginning of the name. Therefore letters may have to be entered first.Don't forget to save your job before closing the Job Definition window.  A Job is usually run multiple times to acquire data in different conditions. It happens that few parameters have to be changed before each run (e.g. length and interval of a time-lapse, well-plate metadata, like drug concentrations or labels, or even threshold intensity level). For this reason it is possible to mark some tasks for reviewing in a Wizard. In this case it is possible to run a Wizard where only the tasks that were marked are presented for definition. To put a task into wizard right-click the task and choose Put in Wizard. Figure&nbsp;966.&nbsp;Putting tasks into WizardDefinition Wizard is also useful for specialist / operator role division. The specialist creates complex Jobs with decisions and analyses. He does not want the operator to see all the complexity of the task but still needs to let him change a few parameters. The wizard enables this. Figure&nbsp;967.&nbsp;The current setting of the wizard can be saved into a HTML file after clicking Export.  Figure&nbsp;968.&nbsp;Job Wizard dialog window in the edit modeThe   Edit Wizard button opens the Job Wizard dialog window in the edit mode. Wizard steps (tasks that were marked with the  wizard icon can be freely sorted using the  up and  down arrows, except the Parameters step which has to stay always first. Wizard steps can be bound together by clicking on the  Connect or  Disconnect buttons. For each task, only selected options can be enabled for the operator, and others can be disabled. Thus the user working with the wizard can use only the options checked in the list under each task (Enabled Tasks Parameters). Figure&nbsp;969.&nbsp;Three programmable actions (events) can be used to validate the user's settings during the wizard run and to adjust them pragmatically or to display warnings. Tasks at the top of the Job Wizard edit window can be used to create a “mini job” which is then performed before the wizard opens -   Open Wizard, between each step/grouped steps -   Move to Step or after the Run button is clicked -   Run JOB.Texts of all used tasks can be changed (by clicking on the task name) and their description can be edited after clicking on the   Edit Task Description icon. Figure&nbsp;970.&nbsp;Edit Task Description window Font Style Select one of the predefined styles which will be applied to the whole paragraph or the selected text. Undo Moves back through the history of changes. Redo Moves forward through the history of changes. Cut Cuts the selected text into clipboard. Copy Copies the selected text into clipboard. Paste Pastes the selected text from clipboard to the cursor position. Bold Applies the bold font style. Italic Applies the italic font style. Underline Applies the underlined font style. Bulleted list Applies a bulleted list to the paragraph. Ctrl + - turns this function on/off. Ordered list Applies a numbered list to the paragraph. Ctrl + = turns this function on/off. Decrease indentation Decreases the paragraph offset. Increase indentation Increases the paragraph offset. Text foreground color Changes color of the selected text. Text background color Changes background color of the selected text.Font Size Changes size of the selected text. Insert image Inserts an image (jpg, gif, png, bmp) from a file. Remove character formatting Removes any character formatting from the selected text.HTML View Displays the HTML markup so that the task description and any inserted thumbnails can be adjusted using HTML 4. First step of any job procedure is to define your samples. You can choose between well plates, slides or stage areas (free shapes). If defining standardized sample holders (well plates, slides), it is recommended to select the specific product model you are using from the database. This can be done by clicking Select from DB.... Figure&nbsp;971.&nbsp;If your well plate cannot be found in the database, you can create a custom well plate. See  .Slide definition is closely described here:   .Any other free shapes on a stage can be defined using   . Please see   . Any sample holder that was selected/defined has to be properly aligned. Well plates are aligned using    inside Jobs or globally using   . Slides are aligned using    in Jobs or globally using   . Alignment of well plates and slides is stored in the database. User can see not only absolute stage coordinates but also coordinates relative to the plate/slide. Relative coordinates are helpful when later revisiting locations on the same or different system and when doing analyses outside NIS-Elements (e.g. doing statistics in MS Excel). Before capturing any images using the    task you have to define the capture settings in the    task window, where you select your optical configuration to be used to capture each of the channels. Typically, laboratory computer image analysis systems consist of a computer, a camera, and a microscope equipped with certain accessories (objectives, filters, shutters, illumination, rotary changers, etc). Most of the mentioned microscopic hardware is often motorized and therefore can be controlled via NIS-Elements. In addition, it is possible to integrate single settings of all these devices into one compact set called Optical Configuration. It is recommended to create several optical configurations containing particular devices settings. Then a single click can completely change the current hardware configuration.For more information about setting up the Optical Configuration please see:  .  Figure&nbsp;972.&nbsp;Loops are special repetitive tasks which contain an inner block. When you fill this block with tasks and run the loop, it repeatedly executes the whole inner block of tasks one after another. You can then define whenever you want to stop the repetition (after a certain amount of time, number of loops, after a condition is true, etc.). You can also loop tasks inside another loop to make building of repeated processes even more effective. Parameters represent “conditions” for single tasks. Multiple experimental situations can be prepared and organized into a single job so that the operator does not have to switch between many jobs.A single task can carry many parameters. If all values of its parameters are met at the beginning of the job run, the task is executed. If one parameter value is different than the initial parameter selection, the task will be skipped.To start using parameters, click the  Properties button inside the   and check Parameters. Now click OK to confirm the selection and open the Job Parameters window by clicking the  Parameters button. Click the  Add New Parameter button to add the first parameter. Give it a name, choose its type (On/Off, choice from a list of options or a numeric value) and define the values it takes clicking .... If multiple parameters are used one can require another one to be executed. This conditional parameter can be selected from the Requires parameter combo box. Don't forget to mark where each parameter will be shown and adjusted - in  Wizard,  Run Time or both. When done with defining all your parameters, click OK.Now you need to assign prepared parameters to single tasks. This can be done in the   by right-clicking on the desired task and choosing Requires parameter.... Parameter relations dialog window opens up. Figure&nbsp;973.&nbsp;Here you can choose from all parameter values defined previously and Insert them separately or use them with Operators defining an expression (e.g. Capture==Z-Stack AND Analysis==Cell Count; task will run only if the user selects to    a    and to use the    at the beginning of a job run). After confirming the expression, it is displayed in the caption of the appropriate task.Before executing the job it is important to define the initial parameter values at the top of the  . Figure&nbsp;974.&nbsp;Job Definition window showing the initial parameter valuesAfter executing the job, a parameter table will be shown. There you have to set the values you wish to use in the current experiment and click Next. All the subsequent steps of your job depend on this selection. Figure&nbsp;975.&nbsp;Parameter table in Job Wizard Figure&nbsp;976.&nbsp;Parameter table in Job Execution Progress window ",
     id: 99 }, 
   { title: "Using Questions",
     xmlid: "id|jobs.introduction.to.questionsx",
     content: " Question (  ) is a useful task if you want to stop your job in a specified part of the Job sequence and let the user decide what to do next. Start by dragging the    task into the main area of the Job Definition window (see:  ). Then double click the task to maximize it and set up your question. Figure&nbsp;1076.&nbsp;Question task.Name your Question task, choose an icon which will be displayed together with your question and type a text into the Caption box to be used in the dialog window heading. Then type in the text of your question.Wait for user action If checked, the question dialog shows at the bottom of the Job Execution window ( )and pauses the job until the question is answered by user. After that, the job continues or terminates depending on the question settings. If solely this option is checked, the question dialog disappears until the loop meets the question again (e.g. until the next Capture).Keep visible during current loop execution Displays the question dialog during the whole Job run. The answering buttons are disabled (gray) until the loop meets the question again.Continue execution and stay visible Proceeds the job even without user interaction (answering the question). The question dialog stays visible until the job run is over.If any variables are available, check Input values. Click the Add New button, modify the Text or change your variables by clicking the ... button. During the Job execution the question task asks you to input these variables manually. Finally define your question buttons. You can use basic predefined buttons by clicking the Combination Presets button or add your custom buttons and captions. Assign the Escape and Enter key to two of your buttons for faster work with questions during the Job progress. ",
     id: 100 }, 
   { title: "Introduction to the JOBS Module",
     xmlid: "id|jobs.overviewx",
     content: "        JOBS is an environment aiming at complex acquisition and data analysis with easy to use and comprehensive user interface.Features highlights:Well-plate, dish and slide supportConditional acquisitionSet of ready-to-use templatesWizard for OperatorsVisual Programming StyleIntegrated AnalysisAsset Management via DatabaseData MiningRemote Access and SharingNIS-Elements platformJobs are automated procedures for complex image acquisition and data analysis. Typically it is a conditional acquisition on well plates, dishes or slides for long times. It may include Confocal and wide-field camera acquisition combined with photoactivation or bleaching.The strength of Jobs is their flexibility. They rely on building blocks that can be freely assembled into arbitrary procedures in order to fulfill any particular needs. As all this is done visually using Drag-drop it is called Visual Programming. This approach combines advantages of programming – flexibility – and limits the drawbacks: having to learn how to write a programming source code.When such procedures – Jobs – are executed, images and analysis data are produced. The images are stored in ND2 files. Metadata and analysis results are stored in the database together with links to the ND2 files. The complete acquisition history is stored in the database as well as the whole organization and structure of Jobs and Projects.   JOBS Explorer  is the heart of the    and    modules. Run the    command to display it. All jobs, job runs and all acquired data are viewed and managed throughout this window. Figure&nbsp;955.&nbsp;JOBS Explorer with the    moduleDatabase Name of the currently connected job database is displayed in this row. The pull-down menu provides tools for managing job databases. See  Project Jobs are organized into Projects - virtual containers which help you organize your jobs. Create your own project or use the default one. You can have projects named e.g. “cell proliferation”, “translocation”, etc. or name it by its author e.g. “Catherine”, “John”, or for example use dates as project names.However, since you can display results of the whole project in a single result view, it may be a good idea to keep similar jobs together. For instance it may be useful to have a 5hour and a 10 hour proliferation jobs together in one project, but it may produce some complications to put 6-well-plate data together with a 96-well-plates.Jobs   Results open Results of the selected Job.Run This area aggregates predefine job templates which are executed by clicking on their button.List of job runs Jobs Explorer shows the contents of one project at a time as a list of jobs. Each job can be unfolded to display all instances of when it was run - so called Job Runs. Single jobs (in bold type) represent the definition which can be viewed and modified in the job editor, wizard or run time. The run instances can be opened to reveal a view on acquired images and data. Database A path to the database file used to store JOBS. Use the right-most pull-down menu to manage the database. See  . New Adds a new project. The Project window appears. Enter name of the new project and/or a custom description. Properties Enables changing the information of the active project which were set in the Project window. Projects (if not active) can be deleted here by clicking Delete. Results (Project),  Results (Jobs) Opens the results window of the selected project, job or job run. New (requires:  ) Adds new job to the active project. The Job Definition window appears. See   for details about creating new jobs. Edit (requires:  ) Opens the job definition window and displays the selected job to be edited. Delete Selected Job Removes the selected job.Add New Job from Template (requires:  ) NIS-Elements comes with a set of job templates. This button opens the Job Templates window where you can select one of the templates and load it to the JOBS Explorer. The predefined template cannot be overwritten however if changed, it can be saved to your new Job or exported to a .BIN file. Import from File (requires:  ) This button imports a previously saved job definition into your current project.HCA Fixed, HCA Live, HCA Fixed with WPL Executes a job optimized for high content analysis of fixed/live samples. A simple wizard appears. See  . Order by The jobs in the list are ordered by Date by default. Within this button, you can add a second field for job sorting. Show/Hide Jobrun Details This button displays/hides job details such as the Jobrun Description defined in the Job Run Properties window accessible from the context menu over the job run. Run Job This icon appears next to a selected job and is used for its execution.... This button opens the Quick Start Toolbar Configuration enabling to adjust the JOBS Explorer panel. System templates can be inserted into the buttons area by checking their check box. Also Templates, Existing Jobs or jobs from a File can be inserted and arranged in the window by dragging&amp;dropping. To remove a button, drag it outside the white area. Once an empty job is created or an existing job is opened, the job definition window appears.  Figure&nbsp;956.&nbsp;Job Definition windowThis window is the main place for the Job program flow and for changing task parameters. Apart from the main toolbar there are two main areas available:The leftmost task palette All tasks available are displayed here in the form of icons (or icon+description). Expert mode button switches between the simple (basic tasks) and advance view mode (all tasks). Tasks are organized into logical groups. Some tasks depend on the presence of some devices. They may not appear in the palette until that device is added into the device manager (  ). To insert a task to the job, drag its icon into the main area which forms the job definition. Width and thus the number of the icons (tasks) in every row in the task list can be set to any number of icons by sliding the right border of the task list. The Search box can be used to quickly find a task after typing its name.The main program area containing the job definition This pane holds the tasks that form the program. Each task has an icon and text that describes what the task will do when executed. Every task has different parameters which are revealed by double-clicking on its caption. Tasks can be marked as to be shown in the Wizard or at runtime. All tasks which were dragged here will be run one after another until the job is finished.Before thinking about single tasks you have to decide whether you want to modify a predefined template ( ), use the assistance of the definition wizard ( ) or create a new Job from scratch ( ).OperationMost of the editing operations are done by moving the Tasks around. By simple dragging and dropping the tasks from the left to the right you create your complete Job sequence in a matter of minutes.To insert a task into the job program, drag the icon to the main program area which contains the job definition.To move a single task or multiple tasks to different place, hold the task in the main area and drag it to another place in the program.To delete a task from the program area, simply drag it out of the job main program area.To set parameters of each task, double-click its caption. Some tasks depend on other and their order of execution needs to be correct. Please see   to avoid error warnings when setting up your Job.  Rows,  Icons,  Hidden These buttons change the appearance of the task panel. It is possible to switch between the Rows + description view mode and Icons view mode. In the Icons view mode it is possible to change the width of the task panel by dragging the right border line of the scroll bar. The task panel can also be fully hidden by clicking on the Hide button in the middle of the  .Expert mode Expert mode in JOBS displays all available tasks. By default, this mode is off and the Job Definition window shows only the basic tasks. Save Changes Automatically saves any changes in the Job Definition. Save As Saves the Job Definition with desired name into a Project that could be then selected from the  . Undo Changes,  Redo Changes Moves back and forth through the history of changes. Export to HTML or Bitmap Copy Bitmap to Clipboard Copies the definition screen shot of the current job including key parameters into clipboard.Save HTML to File... Saves the definition of the current job into a .html file.Copy HTML File to Clipboard... Copies the definition of the current job into clipboard. Import Imports an existing job definition from a .BIN file or from clipboard. Export Exports the current job definition to a single .BIN file or copies it to clipboard. Properties This button opens the Job Properties dialog window where the following features related to the current job can be set.Job description In the Job description edit box a description of each job can be entered and saved together with the job. This description is displayed in the caption of the Job Wizard and Job Execution Progress window.Progress window Progress window section enables the user to Show/Hide the progress window during the job run or to preview image in the progress window after the first capture or on the job start.When Job definition changes When Job definition changes section defines what is done when changes in the job definition are made. These changes can be saved to the current job definition, saved as a new definition or the user is asked to specify the location where to save them.Treat adjustments in Wizard as Job change should be checked if it is requested to apply the changes made in the wizard directly to the job definition.Wizard Wizard section specifies when the Run button is shown in wizard - always or Enable only if all tasks are visited (highlighted green).Advanced Features Advanced features section groups advanced features such as enabling the Test Run, Parameters or Programmable Actions in Wizard.Results Results section enables the user to open the results view after the job is finished (Show Job Results after execution).Storage Storage section specifies the file format used to store images created by a capture task (  ) for all captures inside the current job. The recommended (and default) setting is ND2 (the system tries to save images from different loops to a single file if possible). If the captured data are to be processed by other software, set the format to TIFF (Single or Multi-page). Compression defines the amount of compression for the selected file format. Color Channel Data specify a color channel storage method - Pixel or Plane. The default is per plane. Generate Multi-Resolution specifies whether to use multi-resolution creation per job basis ((requires:  )). Save OME Metadata OME metadata can be saved with the TIFF file to increase the TIFF file compatibility (e.g. proper channel designation).See also  . Edit Wizard This button opens the Job Wizard edit dialog window. Please see:  . Parameters Each job can be parametrized on the task level. This button opens the Job Parameters dialog window where you can define the name of your parameter, type, values it takes and whether it requires another parameter from the list to be set to a certain value. You can also mark each parameter to be shown in wizard/runtime. For more information about using parameters, please see:  . This button is visible only if Parameters are enabled inside Job Properties. Run Job Executes the current Job. Jobs Toolbar (  ) has some common features with  . It enables organizing, editing and running single Jobs, Job Wizards or Test Runs.A job can be added to the toolbar from within the    window: right click on the job name and select Show on Toolbar or just drag and drop a Job from Jobs Explorer. Figure&nbsp;957.&nbsp;Jobs Toolbar with HCA templates ",
     id: 101 }, 
   { title: "Analysis",
     xmlid: "id|jobs.run.analysisx",
     content: " A simple analysis (or a complex one) can be performed either:Inside a job using  . E.g. to make a decision what to do next based on the analysis results - see:  . Figure&nbsp;1011.&nbsp;Analysis task (Cell Count) used inside a jobOn data acquired by a job (see:  ).On any opened image (see:  ).There are several predefined analysis types. No matter what type you choose, the top part of the definition window is the same for all of them: Figure&nbsp;1012.&nbsp;  Capture This button acquires a single image according to the current Capture definition settings. Use this button to preview results of your analysis setting. By altering your parameters (such as preprocessing, thresholding, ...) a binary image is modified within the captured image. Wellplate Navigation Opens the Wellplate Navigation  window where you can overview your well plate and navigate to a desired well stage on a selected well plate. Save Recipe This button saves your recipe to the recipe database. The recipes can then be managed via the    command. Load Recipe This button loads your recipe from the recipe database. Import This button opens a dialog for importing recipe files from your hard drive. Export This button opens a dialog for exporting recipe files to your hard drive. Load Opens the Load Job Recipe dialog window used for loading job recipes. Help Opens the HTML Help about the analysis being set.Analysis Name Analysis name will denote the particular analysis in the result view. Analyses with the same name are considered to be the same and thus available for aggregation. ",
     id: 102 }, 
   { title: "SMTP Configuration for Sending Email/SMS Messages",
     xmlid: "id|jobs.smtp.setupx",
     content: " You can make a running job send you messages via email or SMS (see    and    tasks). To do this successfully, SMTP server and email account settings must be defined correctly. Run the    command to display the following window and fill all its fields. Figure&nbsp;1030.&nbsp; Server Enter the name of your server (e.g. “smtp.mydomainname.com” or “mail.mydomainname.com”).Port Enter the server's port number.Server requires authentication Check if your server requires authentication.Login If the SMTP server requires authentication, fill in the appropriate user account name (e.g. “firstname.surname”).Password If the SMTP server requires authentication, fill in the password for the “SMTPLogin” account.Sender e-mail Type the email address which will be displayed in the “from” field (and also used as the “reply-to” address).Your name “Alias” of the sender email - usually a human-readable equivalent of the email address. E.g. “Peter Smith” for an address such as ps001@company.comTest Account Settings Click this button in order to test your SMTP configuration. ",
     id: 103 }, 
   { title: "Tasks",
     xmlid: "id|jobs.tasksx",
     content: "                  A detailed description of each task can be found here. ",
     id: 104 }, 
   { title: "Creating PFS Surface",
     xmlid: "id|jobs_PFS_surfacex",
     content: " Perfect Focus System Surface, displayed in the XYZ Overview (see:   ) can be defined using a job. Following example shows PFS Surface definition on a well plate. Figure&nbsp;1081.&nbsp;   task defines settings(Optical configuration) to be used by the   task. On each well (  ) of the well selection (  ), the stage moves to the center of a well (  ), turns on PFS (  ), automatically finds the PFS offset value (  ), assigns it to the each well (  ) and turns PFS off (  ). Once all wells undergo this procedure, the PFS Surface is finally created on the well selection using the    task. ",
     id: 105 }, 
   { title: "Using Conditions",
     xmlid: "id|jobs_introduction_to_conditionsx",
     content: " Condition (  ) is a great tool useful in all sorts of laboratory situations where we define which parameters influence the task execution. Condition tasks need to be defined using expressions (see  ). For example: If Expression is TRUE, run the contained tasks...  Figure&nbsp;1064.&nbsp;Example of an expressionDeciding whether the condition is TRUE or FALSE is called “evaluation” and it is computed by NIS-Elements on job runtime.Almost every task produces number of variables which are then available within the    task.Condition use case&nbsp;Let us show you the usage of an condition on a simple job. We will capture a large image in low resolution and measure its mean intensity. Only if the mean intensity is above the defined threshold (which may indicate that there is something interesting on the scene), we will capture the large image again this time using a high magnification objective.Create two capture definitionsDefine two    tasks, each using a different optical configuration. Assign some simple names to the tasks. Figure&nbsp;1065.&nbsp;Capture the preview imageInsert the first    task and set it so that it will use the low magnification capture definition - choose the Preview Capture definition from the Using combo box. Since purpose of this task is to decide whether to capture the high-magnification image or not, there is no need to save the images, so leave the Save Image to Database pull-down menu set to Never or Later. Figure&nbsp;1066.&nbsp;Analysis DefinitionInsert the    task and double-click on it to specify its parameters.  Figure&nbsp;1067.&nbsp;Because intensity of the whole frame will be measured, just select the Full frame option and click OK.Conditional CaptureBy now, everything is prepared to make the decision whether to capture the high-resolution image or not. We captured a preview image, measured its intensity and now we can set the resulting value into the    task and add another    task within. Figure&nbsp;1068.&nbsp;Specify the conditional expression (Condition). If evaluation (result) of this expression is TRUE (i.e. 1), the capture will be performed. Use the Define button to specify the conditional expression as described in ( ). The resulting job could look like this: Figure&nbsp;1069.&nbsp; ",
     id: 106 }, 
   { title: "Using Expressions",
     xmlid: "id|jobs_introduction_to_expressionsx",
     content: " An expression in Jobs is a combination of explicit values, variables, operators and functions (see:  ). It is typically in the form of an assignment (variable = expression) whereby a value is computed (by evaluating the expression on the right side) and assigned to a variable (on the left side). Expression is evaluated and assigned to the variable when the task is executed.Expression use case&nbsp;In the following text we show an example which uses an expression. Imagine that we want to capture random positions on a dish. We want to finish capturing when we gather 3000 objects.First of all we have to define our capture using the   . We choose the optical configuration and insert the    task which we set as follows: Figure&nbsp;1070.&nbsp;Then we insert the    task, define the dish area on the stage and adjust point generating parameters as shown in the following picture. Figure&nbsp;1071.&nbsp;Finally we insert the point loop (  ) and fill it with   ,   ,    and   .Double-clicking on the analysis task we can further define its parameters (see  ).In the task expression we enter the following expression: TotalCells = TotalCells + Job.CellCount.CountInto the    task we define the following condition: TotalCells &gt; 3000. Inside the condition we insert a Break (  ). Thus if the condition is true (we receive 3000 objects), the whole job stops. Figure&nbsp;1072.&nbsp;Loop and its containing tasksThe final job looks as follows: Figure&nbsp;1073.&nbsp;  Figure&nbsp;1074.&nbsp;Job Execution Progress window  Figure&nbsp;1075.&nbsp;Final resultsFor more information about Jobs Progress and Results window, please see    and   . ",
     id: 107 }, 
   { title: "Labels and Metadata",
     xmlid: "id|labels.and.metadatax",
     content: "     Any color/text labels and any metadata can be assigned to each well of your well plate. Label definition/assigning and metadata assigning can be made inside the HCA template or directly in the    task. Figure&nbsp;1013.&nbsp;Labeling and Dosing window.  Open the Manage Custom Metadata window (   to create new metadata. Figure&nbsp;1014.&nbsp;Data Items tab New Creates new custom metadata set. Save Saves the current custom metadata definition using the name on the left. Import Enables the user to import custom metadata definition from a .xml file or reuse them from an existing image file. Export Exports current or all metadata definitions to a .xml file. Remove Removes the current metadata.Presets tab&nbsp;This tab is used for setting up metadata sets (grouped metadata of different types). In the left list, single sets are created, arranged, renamed or deleted. On the right side of the dialog window, values of individual items are set. Predefined presets can be used in the    task.Defining metadata&nbsp;Create a new metadata set - click on   New and give it a name. Click   Add New and name your first metadata, choose its type, enter a text description, edit its properties, choose the default value and select whether it is mandatory or not. Add as many metadata as you like and confirm their definition by clicking OK.Setting metadata&nbsp;Click Define... in the    task or in the Sample Labeling tab of the HCA template to open the Labeling/Quantities window (closely described here:  ). Click   Custom Metadata... to open the Select Custom Metadata window where you can choose the metadata set from the first combo box and select (by a check mark) which metadata will be used in the current job. Make your selection and click OK. In the wellplate table make a well selection of wells to which you want to assign the chosen metadata. To assign metadata to selected well(s), right-click on the selection and choose Set Custom Metadata and confirm your metadata selection or click    on the right side of the metadata name listed in the top right corner of the window.Once you are done with adding labels/metadata, click OK to get back to the    task or Sample/Labeling tab of the HCA template. You can save your label as a .xml file by clicking the  Save to file button. You can also import your saved labels by clicking the  Load from File button. Clear your labeling any time by clicking the  Clear button. After the HCA/Job containing labels/metadata is executed and finished, the Job Results window opens. Click   Show Labeling to display labels over wells. Labels of the wells selected in the wellplate area are highlighted in the labels toolbar (invoked by clicking   Show Labels). Any label can be temporarily turned off by unchecking the check box next to its name.To assign prepared labels and metadata to the wells displayed in the result view, copy the metadata inside your spreadsheet application and click   Import Metadata in the results view. Please see   for importing prerequisites. Figure&nbsp;1015.&nbsp;Job Results View displaying well labels Two types of formats are supported for importing custom presets into the Labeling/Quantities dialog window. Labels and quantities have to be defined similarly to one of the examples below.Plate View Figure&nbsp;1016.&nbsp;Plate View identification and definitionFirst row defines the plate (its name, name meaning, hotel). Next row contains the value type (Label, Numeric Value, Short Text, Long Text, Selection, Date), value name (empty for Label) and Unit (empty for Label). The values are then typed in the following rows as they appear in the well plate. Figure&nbsp;1017.&nbsp;Example of a Plate View table formatList ViewThis view is used to label and quantity features of each well by a single row record. First four rows (Variant A) or three rows (Variant B,C) specify the hotel position (optional), identify the plate, position of each well on a plate, value types (Label, Numeric Value, Short Text, Long Text, Selection, Date) with value names and units. If the Plate name meaning value contains an empty string “”, then the name of the plate is used for plate identification. WellIndex and Plate Position identifiers are considered to start at 1.Variant A&nbsp; Figure&nbsp;1018.&nbsp;Variant A: list view identification and definition  Figure&nbsp;1019.&nbsp;Variant A: example of a list view table formatVariant B&nbsp; Figure&nbsp;1020.&nbsp;Variant B: list view identification and definition  Figure&nbsp;1021.&nbsp;Variant B: example of a list view table formatVariant C&nbsp; Figure&nbsp;1022.&nbsp;Variant C: list view identification and definition  Figure&nbsp;1023.&nbsp;Variant C: example of a list view table format Both row and column well localisation can be described using numbers starting from one (1,2,3, ...) or by letters (A, B, C, ...). For example well “B3” can also be described as well “23”, “2C” or “BC”.Linking with   &nbsp;Imported labels/quantities can be linked based on position of the well plate in a hotel (Linked by Position) or using a barcode (Linked by Barcode). Plate # selection among multiple hotels has to be unique, otherwise the linking is not performed. E.g. if plates #1 and #2 from hotel 1 and plates #3 and #4 from hotel 2 are selected in the   , labels marked with hotel number 1 to 4 are linked correctly. Whereas if e.g. plate #1 is selected in both hotels the link for plate 1 is not created and a new preset named #1 is created instead. Single wells or single points can be easily labeled with a color and a name during runtime. Use tasks    (see:   ) and    (see:  ). ",
     id: 108 }, 
   { title: "Manual Corrections Before Measurement",
     xmlid: "id|manual.corrections.interceptx",
     content: " In the Direct Mode, it is possible to make manual corrections on the intercepts. Once the measurement starts, the following buttons appear on the left toolbar: When the measurement starts while the mask is turned on, it overlaps the intercepts. In such case, please switch the mask off by the    tool to see the intercept layer only.     Figure&nbsp;1274.&nbsp;Select the    tool, click on an intersection between two intercepts to remove the intersection (yellow example).click on an intercept to create a new intersection, thus creating two intercepts (red example).This tool significantly speeds up the correction process as the operator can focus on controlling the curves without having to think about which tool to use. Colors of all intercepts are refreshed after each change.    Figure&nbsp;1275.&nbsp;Intercepts delete. Left click on the intercept deletes it from the intercept layer leaving an empty space instead. Pseudocolorization refreshes after separation. ",
     id: 109 }, 
   { title: "Manual Corrections Before Measurement",
     xmlid: "id|manual.corrections.planimetricx",
     content: " In Direct Mode it is possible to make manual corrections on the grains / intercepts binary overlay image. Once the measurement starts, the corresponding buttons tied with the selected method appear on the left toolbar:    Figure&nbsp;1264.&nbsp;Manually drawn curve separates two grains. It must have two intersections with the grain boundary – one intersection is ignored.    Figure&nbsp;1265.&nbsp;Manually drawn curve connects all the grains the line goes through or touches.    Figure&nbsp;1266.&nbsp;Left click on the grain connects the touched grain to the best neighbour using internal algorithm.    Figure&nbsp;1267.&nbsp;Left click on the grain connects the touched grain to the best neighbour using internal algorithm. ",
     id: 110 }, 
   { title: "3D Measurement",
     xmlid: "id|measurement.3dx",
     content: " (requires:  )2D binary layers on a Z-serie can be connected to 3D objects. Spatial characteristics of these objects can be examined within the    window. This window allows you to create 3D binary objects, measure their features and export the results (e.g. to MS Excel). Which features are displayed in the columns can be determined using functions from the context menu over the measurement table.See also  . Figure&nbsp;546.&nbsp;3D Object Measurement Window Options Define threshold 3D You can define the threshold to create a 3D binary objects according to the color channel and/or the size of objects. The appearing window also provides the possibility to use Smooth, Clean and Fill holes functions, as well as to highlight or choose just one object. Figure&nbsp;547.&nbsp;Defining 3D Threshold Connect 3D Objects Create 3D objects by connecting existing binary layers.All Objects Shows all the objects in all the Z-stacks.Current Z-Stack Shows only the objects of currently selected Z-stack. Delete Selected Object Deletes selected object in the measurement table. Invert Selection Inverts the selection of the objects in the image as well as in the table. Use filter Activate this button to filter the 3D results. The filter is defined by clicking the   Define Filter button. Define Filter This button displays a table where the filters are defined and shown. Check the first check box to enable the filter, then choose the AND/OR condition, filtering Column, set the Comparison mode and enter a custom Value. Add as many filters you like using the check box in the first column. Move the filters up/down using the arrows  . Delete the filter record using the  button at the end of the filter row. 3D Object Measurement to Excel Allows you to export the measurement results to Excel. Refresh Measurement Refreshes the measurement results. Help Opens this help page. Context menu over a selected rowHighlight Objects Flashes the selected object in the source image so that it can be easily found amongst the others.Select All Selects all available objects.Unselect All Clears the object selection.Invert Selection Inverts the object selection.Delete Selected Deletes the selected objects.Keep Selected Only Deletes all but the selected objects.Set Object as Reference Object Selected object is set as a reference object. Distance from this object to others can be measured using the    feature. Context menu over a column headerHide Column Hides the selected column.Show All Columns Displays all available measurement features as columns in the table.Show Column Can be used to manually add measurement features as columns in the table.Show Default Columns and Order Resets the table columns into their default state.Define Visible Columns... Opens a dialog window used for defining which measurement features are shown by default. Figure&nbsp;548.&nbsp;Defined 3D objects ",
     id: 111 }, 
   { title: "Measurement Explorer",
     xmlid: "id|measurement.explorerx",
     content: " Opens the Measurement Explorer organizing the previously defined measurement sequences (see   ). The explorer list shows the measurement sequence definitions in bold and the finished measurements under each definition (click on the drop-down arrow). The last used definitions (either edited or executed) are listed at the top.Double-clicking on the measurement definition opens the definition in the Measurement Sequencer - Definition panel. Double-clicking on the measurement result opens the result in the Measurement Sequencer - Run panel and opens the measured image (if available).Multiple items can be selected inside the Measurement Explorer. For example, two and more measurement results can be selected (using Ctrl) and exported to the report (using the context menu) one after another. Similarly, multiple definitions can be selected and exported, duplicated or removed from the list at once.User Input Required in the context menu over the measurement definition opens an input dialog box after each finished measurement so that the user can enter the sample name. This name can be later used to filter the results. Figure&nbsp;1229.&nbsp;Measurement Explorer Create New Opens a drop-down menu where the user can select which type of new measurement will be created. The first one - Measurement Sequencer opens the Measurement Sequencer - Definition window used for creating a new sequence definition (see  ).The other options utilize the Layer Thickness Measurement dialog (  ). To rename a definition in the Measurement Explorer, use the context menu over the definition name. Run The selected measurement sequence is opened in the Measurement Sequencer - Run ( ) panel where it can be executed. Edit Opens the selected measurement sequence for editing in the Measurement Sequencer - Definition ( ). Open Result Opens the image on which the selected measurement was performed and shows the measurement directly inside the image. Open Document Opens the image of the selected measurement run. Export to Report, Export to Excel, Export to File, Export to Clipboard Exports the measurement data into a Report, Excel, File or Clipboard. Click on the black arrow next to the button to select the exporting method. Then click on the button itself to execute the selected export.When exporting using the Export to Report method, a new window opens showing the report which can be printed or saved as a .pdf or .html file. Report can also be created by clicking on the measurement name in the list and selecting   Generate Report in the context menu.To export measurements into a .csv file, select Choose File..., navigate to a place where the file will be saved, name it (File name), click Save, select Export to File from the drop-down menu and click on the Export to File button to execute the export. Show Data Adds a table containing measurement results for the current measurement selection. Show Graph Displays a time chart summarizing the currently selected measurement. By holding down Ctrl and moving the mouse wheel zooms the x axis, while holding down Shift and moving the mouse wheel zooms the y axis. Show Groups The drop-down arrow next to this button is used to create group(s) and assign any measurement sequence definition(s) into the selected group. The button itself turns on/off the group view. All definitions not assigned to a group are shown in the &lt;ungrouped&gt; group.Start by clicking   Create Group and name the group (repeat this for multiple groups). Then reveal the context menu over selected definition(s) and click   Assign to Group and choose a group where it will be placed. To remove a definition from a group, choose   Remove from Group. To delete a group without affecting the definitions inside, click   Cancel Group. To change the order of multiple groups, click   Reorder Groups and use the   arrows to move the selected group up or down and then click OK. To rename a group use   Rename Group. Sort by Date Sorts all measurement results by their date and groups them by month. Sort by Type Groups and lists all measurement results by their measurement sequence definition. Date Filter Turns the date filter on/off. To set the date filter, click on the black arrow next to the icon and specify the filtering interval in the left and right calendar (). Configuration... Opens the Location dialog window where a new database can be created, copied or located. Also path where the data are stored can be adjusted. Import/Export from File Imports/exports the measurement definition(s) or the settings (current explorer settings with all its definitions) from/to a file. Select the importing/exporting option from the drop-down menu. Remove Deletes the selected measurement sequence definition with all of its results or the selected measurement. Show Statistics Shows the complete history statistics of the currently selected measurement. Resize Columns to Content Resizes each column of the table so that it fits the content inside of it. Graph Settings Opens the Graph Settings window used for assigning features to the graph axes. Select a feature and use the “&gt;” and “&lt;” buttons to move it to the selected column. Reset Zoom Resets the zoom of the graph to default. Auto Scale Automatically adjusts the scale of the graph. Export Options Clicking on the arrow reveals a drop-down menu enabling the user to select to which format the results will be exported. Clicking on the button itself performs the export. All of these export options are also available from the context menu over a finished measurement.Weld Presets&nbsp;If the Weld templates check box was selected during NIS-Elements installation, blue weld presets will be present in the list. These presets represent typical measurement use-cases for a spot weld, plate pole weld, lap weld, fillet weld, butt weld, tee joint, lap joint, butt join, and for hardness measurement (Vickers, Knoop, Brinell). Blue presets cannot be edited or deleted but it is possible to edit their duplicated copy (use the context menu on the weld name).Each definition contains a chain of measurement tools arranged to measure the weld “fuse leg” parameter. Fuse leg is calculated as a percentage (fraction) of the measured distances and can be changed in the   Custom Expression. See the table below showing where to change the calculation defining the fuse leg position for each fillet (5; 0.2; 10; 10; respectively). Table&nbsp;16.&nbsp;Fuse leg calculation for the blue weld presets.Weld typeCalculationFuse leg percentage valueSpot{% raw %}Math.min(ThicknessTop.result,ThicknessBottom.result)/5{% endraw %}20%Plate Pole{% raw %}T1.result*0.2{% endraw %}20%Lap{% raw %}T2.result/10{% endraw %}10%Fillet{% raw %}Math.min(T1.result,T2.result)/10{% endraw %}10% ",
     id: 112 }, 
   { title: "Measurement Panel",
     xmlid: "id|measurement.panel.refx",
     content: "    Measurement ,    View &gt; Analysis Controls &gt; Measurement  Measurement Presets Planimetric, Linear, Circular, Abrams Each method determines the shape of the test mask.Save As This button can save the current preset under a different name.Properties You can modify the name and the sample image of the current preset by selecting this button.Delete This button deletes the current preset.Test Mask Press the    button on the left tool bar to display the mask. Depending on the selected preset, various parameters such as number of lines, circle diameter etc. can be adjusted. Also the mask can be resized and moved by mouse.Fit to screen Check this box to maximize the test mask to the whole image area. Some edit boxes get disabled then.Fit to Centre Press this button to move the whole test mask to the middle of the image area.Default Size Press this button to reset the user settings and resize the test mask according to the standard.Standard Select the standard required by your methodology.Restrictions In special cases, it is possible to apply restrictions to the measured objects and exclude the grains based on their EqDiameter (Planimetric method) resp. Length (Intercept methods) value. This option can be used as an alternative to the Delete grains smaller than button in the    panel.Save, Cancel Whenever the settings are changed, these buttons becomes enabled. You can always use the Save button to store the changes to the current preset, or the Cancel button to discard them. ",
     id: 113 }, 
   { title: "(Not) Merging Z-Stacks Different for Each Point",
     xmlid: "id|merging.different.zstacksx",
     content: "    inside    inside   .   and    are used to define different Z-stacks for each XY point.Result: multiple TZ files, different Z-Stacks cannot be merged in one ND2 file. Figure&nbsp;984.&nbsp; Figure&nbsp;985.&nbsp; ",
     id: 114 }, 
   { title: "Comparative Method",
     xmlid: "id|metalo.comparative.methodx",
     content: "       ",
     id: 115 }, 
   { title: "Detection Panel",
     xmlid: "id|metalo.detection.optionsx",
     content: "  Figure&nbsp;1231.&nbsp;Once the Cast Iron application is run, the Detection tab appears in the right-side docking pane.Select one of the detection presets. This selection influences the set of available detection tools.Select Structure TypeTwo different structure types are pictured on the buttons, cast iron and ferrite/pearlite.Save As This button can save the current preset under a different name.Properties... You can modify the name and the sample image of the current preset by selecting this button.Delete This button deletes the current preset.Select the detection type. One of the easy modes can be selected, or an arbitrary sequence of operations can be created.Detection TypeThe following detection types are available according to the selected structure type:Simple - a predefined detection procedure. The procedure steps depend on the selected structure type.Advanced - defines a custom sequence of commands within the pre/postprocessing portions of the detection procedure.Macro - loads an external macro which would perform the whole detection procedure including threshold.Enable/disable the pre/post-processing operations, and adjust the Threshold settings.Preprocessing&nbsp;Preprocessing adjusts the image in order to enhance the important image features (object borders, etc.) so the afterward thresholding would be reliable. The following preprocessing operations are available according to the selected Detection type. Click the Correct background inhomogenity button to include the operation in the detection procedure.Threshold&nbsp;Specifying correct threshold limits is a crucial task of image analysis. The point is to determine, which pixels will and which will not be included in the binary layer, and thereby distinguish analyzed objects from the background.Postprocessing&nbsp;Binary image postprocessing aims to clean the binary image from any unwanted noise and other inaccuracies. The following binary layer postprocessing operations are available according to the selected Detection type. Click the corresponding icon to include the operation in the detection procedure.Clean - deletes the smallest binary objects of the binary layer.Use the Save button to store changes of the current preset, or use the Save As button to add the current settings as a new preset, then press the Save button to store the setting.Save, Cancel Commands&nbsp;Whenever the detection settings are changed, these buttons becomes enabled. You can always use the Save button to store the changes to the current preset, or the Cancel button to discard them. ",
     id: 116 }, 
   { title: "Applications",
     xmlid: "id|modsx",
     content: " Table of Contents                                                                                                                                                                                                                                                                                                                                                                                 ",
     id: 117 }, 
   { title: "Molecule Analysis",
     xmlid: "id|molecule.analysisx",
     content: " (requires:  ) The Molecule Analysis panel is included in the software as a trial version. Local Option must be installed to display it. Figure&nbsp;1304.&nbsp;This dialog window is used for localizing molecules.STORM AnalysisThis tab is used for defining parameters influencing the molecule localization. Import Imports molecules that were exported from NIS STORM (.txt, .bin) either to the Current image or to a New image. Export Exports the molecules to a text file (ASCII), M425 BIN (.bin), N300 BIN (.bin), ND2 (.nd2), Frame lists (.txt) or Trace lists (.txt). Advanced options export the detected molecules data without the Z coordinate (Export without Z coordinate (flatten)). Clear Deletes all detected molecules. Batch Analysis Opens the Batch STORM  dialog window. For more information please see  . Z-Calibration management Opens the Z-Calibration dialog allowing the user to view and/or change current calibrations used in determining the Z location of all molecules identified during STORM Analysis. For more information please see  . Molecule Drift Correction If the sample moves in time during acquisition, this function can compensate for the drift. For more information please see  . XY Warp Performs XY Warp calibration using information extracted from a multi reporter Z Calibration file. For more information please see  . Configure/Launch cross talk analysis Opens the Crosstalk Options setting the Cross Activation and Search Radius (nm).Cross Activation coefficient value at the top is used for probability calculation using constant activation power method. Default empirically determined value of 0.1 is valid for combination of 460 nm (Cy2) and 532 nm (Cy3) activation laser wavelengths. For other wavelengths (488 or 561) this value needs to be calibrated.Crosstalk subtraction is a software technique that enhances the multi-channel STORM image appearance by reassigning the channel membership for (an ideally small) subset of molecules in overlapped areas based on local probabilities. For each molecule the software calculates probabilities of it not belonging to its current channel.There are two types of crosstalk:Tandem dye pairs tend to spontaneously blink simply under reporter wavelength excitation without any activation. This phenomenon contributes localizations to Non-Specific Activation channel (further NSA) when fluorophores self-activate in frames that don't immediately follow activation (frames 2, 3 and so on). When the same happens in frame 1 the software can not immediately distinguish between specific and non-specific activation. All localizations registered in frame 1 are initially assigned to its corresponding specific channel. During crosstalk calculation software can evaluate the (per frame) local rate of non-specific blinking and then statistically remove molecules based on ratio of local density between given specific channel and NSA. This kind of crosstalk removal is a mandatory part of any subtraction.The second kind is called Cross Activation. It occurs between certain types of activators and activation laser lines. For example Cy3 designed to be activated by 532 nm laser can be accidentally cross activated by 457 nm laser typically used for activating Cy2. This results in some Cy3 molecules appearing in the imaging frame following 457 nm activation. Once again there is no immediate way for software to recognize this during STORM analysis so these cross activated Cy3 molecules are logged as Cy2. Crosstalk probability algorithm calculates the odds of any registered Cy2 molecule being indeed a cross activated Cy3. Similar to NSA cross activation probability is calculated based on local densities around given molecule. It is also affected by the ratio of activation laser power between 457 and 532 nm laser lines. The probability calculation assumes that activation laser powers remained constant during acquisition. Cross activation subtraction is only available for data sets or their portions where this assumption holds true. Laser power changes are tracked and recorded in metadata during acquisition. Use period slider and buttons tool tips to find a data set range with constant activation laser power ratio. Cross activation crosstalk typically holds a much smaller share in comparison with NSA crosstalk.Since NSA crosstalk is usually dominant and mandatory to remove the cross activation crosstalk removal is optional, but (if selected) always applied in combination with NSA subtraction. For such combined removal calculated probabilities (NSA and Cross Activation) are summed for each molecule. Molecules removed from their original specific channels are placed into a temporary “Non Specific Activation-Xt” channel. Ultimately they are destined to general NSA channel once the results are confirmed.Cross talk subtraction is not available for data acquired using spontaneous activation. The criterion for launching crosstalk probability calculation at the end of STORM analysis is the presence of NSA molecules.Since crosstalk probabilities calculation relies on molecules’ XY positions it is highly recommended to perform crosstalk subtraction after drift correction. For 3D data sets Z position is also considered by default but can be optionally disabled.Crosstalk probability calculation is based on local densities around any given molecule. Local densities are essentially counts of neighborhood molecules. The choice of neighborhood size is a tradeoff between the resolution and statistics accumulation. Naturally the neighborhood is round as we track Cartesian distance. If the radius is large the resolution of resultant image is degraded in the crosstalk subtracted areas. If on the other hand the radius is too small the neighbor molecule counts quickly become insufficient for statistical analysis. Crosstalk Settings dialog maintains a list of search radiuses to be tried for probability calculation. Crosstalk Settings dialog is mutually exclusive with Crosstalk Subtraction dialog and is available via the  button only when no data is loaded.Default radius choices are 25, 50, and 75 nm. Any of them can be removed and new values can be added. To add a new value type it in the edit box on the left and press the &gt;&gt; (add) button or press Enter. To remove one or more of the existing radiuses highlight them in the list on the right and click &lt;&lt; (remove) button or press Delete key. Multiple selection follows standard Windows convention with Ctrl and Shift. To replace one or more highlighted list values with a new one typed in the edit control hold Shift while clicking the &gt;&gt; (add) button. The software does not allow accepting the changes while the list is cleared. At least one radius is mandatory.Duplicate entries are not allowed. An attempt to add a duplicated entry is ignored. Radiuses less than 5 or more than 5000 nm are invalid. The following validation warning enforces the range. Large radius values as well as extra number of different radius values increase the probability calculation effort. There is no hard limit to the total number of different radius values but the following warning is displayed when it exceeds three.Standard Gaussian Fitting/Fit Overlapping Peaks Select a localization method suitable for your sample.Channel tabs Localization parameters can be set either for each channel separately (switch to the proper channel tab) or they can be taken from the current channel and applied to all channels by clicking   Apply channel settings to all channels. Replaces the old molecules with the newly detected. Keeps the old molecules and appends the newly detected.Min Height Intensity of the smallest (dimmest) peaks (minus the local background of that peak) to be identified as molecules. Any object whose peak intensity is below this value will not be identified.Max Height The intensity of the largest (brightest) peaks to be identified as molecules. Any object whose peak intensity is above this value will not be identified.CCD Baseline Closed shutter (zero photons) pixel response. For Andor DU-897 and for Hamamatsu Orca-Flash 4.0 cameras it should always be 100.Min Width (nm) The smallest possible width a spot of some intensity in the image can have to be identified as a molecule.Max Width (nm) The maximum possible width a spot of some intensity in the image can have to be identified as a molecule. A suggested default value for 2D is 400 nm. For 3D STORM, this should be slightly larger to take defocused molecules into account (molecules above and below the focal plane). A suggested default value for 3D is 700 nm.Initial Fit Width (nm) This value is used as a starting point for STORM Analysis for identifying molecules. For 2D STORM, this is the expected value of a diffraction limited spot. A suggested default for both 2D and 3D is 300 nm.Max Axial Ratio Fluorescent spots whose ratio of elongation in the X and Y direction is larger than this threshold will be rejected as a single molecule. It is typically set to 1.3 for 2D STORM and 2.5 for 3D STORM.Max Displacement (pix) This value sets the maximum distance (in pixels) that a molecule identified in one frame can be located from a molecule identified in the previous frame to be considered the same molecule and arising from the same activation frame. The default value is 1.Load Defaults Enters default values into all edit boxes of this dialog window.3D This function is used to analyse the molecule image as it contains 3D molecules. Different parameters than in 2D are set and a request for switching to the recommended settings is shown.Auto Minimum Height If checked, this function performs automatic min height detection during analysis. When auto min height is selected the software runs a short (100 periods in the middle of selection range) pre-analysis with relaxed identification constraints to determine the optimal min height for a given data set. This feature is not available when a Z Calibration data set is loaded.Auto Fit ROI Check this checkbox to allow fitting algorithm consider variable number of pixels depending on width and axial ratio of each individual peak. When this check box is cleared a fixed size pixel rectangle (typically 5 x 5 pixels) is used for fitting. Auto Fit ROI is recommended for 3D analysis. Fixed size ROI is recommended for 2D analysis.Drift Correction Captured sample may sometimes slightly move in one direction (e.g. due to temperature). Turn on this feature to calculate the drift.Auto Preview Preview is calculated right after any parameter change.Preview Calculates the analysis and shows the result on one frame. When zooming in the molecule image, use    to see the position of the currently observed area. Process All Frames Performs the analysis on all frames and shows the result. The following progress window appears. Figure&nbsp;1305.&nbsp;Detection Progress window Display peak candidates Displays the “peak candidates” - peaks detected in the image, which were evaluated as invalid (not being molecules). Display peak values (if possible) in graphs Shows the number of detected molecules in each vertical bar. The number is shown only if it fits inside the bar. Keep auto Zoom Turns on the Y axis auto range so that the graph shows data in the optimal scale. If the auto zoom is turned off, the Y axis can be zoomed. Point the mouse over the black area of the graph and use the mouse wheel to zoom in/out. Place the cursor over the X or Y axis numbers and use the mouse wheel to zoom the specific axis separately. If multiple bar charts are shown in the Detection Progress  window, their X axis zoom is automatically synchronized.Detection Properties Displays the main molecule detection properties (described above).Adjust Laser Powers Adjusts power of the connected lasers during the acquisition process.Cancel Immediately stops the detection process and discards all analysed data.Finish Finishes the detection process without analysing all frames. Table tabThis table displays each molecule and its measured features as a row record. Clicking on a row record highlights the selected molecule in the image. If the molecules are grouped by a selected feature (context menu over the column caption &gt; Group By), each row represents the group which can be expanded using the arrow button. If filtering is applied, only the filtered molecules are shown in the image and table. An information text “Molecules total / image” is shown at the bottom of the dialog window. Show statistics Displays an additional table where overall column statistics are displayed. Use Filter Activate this button to filter the results. Define Filter This button displays a table where the filters are defined and shown. Two types of filters can be seen in this filter definition.Enabled filters - filters created directly in this definition area. Click   &lt;select&gt; to set a feature for filtering, choose the AND/OR condition and set the Comparison mode and enter a custom value.Disabled filters - filter defined in the Molecule Options dialog window (  ). The filter is automatically added based on the currently viewed data.  Up/Down Moves the selected filter up/down. Remove Removes the selected filter(s). Data to Excel,  Data to Clipboard Exports all the data to MS Excel or Clipboard.  Expand/Collapse All Expands/collapses the group data. SimpleExport,  AdvancedExport Simple Export directly exports all the data to a .txt file while the Advanced Export selects which features are exported. Use data from entire image Data are shown from the entire image. Use data from probe Data are shown only from inside the probe. Use data from ROIs Data are shown only from inside the ROIs. Create molecules snapshot Creates a new image containing just the viewed molecules without a background. Any defined filters are taken into account. To render a snapshot of the currently viewed molecules with a high magnification, use   . Create View Snapshot from Molecules Opens the Create View Snapshot from Molecules dialog window. Render Volume To visualize the molecule clumps it is possible to create 3D binary shapes bringing a better idea of the sample structure. This button opens the Render Volume Settings dialog window which sets the parameters of the 3D binary. Image Width  and Image Height  define the dimensions of the 3D area. Calibration is calculated accordingly. Set either the Z Slices or the Z Step and set the Radius defining the size of the binary sphere around each molecule. Multimodal Image Registration Opens the   . Show/Hide Columns Opens the Select items to proceed dialog window where the user can select which measured features will be shown in the molecule Table. The user can also hide a feature by revealing the context menu over the column header and selecting Hide Column. All hidden columns can then be shown again by selecting Show All Columns.Grouped Values This drop-down menu sets the statistics shown for the grouped data. Graph tabThis tab visualizes the table data as a graph. In the drop-down menu select the graph type, set the feature on the X axis and Y axis or Bin counts and click    to generate the graph.  ROI Statistics tabThis tab is shown only if ROI(s) are present in the current image with molecules inside the ROI(s). It provides useful information regarding molecules within the ROI(s). For more information please see  .  Molecule AnalysisThis tab offers several methods used for analysing the previously localized molecules.Add Opens the Pick and define molecule analysis window used for selecting the molecule analysis method and output features which will be calculated and showed in the Filters table inside the    dialog window.Nearest distance Distance to the closest molecule is assigned to each molecule.Average distance Average distance of the nearest molecules is calculated and assigned to each molecule. The number of the nearest molecules is determined by the Count parameter.Count in distance Number of molecules present in the circle around each molecule is calculated and assigned. Define the size of the circle using the Radius parameter.DBScan Using the Density-based spatial clustering of applications with noise clustering algorithm, ID and Size is assigned to each molecule in the cluster. Use the Radius and Minimum count parameters to adjust the clusters.Voronoi Creates clusters from the molecules present in the current image with the specified Maximum distance between the adjacent molecules and the specified minimum number of molecules (Minimum molecules) in the emerging cluster. A unique ID (Clusters) is given to each cluster.Trace Analysis If multiple molecules create a trace (see   Show Traces in the   ), this method calculates the Distance, Speed and Acceleration of the molecules in their track.Check Preview to see the effect of the settings directly in the image on the Probe which is automatically turned on and can be moved and resized. If both the probe and ROI is used in the image, specify in the drop-down menu on which of them the preview is shown. To recalculate the preview click   . To confirm the selected method click ADD.Remove Removes the selected method from the list.Edit Edits the selected method.  Apply Parses the molecule list.For more information, please see  . ",
     id: 118 }, 
   { title: "Molecule Options",
     xmlid: "id|molecule.optionsx",
     content: "  Figure&nbsp;1301.&nbsp;Molecules Options dialog window.This dialog window is used for visualizing molecule images (ND2 files containing the molecule data format). This window can also be opened from the   after clicking the   View Molecules button.VisualizationThis section groups all tools used for visualizing the molecules. Normalized Gaussian Rendering Displays single molecules as circles. Adjust the circles visibility using the LUT control below or by the Auto Scale buttons.See  . Render as Crosses Shows single molecules as crosses. Render Density Map Displays the molecule data where intensity of each screen pixel is the number of molecules that fall into that pixel. The pixel area can be adjusted using the Bin Size slider. Show Traces Displays traces of all moving molecules. Time dimension is required. Render Clusters Molecule clouds (clusters) are displayed in color. Opacity of all clusters can be set in Cluster Opacity... found in the drop down menu next to this button. Drift Correction Captured sample may sometimes slightly move in one direction (e.g. due to temperature). Turn on this function to fix this drift. Chromatic aberration correction Corrects the chromatic aberration issue. Mark molecules identified in current frame If source image data exist in the current molecule image, this function visualizes on which frame each molecule was localized. Black and White Overlay Color channels are mixed to form a black and white image suitable for better molecule visualization. Keep Auto Scale This function applies the auto scale procedure continuously when you move through the time dimension. Auto Scale This function adjusts the slider positions of all channels in the LUT control automatically with the purpose to reasonably enhance the image. Reset LUTs This function discards all LUTs settings. Click  next to the icon to reveal more LUT options. Change Photons per Count is used for entering a known value of photons that actually fell on the camera sensor.See  . Extract Molecules from document Creates a new image with the settings currently set in the Molecule Options dialog. Create View Snapshot from Molecules Opens the Create View Snapshot from Molecules dialog window. For more information please see  .Min. Size Changes the minimal size of the molecule circle (available only in the Normalized Gaussian Rendering mode).Cross Size Changes the size of the molecule crosses (available only in the Render as Crosses mode).Bin Size Changes the size of the pixel area of the density map.Color Mapping Channel Color mode colors the molecules in their appropriate layer using the colors defined in the Color column.Intensity mode colors the molecules using the selected pseudo-color gradient based on their pixel intensity.Lateral Localization Accuracy mode colors the molecules using the selected pseudo-color gradient based on their detection precision. Adjust the minimal and maximal accuracy threshold using the sliders.Depth mode colors the molecules based on their Z position calculated by the Z calibration. Adjust the minimal and maximal depth threshold using the sliders or use the  button to set the min/max values values from the Z calibration.Frame mode colors the molecules using the selected pseudo-color gradient based on their detection in time. To create a custom pseudo gradient, choose Manage Custom LUTs (  ).Min. Color Defines the minimal value (left side of the gradient) from which the colors start being applied.Max. Color Defines the maximal value (right side of the gradient) to which the colors are applied.LayersThe molecule detection can produce many molecule layers. This table section controls and manages all layers in the image. For more information please see  . ChannelsNSA (Non-Specific Activation) A set of molecules that are categorized into a predefined class in the final N-STORM Image. These are molecules that were not first identified in the imaging frame immediately following Activation.Z Rejected A class of molecules that are categorized in a pre-defined channel in the final N-STORM Image. Z Rejected molecules only apply to 3D STORM datasets. Molecules are placed into the Z Rejected Channel for one of the two reasons:Z position of the molecules is outside the Z calibration range. Typically, a STORM system is calibrated within +/- 400 nm of the focal plane.Z position of the molecules cannot be determined (based on its Wx and Wy properties) because the (Wx,Wy) point is too far away from the calculated Wy = f(Wx) calibration curve.FiltersThis section defines filters and grouping.Feature Name of the detected molecule feature.Minimum Minimal value of the selected feature which will be shown in the image. Use the histogram below to find the desired values.Maximum Maximal value of the selected feature which will be shown in the image. Use the histogram below to find the desired values.Histogram Move the green arrows of the histogram to change the minimum and maximum molecule feature values. Resets any filters (minimum and maximum changes) and turns all filter features off.Time grouping Groups the time dimension and displays the grouping in a new “F” dimension.Time Steps Cumulates the time dimension into equivalent segments (“timelapses”) with their length specified in the Time slice edit box. Figure&nbsp;1302.&nbsp;F (2/10) grouping frames 11-20.Moving Window Highlights the cumulated time frames with a green color. Figure&nbsp;1303.&nbsp;F (26/100) grouping frames 24-33 with an offset value of 2 (2 frames before and 8 frames after).Time slice Specifies the number of frames in a grouping segment. Its length is specified in the Time slice edit box.Offset Defines the frame offset inside a grouping segment (see image above).Synchronize with time axis If this function is turned on, frame dimension is synchronized with the time dimension showing molecules belonging to the particular frame. Move through “T” and “F” moves along.Z Grouping If a Z dimension is acquired, Z grouping can be defined in the Z Slice edit box.Z Slice Specifies the number of Z planes in a grouping segment.For more information, please see  . ",
     id: 119 }, 
   { title: "Reference",
     xmlid: "id|molecule.referencex",
     content: "               Depending on the computer processing power and the amount of data STORM analysis may take a long time to complete. To facilitate processing of large amount of raw data N-STORM analysis software provides the capability to process multiple data sets in automatic unattended mode (typically overnight). User can create a list of data sets with their associated identification parameters and perform their sequential analysis. Batch processing is configured and launched from the Batch STORM Analysis dialog window. Figure&nbsp;1306.&nbsp; Load SBDx This button opens a standard Windows Open File dialog allowing the user to open a batch job file. Save SBDx This button opens a standard Windows Save File dialog allowing the user to save the batch job file into the specified folder with a specified name. Add current ND2 as task This button inserts a new task line. The task line is populated with an ND2 file path, ID parameters, and selected period range currently set in the main dialog. Delete selected task(s) Deletes the selected tasks. Delete all the task(s) Deletes all tasks. Move selected task(s) up Moves the selected task(s) up. Move selected task(s) down Moves the selected task(s) down. Edit Identification parameters for selected task(s) Opens the Identification Settings  dialog used for this input (please see  ). Start Batch Analysis Executes the batch job. Once the job is executed, this button changes to Terminate Batch Job  so that the batch job can be stopped.Input Only the selected file type can be added to the list of tasks.Default Text Output Sets the default text output. Each row corresponds to one task. If this option is checked, the selected task is included in the current job.Data Set Shows the file system path to the raw data set associated with each task. Double-clicking on the data set name opens the selected source image.Identification Parameters Shows identification parameters for each task. Double-click on the parameters opens the Identification Settings  dialog window used for adjusting the parameters (please see  ).Analysis Range Displays the period (or Time slice for Live Cell data) range for each task.ROIs Displays any available ROI(s).Text Output Specifies the optional text output format.Status This column is active only when the batch job is running. It displays the status of each task. For the tasks that have not yet started it is blank. For the currently active task it displays the percentage progress of each step. For processed tasks the status can be “Completed” or “Terminated” or “Failed” for STORM Analysis tasks, or number of molecules output for Molecule List Conversion tasks. Some of the common failures have specific descriptions such as “Failed to load” or “Failed to Identify”, etc.unique, overwrite, rename old Unique (default) generates unique names as described above. Overwrite saves the output with a default name, overwrites the old one if it exists. Rename old saves the output with a default name, renames the old one if it exists.Processing of a single data set is called a Batch Task. One or more Batch Tasks combined are called a Batch Job. The Batch STORM Analysis dialog represents one batch job and has a list of its tasks. There are two basic types of batch tasks:STORM analysis.Molecule list conversion (generates one or more text molecule lists from the binary molecule list).Each row in the list corresponds to one task. The check boxes in the left column control inclusion and exclusion of their corresponding tasks in the current job. The second column contains the file system path to the raw Data Set associated with each task. The third column holds Identification Parameters (such as CCD Background, Min Peak Height, etc.) for each task. The Analysis Range (or Time slice for Live Cell data) for each task resides in the fourth column. The third and fourth columns are not applicable to Molecule List Conversion tasks. Fifth column is used to specify the optional Text Output format. The last (sixth) column is active only when the batch job is running. It displays the Status of each task. For the tasks that have not yet started it is blank. For the currently active task it displays the percentage progress of each step. For processed tasks the status can be “Completed” or “Terminated” or “Failed” for STORM Analysis tasks, or number of molecules output for Molecule List Conversion tasks. Some of the common failures have specific descriptions such as “Failed to load” or “Failed to Identify”, etc.Text output format column&nbsp;This column for a newly added task is set to “Default”. In this mode, the global Default Text Output at the top right of the Batch STORM dialog window is applied. Figure&nbsp;1307.&nbsp;Each task can have independent setting by specifying Text Output options other than Default as shown below. Figure&nbsp;1308.&nbsp;Multiple tasks can be selected and applied the same Text Output option also. Figure&nbsp;1309.&nbsp; To Add a STORM analysis task, load a data set, adjust the identification parameters, set the analysis range, and then use the  toolbar button. This should add a new row to the list of tasks and copy the current combination of the file path, identification settings, and analysis range. Alternatively one or multiple ND2 files can be dragged and dropped onto the Batch STORM Analysis  window from Windows Explorer. Files which have identification settings embedded show up as batch tasks with those settings. Newly acquired files inherit the current system set of identification settings.The input for STORM analysis task is an ND2 file. The output of STORM analysis task is a binary molecule list. The input of molecule list conversion task is a binary molecule list. The output of the molecule list conversion task is a text formatted molecule list. STORM analysis task can optionally produce text molecule list in addition to unconditionally produced binary molecule list. To add binary molecule list conversion tasks drag and drop one or more binary molecule list files from Windows Explorer to the Batch STORM Analysis window. STORM analysis and molecule list conversion tasks can be mixed in a single batch job. Adding tasks via drag and drop also supports folders. When a folder is dropped onto the Batch STORM Analysis window it is searched for all ND2 files (extension .nd2) and binary molecule lists (extension .bin).The Input drop down list works as a filter during folder or multi-file drag and drop operations. It can accept only ND2 files, or only binary molecule lists, or both. Figure&nbsp;1310.&nbsp;Select the desired option before adding tasks using drag and drop. If the set of files being dropped contains only one type of files (for example only molecule lists) but the input filter is set to only accept the other type (ND2 files for this example) then, as a convenience feature, the filter is ignored and the dropped files are accepted as batch tasks.The task list supports common Windows multiple item selection behavior using Shift or Ctrl. Highlighted tasks as a group or individually can be moved up and down the list ( ) or deleted (). To quickly clear the job, all tasks can be deleted at once (). While batch job is in the setup mode (not running) the main STORM dialog is accessible and can be used to configure individual tasks (one at a time). To edit an existing task user can use the toolbar buttons. The  button loads the tasks data set, identification parameters, and analysis range into the analysis module. The  button facilitates editing of identification settings for the corresponding task. The same button on the toolbar edits identification settings of the first highlighted task. Once clicked, it brings up a standard Identification Settings dialog ( ) pre-populated with values pertaining to the task.Default Text Output is optional for STORM Analysis tasks and mandatory for binary molecule list conversion tasks.The entire batch job can be saved into and loaded from a file using  and  buttons that open standard Windows Open File and Save File dialogs with file filters set for STORM Batch Data (*.sbd) files. A STORM Batch Data file can also be dragged and dropped onto the Batch STORM dialog to open.Use the  button to execute the current batch job. Once the batch job is active, the main Batch STORM Analysis dialog becomes unavailable until the batch is stopped or finished.While the batch job runs the molecule lists are saved in the same directories with their respective ND2 data sets. To avoid file naming conflicts the molecule list file names are suffixed with the time stamp at the time when each is saved. This is the default behavior. It can be changed to one of the behaviors by clicking on the status bar right pane (unique/overwrite/rename old).unique This default option generates unique names and described above.overwrite Saves the output with a default name, overwrites the old one if it already exists.rename old Saves the output with a default name, renames the old one if it already exists.Output Molecule List File Name Format&nbsp;To avoid file name conflict, molecule list files saved by the batch processing have the following format:&lt;ND2-file-title&gt;_list-&lt;Timestamp&gt;_S&lt;#&gt;.binwhere:&lt;ND2-file-title&gt; is a data set ND2 file name without the extension (.nd2)&lt;Timestamp&gt; is the date and time when the batch job is started (e.g. 2019-01-30-13-45-05)&lt;#&gt; is the row number in the task list (1~) This dialog window is used for creating a custom raster document from the currently opened molecule image. Figure&nbsp;1311.&nbsp;Source Set the source area to be exported either by entering the Top/Left/Width/Height values or simply by repositioning the red rectangle in the image. Make sure proper units are selected.Target Set the target size (Width/Height) of the image being created and optionally set its calibration and zoom (image magnification).Channels to tabs If multiple channels are present in the image, the created document will have each mono channel as a separate tab. If this option is not checked, a RGB image will be created.Export time If checked, this function exports Time grouping as a T dimension.Export Z-grouping as Z-stack If checked, this function exports Z grouping as a Z dimension.Export Layers as Z-stack If checked, this function exports Layers as a Z dimension. Turn on Show Conventional Image in the STORM Display Options dialog.In the N-STORM window, move the Period Slider to the middle of the dataset.Make sure an activation frame is not currently being displayed. If so, move the frame slider to an imaging frame.In most cases the correct CCD Baseline value will be automatically determined based on the camera model. N-STORM analysis software displays the CCD Baseline edit field only when it fails to find the correct value. In that case enter correct CCD Baseline value for the camera used to acquire the raw data. It can be evaluated by looking at the average pixel intensity in an image captured without any light striking the camera sensor.Hover the mouse over the candidate PSF peaks. Look at the status bar to see the intensity under the mouse cursor.Find representative intensity peaks in the image and determine the relative peak intensity and the local background intensities around each peak. Subtract the local background from the central peak.Repeat Step 6 for a few peaks over several images to find the smallest (Min. Height) and largest (Max Height) relative peak heights that should be considered molecules.The procedure above assumes that the user is looking at the Status Bar to view the intensity of the pixel it is currently hovering over. The Identification Settings dialog is the first and most important step to making sure that the STORM Image is properly created. This section (and subsequent sections) is an explanation of each setting and suggested values or methods to determine the correct values.Auto Minimum Height Check this box to perform automatic min height detection during analysis. When auto min height is selected the software runs a short (100 periods in the middle of selection range) pre-analysis with relaxed identification constraints to determine the optimal min height for a given data set. This feature is not available when a Z Calibration data set is loaded.Auto Fit ROI Check this box to allow fitting algorithm consider variable number of pixels depending on width and axial ratio of each individual peak. When this check box is cleared a fixed size pixel rectangle (typically 5 x 5 pixels) is used for fitting. Auto Fit ROI  is recommended for 3D analysis. Fixed size ROI is recommended for 2D analysis.Minimum Height The intensity of the smallest (dimmest) peaks (minus the local background of that peak) to be identified as molecules. Any object whose peak intensity is below this value will not be identified. Please see  . If auto min height feature is selected the Minimum Height displays “auto” until the N-STORM analysis is performed at which point the “auto” is replaced with the actual calculated min height value.Maximum Height The intensity of the largest (brightest) peaks to be identified as molecules. Any object whose peak intensity is above this value will not be identified. Please see  .Baseline Closed shutter (zero photons) pixel response. For Andor DU897 and for Hamamatsu Orca Flash 4.0 cameras it should always be 100. When currently loaded ND2 file was acquired with an officially supported camera, the CCD Baseline field becomes read-only and is automatically populated with the name of the camera manufacturer and the correct for the camera baseline value.Minimum Width The smallest possible width a spot of some intensity in the image to be identified as a molecule by STORM Analysis.Maximum Width The maximum possible width a spot of some intensity in the image to be identified as a molecule by STORM Analysis. A suggested default for 2D is 400nm. For 3D STORM, this should be slightly larger to take in account defocused molecules (above and below the focal plane). A suggested default for 3D is 700 nm.Initial Fit Width This value is used as a starting point for STORM Analysis for identifying molecules. For 2D STORM, this is the expected value of a diffraction limited spot. A suggested default for both 2D and 3D is 300 nm.Max Axial Ratio Fluorescent spots whose ratio of elongation in the X and Y direction is larger than this threshold will be rejected as a single molecule. It is typically set to 1.3 for 2D STORM and 2.5 for 3D STORM.Max Displacement This value sets the maximum distance (in pixels) that a molecule identified in one frame can be located from a molecule identified in the previous frame to be considered the same molecule and arising from the same Activation frame. The default value is 1. As mentioned in Z-Calibration section for Z Calibration files this value is ignored and 0 is used during STORM analysis.Default This button can be used to quickly bring Max Width and Max Axial Ratio in correspondence with current 2D/3D status of the data. For 2D Max Width is set to 400 nm and Max Axial Ratio to 1.3. For 3D these settings are 700 nm and 2.5 correspondingly.3D This checkbox indicates whether the dataset will be considered 2D or 3D during the analysis and subsequent display of results. This setting is typically applied prior to acquisition when the experiment is defined. During acquisition, it is stored to the ND file and read when the file is opened in the N-STORM module. However, this setting can be overwritten by checking or unchecking this box.Fit Overlapping Peaks This check box selects the fitting algorithm used for molecule localization. When cleared, regular single Gaussian peak fitting algorithm is used. When checked, the software attempts to discover partially overlapping peaks and perform Gaussian fitting that accounts for such overlap. This algorithm generally produces better results with fluorophores whose blinking efficiency is less than that of Alexa647 (for example Atto488) or in densely labeled and over activated areas where molecule peaks tend to overlap in their respective frames. Algorithm that accounts for overlap may take longer time to compute. A reminder is displayed at the beginning of analysis (excluding Test) when Fit Overlapping Peaks is enabled. This reminder can be disabled for the session. Drift correction is a process that looks at each frame of the dataset and correlates the amounts of drift that may have occurred over time. For 2D STORM datasets, only the lateral (X, Y) drift correction is performed. For 3D datasets, both the lateral and the axial drifts are corrected. Figure&nbsp;1312.&nbsp;Difference between Drift Corrected (left) and non-Drift Corrected (right) dataDrift correction is an automated process. It can be completed as part of STORM Analysis (check box in the STORM Analysis). Figure&nbsp;1313.&nbsp;There are two types of drift correction. One uses the entire set of molecules to track the drift. This method is called Molecule Drift Correction. The other method (Bead Drift Correction) tracks registration fiducials (also called beads) attached to the cover glass. These beads fluoresce just like fluorophore molecules and their STORM localizations are attributed to dedicated “Drift Correction” channel. If STORM data set contains Drift Correction channel (set during acquisition), then by default the latter drift correction method is used. There is still an option to use the Molecule Drift Correction for drift correction if the data set is acquired as fixed cell however it cannot be used for drift correction in live cell data because the live cell molecule motion obscures the drift.Bead drift correction depends on the quality of the captured bead data. Ideally there should be several beads that remain attached to the cover glass and visibly fluorescent in all bead frames. Also ideally extraneous beads should not float around beads tracked from the beginning, and molecule localizations should not appear near beads in the “Drift Correction” channel. In practice these conditions are not always met. The analysis software has a certain amount of tolerance built in it. All beads visible in the first bead frame are the candidates for tracking. Some of them may get disqualified as the tracking progresses. Disqualified beads are called “bad” and those that can be successfully tracked are called “good”. At the end of the bead drift correction “good” beads are marked with circles if ROI mode is enabled.The center of the circle is a mid-point between the first and the last bead localization within each good bead trace. Diameter of the circle is proportional to the amount of drift at reasonably high zoom levels. In a relatively zoomed out view the circles have a fixed size. Otherwise they would be impractically small. Filter settings may exclude some bead localizations from the drift correction calculation, thus affecting qualification of the bead traces.Color with Frame mode (also called Time Map rendering) is useful to visually evaluate bead traces. It can be toggled by the Ctrl+Shift+F key combination. Besides mapping the color to the frame number, for live cell data this mode shows all bead localizations at once (as opposed to single time slice worth of localizations as is normal for live cell rendering). Although Color with Frame mode can be used for other (non-bead) localizations, it is particularly useful for beads. Figure&nbsp;1314.&nbsp;Ideally all the “good bead” traces should look similar. If certain bead traces look better (less spread) than others, then only those can be used as candidates for drift calculation. To do this, draw multi ROIs around good bead candidates, and then click the top toolbar Bead Drift Correction button.Even with manual bead selection the algorithm still evaluates bead quality and can reject some beads. In an extreme case all of the bead candidates may get rejected, which leads to a bead drift correction failure. When bead drift correction fails on fixed cell data set, autocorrelation method can still be used and is automatically offered.Ideally, beads should be wavelength separated from the molecules. In other words, bead localizations should not show up in the molecule channels and the other way around. In practice this is often not true. N-STORM analysis software offers an option to hide bead localizations in the molecule channels. This is done by hiding all molecule localizations located within a small radius of bead localizations. The radius can be adjusted via “Phantom Bead Mols Search Radius” ini file variable. When data set with bead drift correction is loaded, “Hide Localizations near Beads” pop-up menu option becomes available. Figure&nbsp;1315.&nbsp;Hiding of molecules near beads can also be toggled via Ctrl + H, B sequence of keystrokes. After the initial Ctrl + H keystroke the window title displays one of the context sensitive reminder prompts below. Each molecule in the text file will be listed on a separate line. Each line will contain the following pieces of information about that molecule: Table&nbsp;17.&nbsp;Field NameDescriptionChannel NameThis is the name of the Channel where the molecule was detected.XThe X position of the centroid of the molecule in nanometers. Similar to the conventional image, molecules positions in the image are relative to the upper left corner of the image.YThe Y position of the centroid of the molecule in nanometers. Similar to the conventional image, molecules positions in the image are relative to the upper left corner of the image.XcThe X position of the centroid of the molecule (in nanometers) with drift correction applied. If no drift correction was applied to this data then Xc= X.YcThe Y position of the centroid of the molecule (in nanometers) with drift correction applied. If no drift correction was applied to this data then Yc= Y.HeightIntensity of the peak height in the detection frame (after the detection process).AreaVolume under the peak. Units are intensity * pixel^2WidthGeometric mean of Wx and Wy. PhiThe Angle of the molecule. This is the axial angle for 2D and distance from Z calibration curve in nm in Wx, Wy space for 3D).AxAxial ratio of Wy/Wx.BGThe local background for the molecule.IAccumulated intensity.FrameThe sequential frame number where the molecule was first detected.LengthThe number of consecutive frames the molecule was detected.IndexUsed in individual frame localization records only. Contains the index of corresponding linked molecule record.LinkIndex of localization in the molecule list of the frame pointed to by Frame field or -1 indicating the end of the trace.ValidFor internal use only.ZThe Z coordinate of the molecule in nanometers (origin is the cover glass).ZcThe Z position of the molecule (in nanometers) with drift correction applied. If no drift correction was applied to this data then Zc= Z.PhotonsThe total number of photons received from molecule across its entire trace length. This number is calculated using current (at the time of calculation) photons per count value.Lateral Localization AccuracyXY localization accuracy calculated using Thompson formula based on the number of photons.XwX coordinate after XY warp transformation to compensate for chromatic aberration.XwcX coordinate after XY warp transformation and drift correction.YwY coordinate after XY warp transformation to compensate for chromatic aberration.YwcY coordinate after XY warp transformation and drift correction.ZwZ coordinate after chromatic aberration correction Z offset has been applied.ZwcZ coordinate after chromatic aberration correction Z offset and drift correction have been applied.The following image illustrates the scheme used to link multiple localizations from adjacent frames into single molecule record. Figure&nbsp;1316.&nbsp;Molecule list with trace traversing structure.Molecule list with trace traversing presents the molecule information rearranged for easy trace following. It is generally ordered by the master molecule list but the individual frame localization records are inserted immediately following the corresponding molecule record. Molecules with trace length of N will have N+1 records (master and N frames). For consistency molecules with trace length of 1 will have 2 identical records (master and one frame). Table&nbsp;18.&nbsp;Example of molecule records with trace length of 1. Mol[124939] count[1].XYXcYc...&nbsp;111.21791189.36948111.70292189.78879...(master)111.21791189.36946111.21791189.36946...(frame 1) Table&nbsp;19.&nbsp;Example of molecule record with trace length longer than 1. Mol[124944] count[3]XYXcYc...&nbsp;206.6884331.06892207.1741631.48837...(master)206.6700131.07309206.6700131.07309...(frame 1)206.6677731.00289206.6677731.00289...(frame 2)206.7247631.13095206.7247631.13095...(frame 3) If any ROI(s) with molecules inside of them are present in your image, switch to the ROI Statistics tab to view the statistics per each ROI and for the whole image. Figure&nbsp;1317.&nbsp;ROI Statistics TableThe table summarizes the number of molecules in the image and in all ROIs. If multiple ROIs are selected, their features are shown per each ROI (separate column). More features such as the area, perimeter, center of mass and Niquist are calculated as well. To easily switch the units used in the statistics, click on the [px] &lt;-&gt; [µm]  button in the top left corner of the table.The first graph shows the Photon count chart which represents the peak intensity histogram in the ROI.The second graph shows the Lateral localization chart representing the Width of the Gaussian peak.The third graph shows the Ripley graph and cumulative histogram. Select one of the three graphs for molecule population - Ripley's K function (K), Ripley's H function (H), or CHD and set the Max search radius and Border correction. All the three graphs use the XY distance between molecules as the horizontal axis.CHD (Cumulative Histogram of Distances) For any given distance the value of the function is the number of molecule pairs in the ROI molecule population with distance equal or smaller to the given distance. Formal definition: Figure&nbsp;1318.&nbsp;where I r I r is the indicator function defined as Figure&nbsp;1319.&nbsp;nn is the number of pointsrr is the max neighborhood search distance around any given pointd ij is the distance between points i and jRipley’s K function Ripley’s K function is similar to CHD except it is scaled by factor: Figure&nbsp;1320.&nbsp; Figure&nbsp;1321.&nbsp;where λ is average density. In this form Ripley’s K function estimates the expected number of additional points within distance rr around a randomly picked point normalized per unit of density. λλ is the average density of the ROI populationλ = n/awhere aa is the ROI area.Ripley’s K function is useful to explore cluster topology. When molecules form clusters, Ripley’s K function forms horizontal plateaus and steep rises (staircase look). For example, in the simplest case of two clusters, Ripley’s K function has one horizontal plateau. Figure&nbsp;1322.&nbsp;The horizontal axis of Ripley’s K function is the distance between molecules in a given pair. The vertical axis is proportional to the number of pairs in the population with distance not exceeding given. The first vertical rise on the left side comes from a number of close by neighbor molecules within each cluster. The position of the left edge of the horizontal plateau implies the size of clusters. As the distance increases further, the number of matching pairs does not increase until the distance reaches the distance between clusters. At that point the count begins to rise again, because larger distances between molecules of different clusters begin to contribute. Therefore the right edge of horizontal plateau suggests the inter cluster distance. Mouse cursor position in graph units is displayed in the bottom right corner as the mouse hovers over the graph area.Ripley’s H function Ripley’s H function can be defined in terms of Ripley’s K function. Figure&nbsp;1323.&nbsp;When points are randomly distributed, K function approximately follows the area of a circle function. Figure&nbsp;1324.&nbsp;The K function exceeds area function, when data points form clusters on a given scale. Conversely, the K function falls below the area function, when data points disperse (or form voids) on a given scale. The H function makes detection of clusters (or lack thereof) easier than the K function, because its value remains 0 under CSR (complete spatial randomness) condition. When the data points exhibit clustering, the H function becomes positive and forms a visible peak. When the data points are distributed uniformly, the H function becomes negative. Figure&nbsp;1325.&nbsp;All three cluster analysis functions quickly become computationally expensive as ROI population grows. Calculation time is greatly affected by other options such as maximum neighborhood search radius and edge effect correction method discussed below.Max search radius edit box below the function selection buttons displays and adjusts the max neighborhood search radius. The button on its right side provides (and cycles through) three choices for distance units: nanometers (nm), micrometers (µm), and pixels. Pixels correspond to the original camera pixels (typically 160 nm). The largest supported search radius is 16 pixels or approximately 2.5 micrometers. The units apply to both the max search distance and the graph. When units change, the value of the Max search radius edit box automatically converts. The choice of units does not affect the length of calculation, but, when changed, commences the new calculation rather than converting results of the previous one. Reducing the Max search radius can greatly reduce the length of calculation.Border correction is an edge effect correction attempting to compensate for the lack of neighboring points when the search is conducted near the ROI border. The circular search neighborhood then partially falls outside of the ROI. None simply skips the correction. This is the fastest method. Perimeter implements the popular correction method based on the fraction of circle perimeter within the ROI. Each measured distance carries a weight factor. When the circle around point i with radius equal to distance to point j is completely contained in the ROI the weight factor is 1. Otherwise the weight factor is equal to the fraction of the circle perimeter within ROI and is less than 1. Figure&nbsp;1326.&nbsp;During Ripley’s K function calculation contribution of each distance is divided by its weight factor, thus increasing when weight factor is less than 1 and therefore compensating for the possibly reduced neighbor count. Figure&nbsp;1327.&nbsp;Perimeter based edge effect correction is the most computationally expensive. Switching to a faster method, if acceptable, can greatly reduce calculation time.Border correction method simply counts neighbor points outside of ROI that fall within max search radius or ROI points, but does not search their neighborhoods. All weight coefficients remain 1. Figure&nbsp;1328.&nbsp;Edge effect correction applies to CDH and H(r) functions in a manner similar to K(r) shown above.When molecule data has multiple channels, cluster analysis can be performed across channels. In other words the distances are measured in one direction: from molecules in origin subset to molecules in target subset. These subsets can be defined as all molecules or molecules belonging to one of the channels. All combinations are possible. For example a two channel data set provides the following options: Figure&nbsp;1329.&nbsp; This button performs XY Warp calibration using the information extracted from the multi reporter Z Calibration file. This button is enabled only when such file is loaded and STORM analyzed. Since 3D Cylindrical lens can affect chromatic aberration and separate calibration is recommended for each combination of objective, reporter wavelength, and cylindrical lens. To calibrate 2D XY Warp acquire Z Calibration file without cylindrical lens in optical path. 3D XY Warp calibration can be achieved with regular Z Calibration file. Single multi reporter calibration file produces several calibrations at once (one for each wavelength except the shortest one, which serves as a base and does not require calibration). XY Warp is applied to molecule coordinates to compensate for the effects of chromatic aberration. Molecules with longer reporter wavelength can be optionally repositioned to locations where they would be likely to appear. This adjustment is necessary to correctly perceive the relative position between molecules reported in different wavelengths. Unsuitable beads can be manually excluded from the warp calibration by assigning multi ROIs around them. Z Calibration is performed by analyzing a Z-Series dataset to determine the width and orientation of all identified molecules. Each Z Calibration pertains to an objective and an excitation wavelength. Objective name is displayed in the title of the dialog. Excitation wavelengths of different channels are presented as tabs in the upper half of the dialog. When data set (ND2 file) is loaded Z Calibration dialog displays only calibrations linked to the objective and the reporter wavelengths pertaining to the data. To view all known Z Calibrations maintained by the system (for other objectives and wavelengths) close current data set or restart the software and visit Z Calibration dialog when no data set is loaded. In this case two extra buttons allow iterating through the list of calibrated objectives. Individual reporter calibrations can be accessed via tabs named and colored after their respective wavelength. Additionally in this mode the Load and Save buttons at the bottom are renamed into Restore and Backup respectively and perform a different function that will be described below. Figure&nbsp;1330.&nbsp;Start/End These values specify the lower and upper Z limit of the calibrated dataset. Typically, these values will be smaller (by absolute value) than the top and bottom of the Z Stack used during the calibration process. These values are automatically updated during Auto Calibration.Step This value specifies the resolution of the calculated Wx/Wy calibration curve. Typically, data points that make the curve are spaced 1 nm (in Z direction) apart from each other.Tolerance The tolerance value specifies how far from the  vs.  curve a molecule can be and still be considered valid. Any molecules that are further from the curve than the specified tolerance distance will be placed into a special channel called “Z Rejected”. Z Rejected Molecules can optionally be displayed. The Default tolerance value is 1.5 nm.Parabola Coefficients Values There are two ways to determine the Wx and Wy calibration curves. The first method is for the software to capture a special Z series of a calibration slide (sub-resolution beads mounted to the cover glass). The elongation of every molecule is measured and the software automatically generates the curves.The second method is to use the data collected from the Z-Series dataset and calculate the calibration outside of NIS-Elements using third party analysis software. The coefficients that were calculated outside of NIS-Elements can then be input into the parabola coefficients for Wx and Wy.The defocusing curve formula is: Figure&nbsp;1331.&nbsp; is the lowest point in each graph or .  and should be approximately the same. If they are not, one parabola is positioned higher than the other. It means points focus tighter in one direction than in the other likely due to spherical aberration. is the focal depth of the microscope. Ideally it should also be the same between X and Y. Calibration fitting algorithm restricts it to stay below (and including) 5000. is the relative to focal plane Z position where each curve focused the tightest. Ideally the values for X and Y should be the same by absolute value but with opposite signs (X negative and Y positive). This would mean that calibration is perfectly symmetrical around focal plane. and  values can be directly found in the  and  vs.  graph as illustrated below. Figure&nbsp;1332.&nbsp;“A”, “B”, “C”, and “D” are higher order polynomial coefficients. A is the coefficient of the third order. B is the coefficient of the fourth order, and so on. Ideally all coefficients (A, B, C and D) should be 0. Typically coefficients A and B alone produce close enough approximation curve, so C and D are normally kept at 0 even in practice. There is an .ini file setting that removes the C and D constraint to deal with difficult cases. It's called “z calibration auto calib higher order corrections”. It is difficult to judge the quality of calibration by just looking at the values of these coefficients.Auto Calibrate This button automates the process of creating the Z calibration curves. It requires a Z-Calibration molecule list present (loaded or analyzed). Holding Ctrl while pressing Auto Calibration button causes the algorithm to ignore high density cluster localizations that may lead to peak overlap in out of focus frames. Ignored localizations are reassigned to Z Rejected channel that is otherwise not used with Z Calibration data.Load/Save Z Calibrations can be saved to a text file. These files can then be opened and applied to other 3D STORM datasets. These buttons are only available when ND2 data set is loaded. They work with a subset of calibrations pertaining to current data set. These calibrations may be different from the calibrations maintained by the system for newly acquired data sets. Calibrations loaded in this manner are called “foreign”. They become active (in effect for the subsequent 3D STORM analysis) and temporary override system or embedded nd2 file calibrations. They do not overwrite system calibration, which goes back in effect as soon as current nd2 file is closed. Foreign calibration can be permanently embedded into an nd2 file if at any point after foreign calibration is loaded the nd2 file is saved.Graph This button displays the menu with options for various graphs that visualize calibration curves and raw data and help evaluate results of current Z Calibration. There are eight types of Graphs available. Figure&nbsp;1333.&nbsp;Wx-Wy vs. Z – This graph shows the Wx and Wy calibration curves (cyan and yellow respectively) and the identified molecule’s Wx (red) and Wy (blue) values that pertain to currently selected in Z Calibration dialog channel tab. Generally the number of raw data points is limited to 3000. To display all points hold the Ctrl key while selecting the menu item.Ax Ratio vs. Z -This graph will show the ratio of Wy/Wx on the Y axis and Z on the X axis. It also shows the identified molecule’s Wy/Wx ratio values (in blue). Generally the number of raw data points is limited to 3000. To display all points from the currently selected channel hold the Ctrl key while selecting the menu item.Wx vs. Wy – shows the plot of Wx vs. Wy for the calibrated curves (in green) and of the individual localizations from current channel (in blue). Generally the number of raw data points is limited to 3000. To display all points hold the Ctrl key while selecting the menu item.Holding the Shift key while selecting this graph’s menu item displays the Z Rejected points in addition to the points from the currently selected channel tab. This helps checking how well the raw data follow the current calibration. Z Rejected molecules can be further categorized into those too far from the curve (yellow) and those with Z position beyond the calibration range (red).Wx-Wy vs. Z (Frame Average) – this will show the same graph as above (#1) but only show the average Wx and Wy position per frame of the identified molecules. This graph is only available immediately after Z Calibration.Ax vs. Z (Frame Average) - This will show the same graph as above (#2) but only display the average Ax (Wy/Wx ratio) per frame for the molecules. This graph is only available immediately after Z Calibration.Wx vs. Wy (Frame Average) – This will show the same graph as above (#3) but only display the average Wx and average Wy per frame for the identified molecules. This graph is only available immediately after Z Calibration.Axial Ratio vs. Period – This graph is useful for evaluating Z calibration data before performing Auto Calibration because it does not rely on any (previous or current) calibration. It simply displays the raw data points without any curve graphs. This graph is only available when Z calibration data is loaded and STORM analyzed. By default it displays up to 3000 points from the currently selected channel.To display all points from the current channel hold Ctrl while selecting the menu item. Sometimes the data contains outliers that affect scaling. To exclude outliers and see the data that more closely resembles the filtered set used for actual calibration, hold the Alt key while selecting this graph’s menu item. This option picks only 80% of points from the middle of the distribution in every frame and rejects outliers.To observe the effects of chromatic aberration on multichannel data hold the Shift key while selecting the menu item. This displays all channels at once.The channel colors used in this graph match the pseudo colors of the calibration data set.Axial Ratio vs. Period (frame average) – This graph displays almost the same data as the one above (#7). The only difference it calculates the average for every frame and displays it as one point per frame. It has all the same options (Shift for multi channel, Ctrl for all points rather than 3000, Alt for skipping outliers). This is the most useful graph to evaluate raw Z calibration data before actual calibration.The software keeps track of multiple Z calibrations as they pertain to objectives with which they were acquired. During analysis the objective information is retrieved from the metadata and used to apply the correct calibration curves. If the calibration for data set objective is unavailable the software displays the warning and applies the default calibration. Similarly to Objective name software links calibration to reporter wavelengths and warns if calibration curve for any of the dataset wavelength is missing.The objective name pertaining to current Z Calibration is displayed in the title bar of the Z Calibration dialog.Calibrations for different wavelengths are affected by chromatic aberration. With a cylindrical lens in the optical path a given axial ratio of localizations happens at different Z positions for different wavelengths. Conversely a given Z position imaging with different wavelengths produces localizations with different axial ratios. When multiple wavelength calibrations are acquired and analyzed together, the software extracts axial chromatic shifts and maintains them along with calibration curves. When a newly acquired data set (acquired with the same objective and reporter wavelengths) is loaded and analyzed, the calibrated chromatic offsets are also applied to Z positions. The channel with the shortest reporter wavelength is used as a reference for chromatic aberration offsets.All graph dialogs have pop-up menu options to change the color of the background and grid lines. These colors are shared by all N-STORM analysis module graph dialogs and persist across NIS-Elements sessions.Similarly all graph dialogs share the ability to zoom by the mouse scroll wheel around the current cursor position and to pan the graph with a mouse drag operation.Graph image as well as numerical data can be transferred to a third party application via Windows Clipboard or exported into an image or text files on a disk. Both options appear at the bottom of the right click pop-up menu.All Z Calibration dialog buttons provide tooltip reminders. Some Automatic layer alignment algorithms involve examining and matching multiple ROIs. These ROIs can be defined manually or generated automatically. The mix of both is also possible. Click on the drop-down arrow next to the  Start correlation button to open the Z-Stack Alignment Options.ROI Source This section defines the set of ROIs to be used. Automatically generated ROIs are arranged in a grid with definable dimensions (Grid). The Mix Manual and Auto generated ROIs button controls the mixing behavior. When the button is unchecked ()and manually defined ROIs exist, no auto generated ROIs are added. When the button is checked () or no manually defined ROIs exist, the grid of ROIs is generated according to grid dimensions. The left field is the number of columns and the right field is the number of rows.ROI Filters Not all generated ROIs are good for layer alignment. Only ROIs that pass through the ROI filter are considered.First ROI filter is by the molecule count . ROIs with relatively small number of molecules are discarded. The Min Count threshold is defined as a percentage of the maximum single ROI molecule count among the ROI population. The button toggles the filter on and off. The filter is enabled by default.The second filter is the Maximum average Nearest Neighbor distance filter (). If applied, it removes ROIs with average nearest neighbor distance greater than the specified percentage of the maximum average nearest neighbor distance if the molecules were evenly and randomly distributed throughout the ROI area without forming any structure or clusters. In practice this filter is not very useful because, with STORM, individual molecules get localized more than once. Even when fluorophores are evenly distributed, their localizations form tight clusters which significantly drives average nearest neighbor distance down, thus making ROIs that lack structure falsely identified as those containing a structure. This filter is available only in the Advanced mode and is disabled by default.Autocorrelation density map This group of controls provides several options that affect the autocorrelation algorithm input. Autocorrelation expects two similar signals. As it is used for layer alignment of localization data the signals are 2D density maps or histograms with bin values representing molecule counts falling within each spatial bin.Z layer intersection ()or union ()choice affects the Z position filtering during histogram build. Intersection is chosen by default.Another option ()is to merge ROIs for single autocorrelation and single offset result or () to perform separate autocorrelation for each ROI and then filter/average multiple results.Outlier histogram bins can be optionally trimmed to avoid spurious autocorrelation offsets caused by occasional high density clusters. This option is enabled by default. Figure&nbsp;1334.&nbsp; Figure&nbsp;1335.&nbsp;Autocorrelation offset filter This filter is shown in the Advanced mode only. It is applicable to multi ROI alignment workflows that do not merge multiple ROIs (). This produces multiple offset results for a given pair of layers. The Offset Filter () checks how well multiple Cubes agree on the XY offset. DBSCAN algorithm is used to find a cluster that pertains to the subset of cubes that well agree with each other. Offsets obtained from other cubes are then ignored as outliers. The good offsets are averaged. To troubleshoot difficult layer alignment the Test Mode is available via the  button. In the Test Mode intermediate data from each cube autocorrelation operation is available for close examination and alignment offsets are not applied to layers.       Typical N-STORM Z calibration range is 800 nm (from 400 nm below to 400 nm above focal plane). Z stacking feature is available to overcome the 800 nm depth limit and image larger volumes. N-STORM acquisition module captures multiple slices along Z axis. Each slice covers the 800 nm Z range. Each slice raw data is stored in a separate ND2 file. To even out photo bleaching during acquisition, the volume can be acquired by spending shorter time at each slice while visiting each slice multiple times. In Z stacked STORM terminology the slices are also called “layers” and a set of unique layers is called a “pass”. The entire Z stacked STORM data set consists of one or more passes. Similarly to regular (non-Z stacked) 3D storm data, each layer has its Z position defined as the distance between the cover glass and the focal plane. The actual position of the focal plane is affected by spherical aberration. It is calculated by the software from the microscope Z mechanism displacement measured and stored in the ND2 file metadata. The displacement focal plane position (not the corrected one) is the one associated with each layer and displayed in the software.Layers may overlap in the Z direction to aid alignment between adjacent layers and between passes. When they do, the focal plane positions of adjacent layers differ by less than the calibration range. Alignment is necessary to compensate for the drift during acquisition. Z stack STORM ND2 files follow a file naming convention to preserve layer and pass numbering order. Each name is made of:Base file name common to all files.Pass number suffix that starts with letter P followed by two digit pass number (starting with 01).Layer number suffix that starts with letter L followed by three digit layer number (starting with 001)..nd2 extension. Example&nbsp;16.&nbsp;Example of two passes three layers Z stack with base name \"Test_\":Test_P01_L001.nd2Test_P01_L002.nd2Test_P01_L003.nd2Test_P02_L001.nd2Test_P02_L002.nd2Test_P02_L003.nd2 Similarly to the regular (non-Z stack) N-STORM workflow, ND2 files need to be analyzed to produce molecule lists that can later be rendered as super resolution images. Although each ND2 file can be analyzed individually, N-STORM Analysis module provides a simplified batch setup and processing to streamline the analysis of large amounts of data often associated with the Z stack STORM. Multiple ND2 files can be dragged and dropped onto the Batch window from Windows Explorer.The molecule list files produced by batch analysis are named after their ND2 predecessors. The pass and layer index suffixes are preserved. To continue with the ND2 naming example above the molecule list files will be named:Test_P01_L001_list-2015-08-07-10-45-20_S01.binTest_P01_L002_list-2015-08-07-10-47-30_S02.binTest_P01_L003_list-2015-08-07-10-49-40_S03.binTest_P02_L001_list-2015-08-07-10-51-50_S04.binTest_P02_L002_list-2015-08-07-10-54-00_S05.binTest_P02_L003_list-2015-08-07-10-56-10_S06.bin Each molecule list can be loaded individually in a traditional way. To load and merge all layers at the same time simply drag and drop all pertaining molecule list bin files from the Windows Explorer onto the main N-STORM window. Multiple files can also be selected and opened at once via File &gt; Open dialog using standard Windows (Ctrl and Shift) keys for multi file selection. Loading progress feedback is displayed in the window title.One ND2 file can be loaded before multi file drag and drop operation or as a part of it. Multiple ND2 files and other non binary molecule list files are ignored if present among binary molecule lists drag and drop file selection. Skipping some of the layers (by excluding them from drag and drop selection) during load is acceptable. The layers table in the Molecule Options dialog has the following main functions.Displaying layer informationSelecting layer subsets for various operationsSetting up layer pseudo colors (automatically and manually)Adjusting (ΔX, ΔY, ΔZ) layer offsets to align adjacent layers and passes (automatically and manually)Toggling alignment offsets in rendered imageMulti ROI associated with layer subsetsToggling between pseudo colors by layer and by channelLayer information is displayed in a grid where each row represents a single layer. Why do the molecules look dark when I am in Normalized Gaussian mode?The most common cause is that the display properties for Normalized Gaussian are not properly set. It is common that molecules look bright when the image is near 1:1 because of the large concentration of molecules. However, when zooming into the image, they can tend to appear darker. Adjust the Min and Max Threshold in the Advanced Gaussian settings of the STORM Display Options dialog.Why do I not see all of the molecules?It is common that not all molecules will be displayed. The lower right hand Status bar will show how many molecules are currently displayed vs. the total number of molecules identified. Most likely, there are some filter settings in the Filter dialog that are currently selected. Note that some filters (like photon count and trace length) are in effect all the time and cannot be disabled. However, they can be relaxed to include a wide range of values.How do I apply a Z Calibration to an already processed dataset?After calibration, save the calibration settings to a text file (in the 3D Settings dialog). Open the processed dataset, and load this text file. Then perform STORM Analysis again. See section on STORM analysis workflow for more information.When I turn on “Mark Identified Molecules”, only some yellow boxes have molecules inside. Why?The Rel Frame Range filter was set and only molecules within that selected frame range are displayed.Yellow boxes are always aligned to the low resolution image pixels. If you display drift corrected molecule positions the corresponding cross may be far outside the yellow box. In order to see it inside use uncorrected for drift coordinates.There are some other filter settings that prevent the molecule inside the box to be displayed. Some filters cannot be turned off. For example minimum and maximum number of photons, and max trace length. If the number of photons per count is set incorrectly then some molecules could be pushed out of the allowable (to be displayed) range for the number of photons.Yellow boxes remain on for the entire duration of the molecule trace. For example, if some molecule was first discovered in frame 1000 and remained fluorescing until frame 1005 (inclusive for the total of 6 frames) then the yellow box will be visible in all these frames. If you set your frame filter to only display molecules discovered in current frame (0 minus and 0 plus) then the corresponding cross will show up only in frame 1000 and leave empty yellow box in the remaining frames (1001 to 1005). In many cases empty yellow boxes mean that this is the time cross-section of the molecule trace and that this is not the first frame where this molecule was first discovered.Some molecules are invalid. They are present in the list but they are never displayed due to various reasons. This affects the cross but not the yellow box.In multi-channel files the currently selected tab channel acts as a filter. It is possible to navigate to some first after activation frame but switch to non-specific activation channel tab at the same time. Similarly one can navigate to some non-specific frame while selecting some specific activation tab. Either of these actions will leave full screen of empty yellow boxes.Mark Identified Molecule boxes are independent of Filter settings. Yellow boxes are not intended to be used with crosses or with Gaussian rendering at the same time. They are generally used as a test for the correctness of identification settings. They should be viewed superimposed on low resolution image so that user can see how many PSF peaks in raw pixel data were perceived as molecules.How often does Z Calibration need to be performed?Unless some part of the optical system changed (i.e. objective), Z Calibration only needs to be performed once.How do I calculate axial widths (Wx and Wy) of a peak from its general width W and axial ratio Ax?  ",
     id: 120 }, 
   { title: "Running a HCA job, viewing results",
     xmlid: "id|new.hcax",
     content: " Open JOBS ExplorerRun   .Connect to a job databaseA database for storing the job data must be opened or created before running a job. Use the database pull-down menu within JOBS Explorer. See  .  Figure&nbsp;915.&nbsp;Database pull-down menu within Jobs ExplorerRun the HCA Job templateClick one of the buttons in the Run: section of JOBS Explorer. The following jobs are prepared there with the HCA module installed:HCA Fixed Analysis of fixed samples prepared on well plates. HCA Live Analysis of live (motile or growing) samples.Define parameters of the JOBThe following window appears. You should browse all the tabs (tasks) and adjust settings of the job correctly. Once you visit a tab and the settings are acceptable - in the scope of the whole job - the background of the task icon turns green. Figure&nbsp;916.&nbsp;HCA Fixed Job wizard enabling to set up parameters of the Job step by step. Start by setting your first tab called Experiment Options   (see:  ). All the subsequent steps of your job will be affected by your selection in the Experiment Options  tab (some steps may even disappear).Enter a description, custom metadata and adjust any other selected notifications (see:   ,   ,   ).Select an optical configuration and define other listed acquisition parameters (see:   ,   ,   ).If Autofocus was selected in the Experiment Options , adjust its parameters here (see:   ).Select plates which will be used during the experiment (see:   ).Define your plate and select wells used during the experiment (see:   ,   ).Adjust parameters influencing the point generation and large image capture (see:   ,   ).Label wells if you selected Well Labeling (see:   ).Define parameters of the time sequence (see:   ).Define analyses which will be performed on the captured data (see:   ).Once you have set up all available tabs (all are highlighted green) you are ready for the job execution.Test the job on a single wellYou can click   Test Run to run the current job on a selected well.Run the jobWhen everything is set up correctly, click   Run. The HCA Progress window opens.See   for details.Some tasks may appear during run time waiting for user interaction. These tasks have to be set up and confirmed by clicking   Next / Run.Browse the resultsResults are displayed automatically in a separate window once the job is finished.The window containing results from a HCA job is similar to the one used in job runs.See chapter   for more information. ",
     id: 121 }, 
   { title: "NIS.ai Explorer",
     xmlid: "id|nis.ai.cataloguex",
     content: " NIS.ai Explorer (  ) contains a list of all trained AI. These AIs can be executed, deleted or imported from an existing file. Additional information about the selected AI can be displayed below. By default, the list of AIs is shown in a tree view grouped by their type. It can be switched to Sort by date, then each day can be expanded to reveal a list of AIs created that day. An icon with the NIS.ai function type ( Enhance.ai,  Convert.ai,  Segment.ai,  Segment Objects.ai) is displayed next to each AI name. Figure&nbsp;535.&nbsp;NIS.ai Explorer window. Create New Enables the user to selected a NIS.ai function and train a new AI. Run Runs the trained AI. Train All Queued Successively runs all AIs which were added to the queue using the Add to Queue button inside its training dialog. Each queued AI is marked by a clock icon in the AI list. Show Info Pane Displays information about the selected AI. Sort by Date Sorts the AIs by date. Sort by Type Sorts the AIs by the function type. Import From File Imports the AI from a file into your current project. Remove Removes the selected AI.Connected/Disconnected Displays the cluster connection status. Server name and port is shown when the connection is established. Cluster Settings Opens the Cluster Connection Settings dialog window. Start by filling in the Server Name and Port on which the NIS-Elements Compute Cluster is running. The default port number is 4444. Then click Connect . Information about the computer cluster server is shown. The central field shows a summary of the computing capability of all nodes.Active nodes Shows the number of nodes (computers) connected to the cluster also shown in the dashboard.Dashboard Opens the NIS-Elements Compute Cluster Dashboard inside your primary web browser. Jobs tab shows an overview of all tasks sent to the cluster (computer network) to be processed. Currently the following processing types are supported: AI Training, Batch Deconvolution, Batch Ga3 Distributed. Nodes tab shows all computers involved in the processing. Please see the   for more information. ",
     id: 122 }, 
   { title: "Segment.ai Example",
     xmlid: "id|nis.ai.example.segmentx",
     content: " The following example shows how to segment four types of cells using the Segment.ai function.We start by creating a multi-point image from a sequence of single TIF images using the    dialog window. Figure&nbsp;537.&nbsp;Converting singe TIFF files to one multi-point ND2 image.Once we Convert the images, a multi-point ND2 file is opened. Figure&nbsp;538.&nbsp;Training multi-point image sample.Then we open the    for cells segmenting. Four binary groups for four types of cells are created:live cells - dark colordead cells - gray, transparent lookbulged cells - having one blunt endfractured cells - broken into piecesWe use the available drawing tools ( ) to select single cell types and create four different binary categories. Make sure to follow the rules for binary preparation ( ).To add a new cell type (binary layer), we click on  Create New Binary Layer and select it from the drop-down menu next to this button. Each cell type will have a different color. Transparency of the color can be adjusted by a keyboard shortcut Ctrl + Up/Down. After drawing at least one cell of each type we can switch to the    panel and rename layers in the Working Layers section to “live”, “dead”, “bulged”, and “fractured”. Figure&nbsp;539.&nbsp;Binary Layers panel.Once we select and categorize all cells in the multi-point, we can start the training. Figure&nbsp;540.&nbsp;Categorized binaries.We run   , select the source channel (Mono in our case), select which cells will be used as Ground truth binaries , leave the default number of Iterations, specify the output as shown in the figure below and click Train. Figure&nbsp;541.&nbsp;Train Segment.ai dialog window.Sample training begins. The Loss curve (red) should be decreasing over time, faster in the beginning, slower in the end as it approaches zero. There may be a few bumps along the way which is correct. Training time depends on the difficulty of the sample and the use case. Figure&nbsp;542.&nbsp;Training progress showing the loss curve. Figure&nbsp;543.&nbsp;Loss curve after the training was completed.Once the dataset is trained, we can run the segmentation on a new dataset never used for training. Same as before the training, we convert the single TIF files into a single multi-point image. With the image open we run   , browse to the trained network, check Preview and adjust the Multi-Smooth  value while watching the preview image until the result is satisfactory. Then we click OK to run the segmentation. Figure&nbsp;544.&nbsp;Segment.ai dialog window.The dataset is now segmented and four new binary layers are created. Figure&nbsp;545.&nbsp;Segmenting result.You can further use the benefit of the    node in General Analysis 3 to analyze your segmented dataset, e.g. to count the number of cells of each type. ",
     id: 123 }, 
   { title: "NIS.ai",
     xmlid: "id|nis.aix",
     content: "           NIS.ai module in NIS-Elements is an artificial intelligence (AI) module, based on  , that utilizes deep learning and neural networks to automatically perform image processing tasks. The traditional approach of tuning parameters of common image processing algorithms is very time consuming and prone to error. NIS.ai provides an AI-powered automated solution to address these issues, as demonstrated in the diagram below: Figure&nbsp;497.&nbsp;NIS.ai process diagram.NIS.ai brings main benefits for:Samples unsegmentable by common methods.Label free samples (Transmitted light images are usually impossible to segment by standard techniques like Threshold, Spot Detection, etc.).Samples with a low signal (enhance the signal and prolong viability, reduce light and chemical toxicity of living cells).With NIS.ai it is possible to:Train more than one channel set as input or output.With Convert.ai, Segment.ai and Segment Objects.ai you can train M -&gt; N (M, N &gt;= 1).With Enhance.ai you can train M -&gt; M (M &gt;= 1, number of channels has to be the same for input and output).Training multiple channels at once is better than training them one by one.Train on multiple image files on your hard drive. Channels or binaries used for the training must be present in all listed image files and must have the same name in each file. Two channels having the same name in one document cannot be used. Image sizes and types can be freely combined (e.g. first file 1024 x 1024 px 8-bit with the second file 512 x 512 px float).Train on RGB images.Combine the channels (e.g. on an RGB image segment two cell types).Segment multiple classes at once (e.g. four cell types, see  ).Convert modalities (e.g. from a Brightfield channel create DAPI and FITC).Train multiple networks using a queue. Simply click Add to Queue (on multiple files and networks) and then click Train Queued in  .Train multiple networks on a dedicated cluster of computers to better manage the workload across various workstations. Click Add to Cluster and the cluster system automatically selects the best computer for your task and starts processing it. You can even turn off your computer while the cluster system works on your tasks. The channel order should remain the same in training and forward. If the document used for training contains DAPI and FITC channel, dataset with the exact channel order should be used for running the AI process.Four main functions, each suited to a different use case, are available – Image Enhancement (Enhance.ai), Convert modality (Convert.ai), General Segmentation (Segment.ai) and Object Segmentation (Segment Objects.ai). The general usage of NIS.ai is as follows:Pick a training dataset, which contains ground truth that AI learns.Train AI on this dataset.Evaluate AI on another but similar dataset (data not used for AI training).If results are sufficient then you can use this AI to process any other images you have automatically.If results are not sufficient you can extend your training dataset and retrain the AI. ",
     id: 124 }, 
   { title: "Functions",
     xmlid: "id|nis.ai.modalitiesx",
     content: "     Four main methods of use (functions) of the NIS.ai module are available: ",
     id: 125 }, 
   { title: "Convert.ai",
     xmlid: "id|nis.ai.modalities.modality.transferx",
     content: "  Use-case:Convert the modality from one to another. For example, generating nuclei from transmitted light channel (DIC, PhC, BF) or generating nuclei from one fluorescent channel to another fluorescent channel.Advantages:No need to acquire fluorescent image or acquire one fluorescent channel less.No bleaching or less bleaching.Very low light toxicity (by transmitted light only or higher wavelength fluorescent channel only).Longer cell viability.Convert.ai is trained using the    window and executed using the    window. Figure&nbsp;519.&nbsp; Figure&nbsp;520.&nbsp; Acquire sample with two channels – the source modality channel and destination modality channel. Train AI to convert the source modality channel into destination modality channel. Once the AI is trained, you no longer have to acquire the destination modality channel.For example, it is possible to train AI to transform the transmitted light channel into the DAPI channel to detect nuclei or to transform the FITC channel into the DAPI channel.To properly train Convert.ai, perform the following steps:Acquire sample with two channels – source modality channel and destination modality channel. Both channels should be acquired with sufficient light intensity and acquisition time to contain all relevant details and must contain image of the same scene. ND acquisition is the best solution (see the “ND Acquisition of Lambda channel setup image” below).The source modality channel can be a transmitted light channel (e.g. DIC) and should contain enough relevant information.The destination modality channel is usually a fluorescent channel (e.g. DAPI).Open the sample image having both channels (DIC, DAPI).Select Train Convert.ai in NIS.ai menu. In the Train Convert.ai dialog window you should:In the Training data  section select file(s) from your hard drive used for training by clicking on ... or Add files.Move the DIC channel to the Convert from (source) field.Move the DAPI channel to the Convert to (ground truth) field.In Options specify:The number of neural training Iterations (use 1000 by default).Input the name (Name in Explorer) for your AI used in the NIS.ai Explorer (e.g. DIC-DAPI).Specify the Output (Save) where the AI will be stored.Check Save graph screenshot if you want to create a screenshot of the loss chart when the training is done.Click Train to begin the training. Or you can click Add to Queue to run this training using the    later.Wait for the training progress to automatically end when all iterations are done or wait until the curve seems to be constant and then click Finish (see the image “Loss chart overview” below).If the result is not satisfactory then try the following options:Retrain the AI with a training dataset adding more multi-points to increase the variability of the training dataset.Add more iterations in Continue training on. Figure&nbsp;521.&nbsp;Train Convert.ai execution dialog and settings.OptionsContinue training on If you want to teach the existing AI further, check this function and Browse... to a *.cai AI file. See   for more information. Figure&nbsp;522.&nbsp;ND Acquisition of Lambda channel setup. Figure&nbsp;523.&nbsp;Fig. 6 Acquisition of DAPI (Ground truth channel) and DIC (Source channel). [Courtesy of Nikon Imaging Center, RIES at Hokkaido University] Figure&nbsp;524.&nbsp;Loss chart overview.Loss curve (red) should be decreasing over time, faster in the beginning, slower in the end as it approaches zero. There may be a few bumps along the way which is correct. Training time depends on the difficulty of the sample and the use case. To improve the training results, it is best to use multi-point image with higher variability, however more training images require more training time.To test the trained AI:Open the test image only with DIC channel (another but similar to the training dataset).Select Convert.ai in NIS.ai menu and run its execution dialog.Match the Document channels with the Source AI channels (e.g. DIC) by connecting them.Select the Trained AI with the network trained for the particular function from a file (From File) by clicking Browse... or pick it from the explorer (From Explorer).Click OK to run the process.After the progress is finished a new channel containing the result is created (see the resulting image below). Figure&nbsp;525.&nbsp;Convert.ai execution dialog. Figure&nbsp;526.&nbsp;Generating Nuclei from DIC by Convert.ai. [Courtesy of Nikon Imaging Center, RIES at Hokkaido University]Nuclei were generated from trained AI, which converted DIC channel into the DAPI channel. No need to acquire DAPI channel, only DIC channel is needed.To execute the trained AI, perform all steps from the testing trained AI:&nbsp;The created AI can be applied on an infinite number of similar images with acquired under the same conditions. ",
     id: 126 }, 
   { title: "Segment.ai",
     xmlid: "id|nis.ai.modalities.segmentx",
     content: "    Use-case:Segmentation.Identification of rare events.Advantages:Segment sample that are unsegmentable by common methods.Segment samples without a necessity to tune the parameters of the segmentation method for every sample.Segment.ai is trained using the    window and executed using the    window. Figure&nbsp;498.&nbsp; Figure&nbsp;499.&nbsp; Segmentation created by using Threshold, Spot Detection, Auto Detect, Homogeneous Area Detection, created manually by Binary Editor or by any combination of these methods. It is usually best to start by using some common method (Threshold, Spot Detection, etc.) and then manually correct/edit the segmentation to get the desired result. The other method is to create the segmentation manually from scratch.Guidelines for the binary preparing:Segment.ai requires a precise drawing of binary objects (see the image examples below).Also, it is possible to acquire a fluorescence image which can be easily segmented by one of the conventional methods mentioned above.Binary layers should not have any intersection. If an intersection is found, it is automatically deleted before the training. This control process proceeds from the first binary layer to the last one. Therefore the most important objects should be drawn in the first binary layer which is always preserved.Different types of objects should be drawn into separate binary layers. E.g. when segmenting neurons, the neuron body should be drawn into a different layer than the dendrites, because they differ significantly from each other.Background should not be added to the training set because it is generated automatically.Do not crop smaller chunks of the image as training data - always segment the whole image.For successful training, AI needs both positive samples (where objects are located) and negative samples (where objects are not located). For example, if edges of the wells are visible on some images, then the training dataset should contain such images, so that the AI can recognize that these parts of image do not contain any objects. If the AI was trained only on images without visible edges of a well, then it might not work on images which contain it.If the ground truth segmentation is imprecise, then the trained AI will be imprecise as well. That being said, the AI is robust and a few incorrect cells will not confuse it but try to be precise.Keep the same rule for the entire segmentation. Be consistent.Draw lines precisely over the cell boundaries (see example below). Draw the bright border inside the rounded object and do not draw the black border of the rounded object. Keep the same rule for entire drawing and do not mix drawings inside/outside border of the objects. Stay consistent.Segment all objects of the image – if you miss some objects it will lead to errors in training (see images below). If particular frame does not contain any objects then this is not a problem but if it does, you need to segment all of them.The number of required images depends on the difficulty of the sample. When the image seems easy, you may get away with segmenting only 2-3 images (of reasonable size – e.g. 1000 x 1000 pixels; see the example image “Image suitable for the Segment.ai” below). When judging whether the scene is difficult or not, use intuition. If it looks hard, it probably is and it will require more training images thus requiring more iterations to train.The other option is to always segment only few images (e.g. 2-3 images in case of simple scene), train the AI and if the result is not sufficient then add some more images of scenes that are difficult for the AI to segment. Repeat this process until the results are sufficient.&nbsp;&nbsp;Segment all objects of the image. If you miss some objects, it will lead to errors in training (areas marked by the red arrow in the images above). Even the object on the border should be included. Figure&nbsp;500.&nbsp;Image suitable for the Segment.ai. Only 2-3 images are needed for the segmentation. Examples of precise binary object's drawing in DIC: Figure&nbsp;501.&nbsp;The cells without drawn border. [Courtesy of Nikon Imaging Center, RIES at Hokkaido University]  Figure&nbsp;502.&nbsp;Draw the bright border inside the rounded cell. [Courtesy of Nikon Imaging Center, RIES at Hokkaido University]  Figure&nbsp;503.&nbsp;Do not draw the black border of the rounded cell. [Courtesy of Nikon Imaging Center, RIES at Hokkaido University]  Figure&nbsp;504.&nbsp;Keep the same rule for entire drawing and do not mix drawings inside/outside border of the objects; stay consistent. [Courtesy of Nikon Imaging Center, RIES at Hokkaido University]Missing cell segmentation leads to errors: Figure&nbsp;505.&nbsp;Correct segmentation. [Courtesy of Nikon Imaging Center, RIES at Hokkaido University]  Figure&nbsp;506.&nbsp;Incorrect segmentation. [Courtesy of Nikon Imaging Center, RIES at Hokkaido University] Figure&nbsp;507.&nbsp;Correct segmentation. [Courtesy of Nikon Imaging Center, RIES at Hokkaido University]  Figure&nbsp;508.&nbsp;Incorrect segmentation. [Courtesy of Nikon Imaging Center, RIES at Hokkaido University] Figure&nbsp;509.&nbsp;Correct segmentation including border objects. Figure&nbsp;510.&nbsp;Incorrect segmentation.Segment all objects of the image. If you miss some objects, it will lead to errors in training (areas marked by the red arrow in the images above). Even the object on the border should be included. Figure&nbsp;511.&nbsp;Image suitable for the Segment.ai. Only 2-3 images are needed for the segmentation. To properly train the Segment.ai, perform the following steps:Prepare the binary layers (e.g. nuclei) on the channel (e.g. DIC).Open the sample image having the binary layers.Select Train Segment.ai in NIS.ai menu. In the Train Segment.ai dialog window you should:In the Training data  section select file(s) from your hard drive used for training by clicking on ... or Add files.Select your Source AI channels (for example the DIC channel) by dragging the channel rectangle from Available channels to the Source AI channels area.Select the Ground truth binaries  (e.g. nuclei) by dragging the binary rectangle from Available binaries to the Ground truth binaries area.In Options:To exclude a certain area from the training, drag a binary layer into the Cutout area. Image under this binary area will be cut out from the training.Set the number of neural training Iterations (use 1000 by default).Input the name (Name in Explorer ) for your AI used in the NIS.ai Explorer (e.g. nuclei on DIC).Specify the Output (Save) where the AI will be stored.Check Save graph screenshot  if you want to create a screenshot of the loss chart when the training is done.Click Train  to begin the training. Or you can click Add to Queue to run this training using the    later.Wait for the training progress to automatically end when all iterations are done or wait until the curve seems to be constant and then click Finish (see the image “Loss chart overview” below).If the loss curve is not decreasing, try the following options:Retrain the AI with a training dataset having a higher number of binary objects.Add multi-points to increase the variability of the training dataset.Add more iterations in the Continue training on  field. Figure&nbsp;512.&nbsp;Train Segment.ai execution dialog and settings.OptionsContinue training on If you want to teach the existing AI further, check this function and browse (...) to a *.sai AI file. See   for more information. Figure&nbsp;513.&nbsp;Loss chart overview.Loss curve (red) should be decreasing over time, faster in the beginning, slower in the end as it approaches zero. There may be a few bumps along the way which is correct. Training time depends on the difficulty of the sample and the use case. To improve the training results, it is best to use multi-point image with higher variability, however more training images require more training time.To test the trained AI:Open the test image without a binary layer.Select Segment.ai in NIS.ai menu and run its execution dialog.Match the Document channels with the Source AI channels by connecting them.Select the Trained AI  with the network trained for the particular function from a file (From File) by clicking Browse... or pick it from the explorer (From Explorer) (e.g. …\\nuclei_on_DIC.sai).Switch on the Preview. If your trained network creates more than one binary, Multi-Smooth  feature is shown instead. This feature is used to smooth out multiple binaries at once.If you are satisfied with the result, click OK.When the process is finished the binary appears on the image (see the resulting image below). Figure&nbsp;514.&nbsp;Segment.ai execution dialog. Figure&nbsp;515.&nbsp;Segmentation of Nuclei from DIC by Segment.ai. Nuclei were detected from trained AI which was trained on DIC channel and the ground truth binary layer defined by a manual drawing. [Courtesy of Nikon Imaging Center, RIES at Hokkaido University] ",
     id: 127 }, 
   { title: "Segment Objects.ai",
     xmlid: "id|nis.ai.modalities.segment.objectsx",
     content: " This function is suited for segmenting many round objects (e.g. circular cells) close to each other. AI pays special attention to the border area between the cells. The binary objects must not contain any holes. The user interface is the same as in  .Segment Objects.ai is trained using the    window and executed using the    window. Figure&nbsp;516.&nbsp; Figure&nbsp;517.&nbsp; Figure&nbsp;518.&nbsp;Original image (left), Segment.ai (middle), Segment Objects.ai (right) ",
     id: 128 }, 
   { title: "Enhance.ai",
     xmlid: "id|nis.ai.modalities.signal.enhancementx",
     content: "  Use-case:Enhancing the low signal of your sample and thereby saving the sample from light toxicity and prolong viability of your sample.Advantages:Significantly less exposure needed to acquire an image good for analyzing.Less bleaching.Low light toxicity.Longer cell viability.Enhance.ai is trained using the    window and executed using the    window.Enhance.ai is a module based on deep learning technology. It utilizes deep convolutional neural networks to enhance quality of captured images. Since the end of year 2012, deep learning based approaches have provided superior results on various image processing tasks, compared to traditional methods.The module uses a supervised learning approach to enhance quality of captured images. Before training, user must acquire dataset that contains pairs of images captured with two different settings – low quality and high quality (ground truth). This is usually accomplished by capturing the same image with low exposure time and high exposure time. The network is then trained to transform the low quality image into high quality image.Because users train their own networks, the reconstructed image is usually of higher quality compared to using pretrained networks or other methods of image processing. Enhance.ai is therefore a very flexible tool and can be used for any specific use case and any image modality, as long as high quality images for training can be acquired. Training itself is a very straightforward process and does not require any complex knowledge of math or neural networks. All parameters and hyperparameters of training are calculated automatically and without user interaction.While training can be executed on a CPU, for efficiency reasons it is best to use an NVIDIA GPU card. Multiple GPU graphics cards are supported however it is recommended to use an even number of cards all of the same type. Training time varies on given use case and can be anything between 1/2 to 8 hours, depending on the difficulty of the dataset and the performance of the GPU card.It is common knowledge that neural networks require thousands, if not millions of training samples. This is not the case with Enhance.ai, where special care was taken to alleviate this restriction. Most researchers do not have time to capture thousands of images to train the neural network. Enhance.ai therefore typically requires only about 20-100 images. Images should be acquired with same imaging conditions and on the same type of sample that will be used during the actual use of Enhance.ai in the experiments. It is best to acquire several multi-points from the sample, so that the dataset contains as much variability as possible. Finally, care should be taken so that the two channels (low and high quality) are mutually aligned and that there is no shift between channels.Image augmentation is done automatically before training to expand the dataset and to train a more robust network. During training, sub-volumes of data are sampled from the training dataset and are fed into the neural network. The network extracts features from such images, processes them and finally attempts to reconstruct the ground truth image from the corresponding low quality image. It then compares the result and the corresponding ground truth image and updates its parameters accordingly. This process is iterative and over time, the network gets better and better at enhancing images.After the network is trained, it can be used to enhance quality of other similar images, which can be acquired in low quality settings, therefore increasing the throughput of image acquisition and reducing phototoxicity on samples. Enhancing of samples, after the network is fully trained, is fast and is in order of seconds to minutes, depending on the dataset size and the performance of the GPU card.Sometimes, more data might become available after the network has already been trained. In that case it is possible to retrain the network using this additional data. After that, the quality of resulting network will improve, enabling to use the network on more use-cases or in different imaging conditions. Figure&nbsp;527.&nbsp; Figure&nbsp;528.&nbsp; Acquire sample with two channels – low signal channel and high signal channel. Train AI to convert the low signal channel into high signal channel. Once the AI is trained, you no longer have to acquire the high signal channel, thereby preserving your sample from light toxicity.To properly train Enhance.ai, perform the following steps:Acquire sample with two channels – low signal channel and high signal channel. The ideal dataset should consist of the different multi-points representing all types of objects and structures.The low signal channel should use the lowest possible light/acquisition time, while still containing enough relevant information.The high signal channel should use the highest possible light/acquisition time and should contain all the details - this channel will be used as a ground truth image for AI to properly reconstruct the objects and the structures in the low signal channel.Both channels must contain image of the same scene. It is the best to capture both channels in a single experiment e.g. by ND Acquisition (see the “ND Acquisition of Lambda channel setup image below”).Open the sample image, containing both low signal channel and high signal channel.Select the Train Enhance.ai ( ) in NIS.ai menu. In the Train Enhance.ai dialog window you should:In the Training data section select file(s) from your hard drive used for training by clicking on ... or Add files.Select the low signal channels as Source AI channels (e.g. channel TRITC-10ms).Select the high signal channels as Ground truth channels (e.g. TRITC-Best).In Options:The number of training Iterations (use 1000 by default).Type in the name (Name in Explorer) for your AI used in the NIS.ai Explorer (e.g. Channel_TRITC_Result_Comparison).Specify the Output (Save) where the AI will be stored.Check Save graph screenshot if you want to create a screenshot of the loss chart when the training is done.Click Train to begin the training. Or you can click Add to Queue to run this training using the    later.Wait for the training progress to automatically end when all iterations are done or wait until the curve seems to be constant and then click Finish (see the image “Loss chart overview” below).If the loss curve is not decreasing, try the following options:Retrain the AI with a training dataset having a slightly higher signal.Add multi-points to increase the variability of the training dataset.Add more iterations in Continue training on. Figure&nbsp;529.&nbsp;Train Enhance.ai execution dialog and settings.OptionsContinue training on If you want to further train an existing AI, check this function and Browse... to a *.eai AI file (see  ). Figure&nbsp;530.&nbsp;ND Acquisition of Lambda channel setup. Figure&nbsp;531.&nbsp;Loss chart overview.Loss curve (red) should be decreasing over time, faster in the beginning, slower in the end as it approaches zero. There may be a few bumps along the way which is correct. Training time depends on the difficulty of the sample and the use case. Enhance.ai tends to be faster, difficult DIC Segment.ai/Convert.ai are usually slower. To improve the training results, it is best to use multi-point image with higher variability, however more training images require more training time.To test the trained AI:Open the test low signal image (another but similar to training dataset).Select the Enhance.ai execution dialog (see image below).Match the Document channels with the Source AI channels (e.g. TRITC-10ms) by connecting them.Select the Trained AI with the network trained for the particular function from a file (From File) by clicking Browse... or pick it from the explorer (From Explorer).Click OK to run the process.After the progress is finished a new channel containing the result is created (see the resulting image below).If the result is not satisfactory then try the following options:Retrain the AI with a training dataset having a slightly higher signal.Add multi-points to increase the variability of the training dataset.Add more iterations in Continue training on. Figure&nbsp;532.&nbsp;Enhance.ai execution dialog. Figure&nbsp;533.&nbsp;The result of signal enhancement of TRITC by Enhance.ai. After the progress is finished the new channel containing the result is created.To execute the trained AI, perform all steps from the testing trained AI:&nbsp;The created AI can be applied on an infinite number of similar images acquired under the same conditions. ",
     id: 129 }, 
   { title: "Binary operations",
     xmlid: "id|node.binaryoperations_sectionx",
     content: "  Binary x Binary              Single Binary    Classification      Binary operations &gt; Binary x Binary &gt; And🔗 Creates an intersection of the two connected binary results.   Binary operations &gt; Binary x Binary &gt; And Multi🔗 Creates an intersection of the more than two connected binary results.   Binary operations &gt; Binary x Binary &gt; Or🔗 Union of the connected binary results is shown.   Binary operations &gt; Binary x Binary &gt; Or Multi🔗 Union of more than two connected binary results is shown.   Binary operations &gt; Binary x Binary &gt; Xor🔗 Non-Equivalence operation is performed on the two connected binary results.   Binary operations &gt; Binary x Binary &gt; Xor Multi🔗 Non-Equivalence operation is performed on more than two connected binary results.   Binary operations &gt; Binary x Binary &gt; Subtract🔗 Displays subtraction of the connected binary result (A) from the result (B).   Binary operations &gt; Binary x Binary &gt; Equivalence🔗 Equivalence operation is performed on the two connected binary results.   Binary operations &gt; Binary x Binary &gt; Having🔗 Only the objects from the connected binary result (A) being inside or touching the result (B) are shown.   Binary operations &gt; Binary x Binary &gt; Not Having🔗 Only the objects from the first connected result not being inside or not touching the other connected result are shown.   Binary operations &gt; Binary x Binary &gt; First Not Empty🔗 Returns the first not empty binary layer from the parameters, see the example below. Figure&nbsp;784.&nbsp;   Binary operations &gt; Single Binary &gt; Invert🔗 Only the area without the connected binary result is shown.   Binary operations &gt; Classification &gt; Detection Classifier🔗 Compares the ground truth binary layer (A) and the predicted binary layer (B) generated by segmentation using AI. It pairs the objects from both layers, classifies them (based on IoU threshold) and visualizes them as new binary layers:true positives (TP) matched correctly,false positives (FP) incorrectly segmented objects, andfalse negatives (FN) incorrectly missed objects.IoU Threshold Defines the threshold of quantity: Figure&nbsp;785.&nbsp;. ",
     id: 130 }, 
   { title: "Binary processing",
     xmlid: "id|node.binaryprocessing_sectionx",
     content: "  Cell Processing    Basic               Morphology       Circular Morphology         Linear morphology           Detect                Growing           Skeleton           Connectivity     Filter objects       Remove objects     Insert objects                Colors &amp; Numbers         Transformations             JavaScript    Connect slices       Binary processing &gt; Cell Processing &gt; Make Cell🔗 This node is used to process cell and nuclei binaries. Cells without nucleus are removed. Only the largest nucleus in every cell is kept. Both binaries are given IDs so the cell and its nucleus has the same ID. This node processes binaries for the use in Cell measurement.Cell/Nucleus Area Ratio Filter out cells based on the cell nucleus area ratio.   Binary processing &gt; Basic &gt; Clean🔗 Removes binary objects which cannot contain a circle of the specified Radius. Figure&nbsp;741.&nbsp;Example Figure&nbsp;742.&nbsp;Radius Removes those binary objects that cannot contain a circle of specified radius.Matrix Click the button to change the structuring element used for this operation. See  .Count Number of iterations. Reset Sets the dialog default values.   Binary processing &gt; Basic &gt; Close Holes🔗  Removes small holes from the current binary image.Fills all holes in the binary image which would be destroyed by Erosion of the specified parameters.  Figure&nbsp;743.&nbsp;ExampleMatrix Click the button to change the structuring element used for this operation. See  .Count Number of iterations. Reset Sets the dialog default values.   Binary processing &gt; Basic &gt; Close Inside Ref🔗 Morphological closing performs Dilation followed by Erosion (see   ). Close operation is restricted by Parent (Ref) binary and will connect only children in same parent. Node can be used, for example, to connect multiple nuclei in cell.   Binary processing &gt; Basic &gt; Fill Holes🔗  Fills in holes in binary image. Figure&nbsp;744.&nbsp;Example   Binary processing &gt; Basic &gt; Fill if Empty🔗 If the connected binary does not contain any binary information, it is fully filled.   Binary processing &gt; Basic &gt; Invert🔗  Inverts the binary image to its negative.   Binary processing &gt; Basic &gt; Remove Border🔗 Removes binary pixels on the image border.   Binary processing &gt; Basic &gt; Separate Objects🔗  Separates binary objects into multiple smaller objects. The higher the Count, the fewer objects will be separated. Figure&nbsp;745.&nbsp;ExampleMorpho Separate Objects dialog box appears. Figure&nbsp;746.&nbsp;Matrix Click the button to change the structuring element used for this operation. See  .Count Number of iterations.   Binary processing &gt; Basic &gt; Separate Bright🔗 Separates bright objects from the background based on both the binary layer and the intensity. The object detection can be adjusted using the Peak strength  parameter.   Binary processing &gt; Basic &gt; Separate Dark🔗 Separates dark objects from the background based on both the binary layer and the intensity. The object detection can be adjusted using the Peak strength  parameter.   Binary processing &gt; Basic &gt; Smooth🔗  Smooths binary image contours. Figure&nbsp;747.&nbsp;Example  The algorithm is based on weak morphology (combination of appropriate convolution followed by threshold). This operation removes small objects and cuts narrow protrusions. On the other hand it fills in small holes and thin gulfs.  Figure&nbsp;748.&nbsp;It is possible to define the strength of the function by specifying the number of iterations.Radius Smooths every binary pixel using circle of the specified radius. Reset Sets the dialog default values.   Binary processing &gt; Basic &gt; Smooth Objects🔗 Smooths the contours of each binary object present in the connected binary image and correctly separates objects from one another.Radius Smooths every binary pixel using circle of the specified radius.   Binary processing &gt; Morphology &gt; Open🔗  Morphological opening performs   followed by  . This has the effect of clearing all objects which are too small with respect to the specified parameters. Larger structures are not significantly affected. Figure&nbsp;749.&nbsp;ExampleMatrix Click the button to change the structuring element used for this operation. See  .Count Number of iterations. Reset Sets the dialog default values.   Binary processing &gt; Morphology &gt; Close🔗  Morphological closing performs   followed by  . It fills all holes in objects which are too small with respect to the specified parameters. Closely neighboring objects get connected. Figure&nbsp;750.&nbsp;Example Figure&nbsp;751.&nbsp;Matrix Click the button to change the structuring element used for this operation. See  .Count Number of iterations. Reset Sets the dialog default values.   Binary processing &gt; Morphology &gt; Erode🔗  Erosion sets each pixel to the value computed as minimum from all pixels in the matrix. Erosion shrinks objects and removes small ones. A non-convex object can split into several parts. Figure&nbsp;752.&nbsp;ExampleMatrix Click the button to change the structuring element used for this operation. See  .Count Number of iterations. Reset Sets the dialog default values.See Also&nbsp;     Binary processing &gt; Morphology &gt; Dilate🔗  Dilation sets each pixel to the value computed as maximum from all pixels in the matrix. Dilation expands objects and structures in binary image. Neighboring objects are connected and small holes are filled. Figure&nbsp;753.&nbsp;ExampleMatrix Click the button to change the structuring element used for this operation. See  .Count Number of iterations. Reset Sets the dialog default values.See also   .   Binary processing &gt; Circular Morphology &gt; Circular Open🔗 Performs Erosion followed by Dilation. This has the effect of clearing all objects which are too small with respect to the specified parameters. Larger structures are not significantly affected.Radius [px] Pixel neighborhood specifying source-data for the current function. Radius as a dynamic parameter is set in µm units.   Binary processing &gt; Circular Morphology &gt; Circular Close🔗 Morphological closing performs Dilation followed by Erosion. It fills all holes in objects which are too small with respect to the specified parameters. Closely neighboring objects get connected.Radius [px] Pixel neighborhood specifying source-data for the current function. Radius as a dynamic parameter is set in µm units.   Binary processing &gt; Circular Morphology &gt; Circular Erode🔗 Euclidean erosion sets each pixel to the value computed as minimum from all pixels within the specified Radius. This will shrink/remove binary objects in the image.Radius [px] Pixel neighborhood specifying source-data for the current function. Radius as a dynamic parameter is set in µm units.   Binary processing &gt; Circular Morphology &gt; Circular Dilate🔗 Euclidean dilation sets each pixel to the value computed as maximum from all pixels within the specified Radius. This will enlarge/merge binary objects in the image.Radius [px] Pixel neighborhood specifying source-data for the current function. Radius as a dynamic parameter is set in µm units.   Binary processing &gt; Circular Morphology &gt; Circular Clean 🔗 Removes binary objects that cannot contain a sphere of the specified radius i.e. objects smaller than Radius.Radius [px] Pixel neighborhood specifying source-data for the current function. Radius as a dynamic parameter is set in µm units.   Binary processing &gt; Circular Morphology &gt; Circular Close Holes 🔗 Fills all holes which cannot contain a sphere of the specified radius i.e. holes smaller than Radius.Radius [px] Pixel neighborhood specifying source-data for the current function. Radius as a dynamic parameter is set in µm units.   Binary processing &gt; Linear morphology &gt; Linear Open🔗  Performs morphologic opening on binary image using the selected linear matrix. The choice of the structuring element defines the direction in which the binary image will be affected. See   . Figure&nbsp;754.&nbsp;ExampleMatrix Click the button to change the structuring element used for this operation. See  .Count Number of iterations. Reset Sets the dialog default values.   Binary processing &gt; Linear morphology &gt; Linear Open Z 🔗 Highlights structures with a linear pattern using the Linear Open function in the Z direction. Please see   .   Binary processing &gt; Linear morphology &gt; Linear Close🔗  Performs morphologic closing on binary image using the selected linear matrix. The choice of the structuring element defines the direction in which the binary image will be affected. See   . Figure&nbsp;755.&nbsp;ExampleMatrix Click the button to change the structuring element used for this operation. See  .Count Number of iterations. Reset Sets the dialog default values.   Binary processing &gt; Linear morphology &gt; Linear Close Z 🔗 Highlights structures with a linear pattern using the Linear Close function in the Z direction. Please see   .   Binary processing &gt; Linear morphology &gt; Linear Erode🔗  Performs erosion on binary image, it removes a layer of pixels in the direction of the selected matrix. See   . Figure&nbsp;756.&nbsp;ExampleMatrix Click the button to change the structuring element used for this operation. See  .Count Number of iterations. Reset Sets the dialog default values.   Binary processing &gt; Linear morphology &gt; Linear Erode Z 🔗 Highlights structures with a linear pattern using the Linear Erode function in the Z direction. Please see   .   Binary processing &gt; Linear morphology &gt; Linear Dilate🔗  Performs dilation on binary image, it adds a layer of pixels in the direction of the selected matrix.See also   . Figure&nbsp;757.&nbsp;ExampleMatrix Click the button to change the structuring element used for this operation. See  .Count Number of iterations. Reset Sets the dialog default values.See Also&nbsp;     Binary processing &gt; Linear morphology &gt; Linear Dilate Z 🔗 Highlights structures with a linear pattern using the Linear Dilate function in the Z direction. Please see   .   Binary processing &gt; Detect &gt; Centers🔗 Calculates and marks the centroid of the objects found in the connected binary image.   Binary processing &gt; Detect &gt; Centroids🔗 Converts binary objects to their centroids. Select the method of determining centroid positions.  Figure&nbsp;758.&nbsp; Build centroid from:, Signal is Bright, Dark If bright/dark areas dominate aside of the centroid position calculated from just the binary layer, the centroid will be shifted in that direction.Marker Choose a symbol visualizing the centroid.   Binary processing &gt; Detect &gt; Connect Objects🔗  Connects binary objects which are closer to each other than the specified distance by a thin line. Figure&nbsp;759.&nbsp;Maximal Distance Set the maximal distance of two objects to be connected.   Binary processing &gt; Detect &gt; Contour🔗 Transforms binary image to its 1 pixel wide contour. Figure&nbsp;760.&nbsp;Example   Binary processing &gt; Detect &gt; Convex Hull🔗 This function expands non-convex binary image objects to their convex boundaries. Figure&nbsp;761.&nbsp;Example   Binary processing &gt; Detect &gt; Distance Function🔗  Displays the distance of each object pixel to the nearest boundary (background pixel) as an intensity value.The resulting color image is a  .  Figure&nbsp;762.&nbsp;ExampleMatrix Click the button to change the structuring element used for this operation. See  .See Also&nbsp;     Binary processing &gt; Detect &gt; Geodesic Centers🔗 Creates a binary layer with marked geodesic centers.A Geodesic Center is a point within the object, such that the sum of its distances to every other point in the object is minimal. For convex objects, it corresponds to the center of gravity. The algorithm used to calculate the geodesic center is approximate. Figure&nbsp;763.&nbsp;Illustration showing the geodesic center of the green object. This operation on one object may result into multiple centers. Same as if the you use the    node using the matrix. Figure&nbsp;764.&nbsp; Gray color shows the original object, green shows the object after    and red shows the geodesic centers.   Binary processing &gt; Detect &gt; Granulometry🔗  This command creates the granulometry image from the binary image.   Binary processing &gt; Detect &gt; Homotopic Marking🔗  Marks objects with homotopic marks. Homotopic marking is a sequential homotopic thinning. It is used for marking objects. A filled object (with no holes) is transformed to a single point. Every hole leaves a closed contour. Figure&nbsp;765.&nbsp;Example   Binary processing &gt; Detect &gt; Inscribed Circles🔗 Replaces each binary object by its inscribed circle. Inscribed circle is the largest circle that can be contained in the object. Figure&nbsp;766.&nbsp;   Binary processing &gt; Detect &gt; Medial Axis🔗  This command creates medial axis from the current binary objects. Figure&nbsp;767.&nbsp;Matrix Specifies a structuring element for finding the distance to the boundary.Endlines suppression Determines the axis shape. The higher is the value, the more simple is the shape.#Points for direction Determines how many points are used to calculate the direction of the medial axis prolongation. Values are larger than 2.Prolongation Determines whether the medial axis prolongation is used or not.   Binary processing &gt; Detect &gt; Pruning🔗  Sequentially removes end-points from binary image. This function is used for simplification of skeletons by removal of branches. Closed contours remain unchanged. Figure&nbsp;768.&nbsp;ExampleThe following window appears. Figure&nbsp;769.&nbsp;Layers Specifies the number of iterations; 0 stands for complete removal of branches. This command is mainly used for skeletonized images.   Binary processing &gt; Detect &gt; Ultimate Erosion🔗  Sequentially erodes binary image, but leaves small areas which would completely disappear in the next erosion. Figure&nbsp;770.&nbsp;ExampleMatrix Click the button to change the structuring element used for this operation. See  .Count Set 1 for maximum possible erosion. Any number higher than 1 will do fewer erosions and preserve bigger parts of each object.   Binary processing &gt; Growing &gt; Grow by Size🔗 Grows each binary object by a circle (2D) / sphere (3D) of the specified radius. Objects will not be merged together.Radius [px] Pixel neighborhood specifying source-data for the current function.   Binary processing &gt; Growing &gt; Grow to Fill Ratio🔗 Grows objects in the image until the fraction of the total object area to total image area is the same as the parameter Area Fraction.Area Fraction Fraction of total object area to total image area.   Binary processing &gt; Growing &gt; Grow to Intensity🔗 Grows each binary object up to the specified intensity. Objects will not be merged together.Type Grow Bright Regions will grow from maximum intensity to the specified intensity.Grow Dark Regions will grow from minimum intensity to specified intensity.   Binary processing &gt; Growing &gt; Fast Exoskeleton🔗  Creates zones of influence by the fast 'pipe of pixels' algorithm. Performing this command, the {% raw %} {% endraw %} is called and appended to the list of executed functions.   Binary processing &gt; Growing &gt; Make Circle &amp; Ring🔗 For every binary object in the connected image this node creates an interior circle object and a ring around it. It is guaranteed that the corresponding circle and ring have the same object ID and that the number of circles and rings is equal. Places where the circle or ring should disappear both are removed in order to maintain the same IDs. In case it breaks in more objects the biggest is taken and the others are discarded. Circle can be restricted by adding a hole (width of the hole is measured from the circle boarder). Adjust the width of each feature by typing a value into the edit box or using the arrows. Figure&nbsp;771.&nbsp;Features of the Circle &amp; Ring node.   Binary processing &gt; Growing &gt; Thickening🔗 Dilates a binary image without changing the number of objects - it is a homotopical transformation. Figure&nbsp;772.&nbsp;ExampleLayers Specifies the number of iterations, i.e. number of layers being added to each object. Parameter 0 performs ultimate dilation (neighboring objects touch each other).   Binary processing &gt; Growing &gt; Watershed🔗 Performs the watershed (flooding) algorithm starting from the connected binary objects. The flooding is based on image pixel intensities.Type Defines the direction of flooding - from high intensities (From Bright Regions ) or from low intensities (From Dark Regions ).   Binary processing &gt; Growing &gt; Zones of Influence🔗  Creates zones of influence by drawing four connectivity borders. For each pixel of a border between two objects, the distance to both objects is the same. Figure&nbsp;773.&nbsp;Example   Binary processing &gt; Skeleton &gt; Skeletonize🔗  Makes a skeleton of the binary image. The current binary image is dilated and then intersected by reference binary image. This step is repeated until there is no difference in the sequence of consecutive images. The current image may be e.g. erosion of the original image copied to reference binary image. In this case the function reconstructs only bigger objects and rejects smaller objects.Skeleton is a representation that largely preserves extent and connectivity of original binary objects, while pruning most of original pixels. Figure&nbsp;774.&nbsp;Example   Binary processing &gt; Skeleton &gt; Skeleton 🔗 Makes a skeleton of the 3D binary image.See Also&nbsp;     Binary processing &gt; Skeleton &gt; Detect Branching🔗  The purpose of this command is to create 1pixel seeds out of a skeletonized binary image. This function serves for automatic recognition of the intersection points of single-pixel lines. Figure&nbsp;775.&nbsp;Example   Binary processing &gt; Skeleton &gt; Detect Endings🔗  Purpose of this command is to create 1pixel seeds out of a skeletonized binary image. It preserves only ending points of the skeleton and clears all other pixels.   Binary processing &gt; Skeleton &gt; Cut Branches🔗  On a skeletonized binary image, branches (free endings) shorter than the defined length will be removed. Figure&nbsp;776.&nbsp;Example Figure&nbsp;777.&nbsp; Delete branches of maximal length Shorter branches will be deleted.    Binary processing &gt; Skeleton &gt; Connect Free Endings🔗  On a skeletonized image, connects branch peaks. Set sensitivity of the function in the window: Figure&nbsp;778.&nbsp; Connections Set sensitivity of the function. The higher the more connections will be created.Use Gabor Response As Input Apply Gabor edge detection to the image before evaluating the edge strength.   Binary processing &gt; Skeleton &gt; Vanish Small Objects🔗  On a skeletonized image, circular (enclosed) objects with equivalent diameter ( ) smaller than the defined value will be deleted. You do not necessarily want to delete small objects but the ones which are mis-detected.  Figure&nbsp;779.&nbsp; Vanish objects with eqdia less than: Objects with   smaller than this value will be erased.but keep objects with border stronger than: Set the threshold value for deletion. Binary objects containing a strong-enough edge in the color image will be preserved regardless of the   value.Use Gabor Response As Input Apply Gabor edge detection to the image before evaluating the edge strength.   Binary processing &gt; Skeleton &gt; Remove Short Filaments🔗 Shortens each branch of a skeleton by the specified amount in pixels.   Binary processing &gt; Connectivity &gt; Make 4-Connective🔗  This operation adds pixels to ensure that each pixel which is only 8-connected will become 4-connected. 4-connected pixels are those connected vertically or horizontally, but not diagonally.See  .   Binary processing &gt; Connectivity &gt; Make 8-Connective🔗  This operation removes pixels to ensure that each pixel which is only 4-connected will become 8-connected. 8-connected pixels are those either vertically, horizontally, or diagonally. See  .   Binary processing &gt; Filter objects &gt; Filter Objects🔗,   Binary processing &gt; Filter objects &gt; Filter Objects  Filters the connected objects using a filter or multiple filters. Set a filter by selecting the filtering Feature, setting the Comparator  and the numeral Value. To add another filter, click  New Filter.... To delete a filter, click .   Binary processing &gt; Filter objects &gt; Select Objects🔗 Selects binary objects from the binary image which are present in input table.Object Id Column This drop-down menu allows the user to select the column that represents the Object ID.   Binary processing &gt; Filter objects &gt; Largest Objects🔗 Selects the number of largest objects specified in the Count of objects  box.   Binary processing &gt; Filter objects &gt; Delete Objects🔗 Deletes binary objects from the binary image which are present in input table.Object Id Column This drop-down menu allows the user to select the column that represents the Object ID.   Binary processing &gt; Remove objects &gt; Touching Borders🔗 Removes binary objects touching the image border.   Binary processing &gt; Remove objects &gt; Touching Frame🔗 Removes binary objects touching the image frame. Select units and set the width, height and position of the frame. Then choose a mode defining which objects are to be removed.   Binary processing &gt; Insert objects &gt; Circle🔗 Draws a circle to the binary image.Center x, Center y Coordinates of the center of the drawn object.Radius Radius of the circle in Units.Filled Closed shapes can be either filled or hollow. Select TRUE to fill the shape or FALSE to leave it hollow.Mode Select how to interact with the existing binary layer.Copy Clears the input binary image.Logical OR Performs logical OR between the binary layer and the drawn object.Logical AND Performs logical AND between the binary layer and the drawn object.Subtract Subtracts the drawn object from the binary layer.Units Select whether the coordinates/dimensions of the shape will be specified in pixels or in the image calibration units.   Binary processing &gt; Insert objects &gt; Ellipse🔗 Draws an Ellipse to the binary image.Center x, Center y Coordinates of the center of the drawn object.Radius a Length of the semi-major axis in Units.Radius b Length of the semi-minor axis in Units.Filled Closed shapes can be either filled or hollow. Select TRUE to fill the shape or FALSE to leave it hollow.Mode Select how to interact with the existing binary layer.Copy Clears the input binary image.Logical OR Performs logical OR between the binary layer and the drawn object.Logical AND Performs logical AND between the binary layer and the drawn object.Subtract Subtracts the drawn object from the binary layer.Units Select whether the coordinates/dimensions of the shape will be specified in pixels or in the image calibration units.   Binary processing &gt; Insert objects &gt; Frame🔗 Adds a rectangle frame to the connected binary layer. Define the frame by entering the distance from the image borders (Left, Top, Right, Bottom) with proper Units. Define whether the frame will be Filled or not and set the visualization Mode determining the interaction between the binary layer and the new frame.   Binary processing &gt; Insert objects &gt; Line🔗 Draws a line to the binary image.Start x, Start y, End x, Start y Coordinates of the endpoints in pixels.Connectivity 4 or 8, defines the way a line is drawn. See  .Mode Select how to interact with the existing binary layer.Copy Clears the input binary image.Logical OR Performs logical OR between the binary layer and the drawn object.Logical AND Performs logical AND between the binary layer and the drawn object.Subtract Subtracts the drawn object from the binary layer.Units Select whether the coordinates/dimensions of the shape will be specified in pixels or in the image calibration units.   Binary processing &gt; Insert objects &gt; Line Between Objects🔗 Draws a line connecting centers of two largest binary objects in the connected binary layer.Center Select how center of object is calculated - Center, Centroid (Light signal), Centroid (Dark signal).Type Select type of line (Segment Line, Line, Perpendicular).Count Number of parallel lines.Step Distance between neighboring parallel lines.Offset Line offset in perpendicular direction.Connectivity 4 or 8, defines the way a line is drawn.Mode Select how to interact with the existing binary layer.Copy Clears the input binary image.Logical OR Performs logical OR between the binary layer and the drawn object.Logical AND Performs logical AND between the binary layer and the drawn object.Subtract Subtracts the drawn object from the binary layer.   Binary processing &gt; Insert objects &gt; Parallel Lines🔗 Draws multiple parallel lines to the binary image.Distance Distance between two adjacent lines defined in Units.Orientation Angle of the lines.Offset Offsets the parallel lines by the Offset amount (using the specified Units), adjusting them perpendicularly to their original direction.Connectivity 4 or 8, defines the way a line is drawn. See  .Mode Select how to interact with the existing binary layer.Copy Clears the input binary image.Logical OR Performs logical OR between the binary layer and the drawn object.Logical AND Performs logical AND between the binary layer and the drawn object.Subtract Subtracts the drawn object from the binary layer.Units Select whether the coordinates/dimensions of the shape will be specified in pixels or in the image calibration units.   Binary processing &gt; Insert objects &gt; Rectangle🔗 Draws a rectangle to the binary image.Position X, Position y Coordinates of the top-left corner of the rectangle in Units.Width, Height Rectangle dimensions in Units.Filled Closed shapes can be either filled or hollow. Select TRUE to fill the shape or FALSE to leave it hollow.Mode Select how to interact with the existing binary layer.Copy Clears the input binary image.Logical OR Performs logical OR between the binary layer and the drawn object.Logical AND Performs logical AND between the binary layer and the drawn object.Subtract Subtracts the drawn object from the binary layer.Units Select whether the coordinates/dimensions of the shape will be specified in pixels or in the image calibration units.   Binary processing &gt; Insert objects &gt; Markers🔗 Draws a grid of markers to the binary image.Shape Select shape for the grid markers.Distance X X axis distance between two adjacent markers defined in Units.Distance Y Y axis distance between two adjacent markers defined in Units. To enable different X and Y values, unlock this edit field using the lock button.Offset X X axis offset distance of the markers defined in Units.Offset Y Y axis offset distance of the markers defined in Units. To enable different X and Y values, unlock this edit field using the lock button.Units Select whether the coordinates/dimensions of the shape will be specified in pixels or in the image calibration units.Mode Select how to interact with the existing binary layer.Copy Clears the input binary image.Logical OR Performs logical OR between the binary layer and the drawn object.Logical AND Performs logical AND between the binary layer and the drawn object.Subtract Subtracts the drawn object from the binary layer.   Binary processing &gt; Insert objects &gt; Marker🔗 Draws one marker to the binary image.Shape Select the marker shape.Position X, Position y Coordinates of the marker in Units.Units Select whether the coordinates/dimensions of the shape will be specified in pixels or in the image calibration units.Mode Select how to interact with the existing binary layer.Copy Clears the input binary image.Logical OR Performs logical OR between the binary layer and the drawn object.Logical AND Performs logical AND between the binary layer and the drawn object.Subtract Subtracts the drawn object from the binary layer.   Binary processing &gt; Insert objects &gt; Multiple Circles🔗 Insert arbitrary number of circles from a table into the binary image.   Binary processing &gt; Insert objects &gt; Multiple Fixed Circles🔗 Insert arbitrary number of fixed circles from a table into the binary image.   Binary processing &gt; Insert objects &gt; Multiple Rectangles🔗 Insert arbitrary number of rectangles from a table into the binary image.   Binary processing &gt; Insert objects &gt; Multiple Lines🔗 Insert arbitrary number of lines from a table into the binary image.   Binary processing &gt; Insert objects &gt; Plane 🔗 Draws a plane to the binary volume. Plane is defined by coordinates of any point belonging to it, by the space orientation and mode of interacting with the existing layer. Select the units and coordinates of any point belonging to the plane. Then choose a parallel Plane and Mode for interacting with the existing layer.ModeCopy Clears the input binary image.Logical OR Performs logical OR between the binary layer and the drawn object.Logical AND Performs logical AND between the binary layer and the drawn object.Subtract Subtracts the drawn object from the binary layer.   Binary processing &gt; Colors &amp; Numbers &gt; Color by Id🔗 This function displays the binary objects in several different colors. The algorithm is optimized so that two neighboring objects would never display in similar colors.   Binary processing &gt; Colors &amp; Numbers &gt; Color by Value🔗 Assigns any color to any value of the selected source column used from the connected table.In the following scenario, positive and negative numerical values are divided into two intervals labeled with different colors. Therefore all negative values will be assigned a red color and all positive values will be assigned a green color. Figure&nbsp;780.&nbsp;Positive and negative values divided into two intervals by number 0. Figure&nbsp;781.&nbsp;Example of the color assignment defined above.Source column Table source column.Binary Id column Table binary ID column. Add bin (after selected) Adds a new bin to the table. Remove selected bin Removes the selected bin from the table. Remove all bins Removes all bins from the table. Generate bins automatically Opens the Generate Bins  window where it is possible to set the parameters generating the bins automatically. Please see the description here:  . Paste from clipboard Inserts the table data from clipboard into the frequency table. Copy colors to clipboard Copies the full table to clipboard.   Binary processing &gt; Colors &amp; Numbers &gt; Renumber Objects🔗 If you have deleted some objects , this command renumbers all the remaining ones so that numbering starts from 1 and is continuous. Only numbers greater than the deleted ones are renumbered.   Binary processing &gt; Colors &amp; Numbers &gt; Renumber by Table🔗 Renumbers the binary objects with values taken from the connected table. Source Column has to be specified.Object Id Column This drop-down menu allows the user to select the column that represents the Object ID.   Binary processing &gt; Colors &amp; Numbers &gt; Separate Ids🔗 Separates connected objects in colorized (labeled) binaries and makes the objects valid for further processing. These objects cannot be created by default processing in GA3 but can be the output of JavaScript or Python nodes. For example, some Python libraries do not separate labeled objects.   Binary processing &gt; Colors &amp; Numbers &gt; TMA Renumber🔗 Renumbers the binary objects of detected TMA (Tissue microarray) cores. Select the Orientation and then set the indexing start (Index from) or choose the Meander indexing. The settings are shown in the preview scheme below. Select Continuous indexing to not include numbers of missing cores in indexing (so binaries will be numbered from 1 to N, where N is the count of binary objects).This node automatically detects columns and rows of grid of TMA cores. Then it assigns new indexes so that cores in the upper row have smaller indexes than cores in the lower row. In each row, cores are indexed from left to right. Numbers of missing cores are not used (for example, if you have 10 cores in a 4x3 grid, core in the bottom right corner (if not missing) will always have an index of 12).For good results the cores should be laying in a square grid which is not rotated. This node can be used on any binary objects laying in a square grid (not just TMA cores). Figure&nbsp;782.&nbsp;TMA Renumbering Example (before) Figure&nbsp;783.&nbsp;TMA Renumbering Example (after)   Binary processing &gt; Transformations &gt; Add Borders🔗 Adds a border to the binary layer. Define the Top, Left, Right and Bottom borders and set the Value of the borders (1 = is part of the binary layer, 0 = is not part of the binary layer).   Binary processing &gt; Transformations &gt; Crop🔗 Crops the connected binary image to the specified width and height [px]. Shift the cropping rectangle by filling the Start x and Start y position.   Binary processing &gt; Transformations &gt; Change Canvas🔗 Crops to the center of the connected binary image by the specified width and height [px].   Binary processing &gt; Transformations &gt; Fit Size🔗 Shrinks the source binary image so that it fits into a square with a given size. Aspect ratio of the binary is maintained as well as all other properties.Size Number of pixels of the longer side of the image.   Binary processing &gt; Transformations &gt; Flip🔗,   Binary processing &gt; Transformations &gt; Flip  Flips the source 2D/3D binary horizontal and/or vertical. Simply check Horizontal flip  or Vertical flip  to flip the image in the particular direction.   Binary processing &gt; Transformations &gt; Move🔗 Shifts the connected binary image horizontally (X Offset) or vertically (Y Offset). Area shifted outside the image borders can be placed to the opposite side if Rotate values around image borders  is checked.   Binary processing &gt; Transformations &gt; Resize🔗 Resizes the binary image by a given ratio.Width Ratio in X axis.Height Ratio in Y axis.   Binary processing &gt; Transformations &gt; Resize to Ref🔗 Resizes the source binary image (A) so that it has the same size as the reference binary image (B).   Binary processing &gt; Transformations &gt; Rotate🔗 Rotates the connected binary image by the specified angle around the center of the image. If you need to change the center of rotation, you can use the    node, rotate the image, and then crop it back to its original size.   Binary processing &gt; JavaScript &gt; JS Postprocess🔗 Transforms the source into the result with transformation programmed in JavaScript. Up to 6 binary/color/table inputs can be added.   Binary processing &gt; Connect slices &gt; Connect 🔗 Connects 2D slices from subsequent frames into the 3D binary. Most of the time, it is not necessary to use this node. All 3D binary processing nodes do this operation automatically. Use it only when the final binary result is 2D and is desired to be 3D. This node does not change the underlying binary data.   Binary processing &gt; Connect slices &gt; Connect Cells 🔗 Connects 2D slices from subsequent frames which have the largest overlap into 3D binary object.    Binary processing &gt; Connect slices &gt; Connect Cells IoU 🔗  ",
     id: 131 }, 
   { title: "Data management",
     xmlid: "id|node.datamanagement_sectionx",
     content: "  Basic         Grouping       Sort &amp; Select          Table Manipulation          Statistics         Statistical tests         Position          Processing        Curve Fitting       Detect        Sequences           JavaScript        Data management &gt; Basic &gt; Accumulate Records🔗 Measurement on the whole image file is performed frame-by-frame (volume-by-volume). This action can be used once or multiple times to obtain records (table rows) from all frames (or volumes) in the selected loop. By default (without accumulate) for performance reasons, records are generated per frame (or per volume). In order to work (sorting, filtering, calculating statistics, ...) on a larger record, use accumulate.Accumulate loop Which loop to accumulate. Select All Loops to process all data at once.Apply only on current frame in preview Action is applied just on the current frame.   Data management &gt; Basic &gt; Append Columns🔗 Columns from the input tables are appended in the order that they appear. This is useful when joining more features measured on the same number of objects. Number of records across all input tables is expected to be the same. Figure&nbsp;795.&nbsp;Append Columns action results shown in the bottom table. Left table belongs to “Records 1” whereas the right table belongs to “Records 2”. EqDiameter feature is duplicated.   Data management &gt; Basic &gt; Calculated Column🔗 This node simply works as a calculator with pre-defined operators. Existing columns  or their statistics (see   ) can be used as variables. Type of the calculated column and Unit should be set first. Once the calculation is written, confirm it by clicking Apply.   Data management &gt; Basic &gt; Modify Columns🔗 Selects which features are displayed in the resulting table, change their order using the arrows, change their name (New Title ), number format (New Format ) and Precision. The Modify Table Columns dialog having the same functionality as this node can be opened directly from Analysis Results by clicking   .   Data management &gt; Basic &gt; Reduce Records🔗 Groups records and computes the selected statistic of rows. If the table is grouped the statistic is calculated for each group. Each group of rows becomes one row in the output table.For more information about grouping see   , for more information about statistics see   .   Data management &gt; Basic &gt; Scale Column Data🔗 This node is used for changing units, skipping values and similar tasks. Select the Column which is about to be changed, define the new Unit (NewVal example is shown below) and set the value Offset or Gain. No new column is created, only the values in the source column are skipped.   Data management &gt; Grouping &gt; Aggregate Rows🔗 Aggregate Rows computes the selected statistic of rows. If the table is grouped the statistic is calculated for each group. Each group of rows becomes one row in the output table. Table&nbsp;8.&nbsp;Aggregation StatisticsNoneAny value (typically first).TotalNumber of values including empty (null).CountNumber of values not including empty (nulls).*DistinctNumber of distinct (unique) values.Mean . SUM(values)/N.*Median  of all ordered values.*SumAll values added.*MinMinimum value.*MaxMaximum value.*StDev.PPopulation  .*StDev.SSample  .*Var.PPopulation  .*Var.SSample  .*VarCoef.PPopulation  .*VarCoef.SSample  .*StErr.PPopulation  .*StErr.SSample  .*Skewness.PPopulation  .*Skewness.SSample  .*Kurtosis.PPopulation  .*Kurtosis.SSample  .*RootMeanSquare.PPopulation .*RootMeanSquare.SSample .*CoalesceFirst not empty (non-null) value.LastMinusFirstLast value minus first (values[N-1] - values[0])FirstFirst value (values[0])LastLast value. (values[N-1])Mode .*Entropy .*HistoUniformityUniformity of histogram (link to Measurement - Uniformity).*QuartileQ1First  .*QuartileQ3Third  .*PercentileP011st  .*PercentileP055th  .*PercentileP1010th  .*PercentileP9090th  .*PercentileP9595th  .*PercentileP9999th  .** Does not include empty (null) values.   Data management &gt; Grouping &gt; Filter Groups🔗 Filters whole groups (selected in the Column) based on the selected Statistics per group. Define the filtering using the Comparator  drop-down menu and the Value edit box.This node uses aggregation statistics. Please see   .   Data management &gt; Grouping &gt; Group Records🔗 All rows are in one group by default. Rows having an equal given column (selected in this node) form a group which is visualized in the table. Select the feature(s) in the Group by drop-down menu. To remove a grouping feature, switch it back to “---”.Grouping is used with statistics (Aggregate Rows) for calculating per group (BinaryLayer, ZStack, Object...) aggregates. Only limitation is that the result of the grouping is currently not visible in the preview and in the final result.   Data management &gt; Grouping &gt; Ungroup Records🔗 Previously grouped records are ungrouped by this action.   Data management &gt; Sort &amp; Select &gt; Current Records🔗 Filters records associated to the current frame. This node is useful only when the connected table is accumulated.   Data management &gt; Sort &amp; Select &gt; Filter Records🔗 This action can be used to filter table records (rows) using a selected column. Resulting table contains only the rows satisfying the filter condition applied to the given column.Column Column to be filtered.Comparator Comparison operator.Value Comparison operand. Example&nbsp;10.&nbsp;Filtering out Spots not having any Cell as a parent. Figure&nbsp;796.&nbsp; Example&nbsp;11.&nbsp;Filter Records action used in series to act as an intersection. Both conditions (10 &lt;= EqDiameter AND 0.9 &lt;= Circularity) must be met at the same time. Figure&nbsp;797.&nbsp; Example&nbsp;12.&nbsp;Filter Records action used n parallel to act as an union. One or both conditions (10 &lt;= EqDiameter OR 0.9 &lt;= Circularity) must be true. Figure&nbsp;798.&nbsp;   Data management &gt; Sort &amp; Select &gt; Pivot Table🔗 This node “pivots” the input table by the specified Pivot Column  (“ObjectId” in the example below), thus it creates a new column for each “ObjectId” value. The new column will contain values of columns having a column Role  in the bottom definition table (“Entity” and “EqDiameter” in the example below). E.g. in the second column of the results table, the “EqDiameter” values for objects with ID==1 are arranged in rows by the “ZStackIndex”. The number of rows depends on the possible combinations of all values of columns marked with the role Row. Each column will be present for the first number of values specified in the Column Count  edit box. The order of the columns is specified by the Column Order  switch. In our example below, Append would add 10x Entity whereas Mix  orders them: Entity, EqDia, Entity, EqDia, etc. Column Suffix  sets the name of the created column. PIVOT_NAME (“ObjectId” in our example) and PIVOT_VALUE (“ObjectId” value) or any other text can be used here. Figure&nbsp;799.&nbsp;Pivot Table example Figure&nbsp;800.&nbsp;Pivot Table example results   Data management &gt; Sort &amp; Select &gt; Select First &amp; Last🔗 Shows the first and last record from the connected table.   Data management &gt; Sort &amp; Select &gt; Select Records🔗 Select Records action takes a specified number of records (Count) from a selected starting row (First Row ). Typical use-case is to select the first row of a sorted record set.   Data management &gt; Sort &amp; Select &gt; Select Top🔗 Reorders the records based on a given column and then shows the defined number of records (either smallest or biggest). This node is a combination of    and   .   Data management &gt; Sort &amp; Select &gt; Sort Records🔗 Sort action reorders records so that the selected column is sorted in an ascending order.   Data management &gt; Table Manipulation &gt; Append Records🔗 Appends records (rows) from input tables one by one into the output table. If an input table contains a column already present (has same column ID) in the output table (from the preceeding input tables) it will use it (i.e. not append a new column).The resulting table is ordered by the identification columns.Column ID&nbsp;Every column has an implicit ID (invisible to the user) given to it by the node that creates the column. The ID is used internally to reference columns. Therefore, even after a column is renamed it is still correctly pointed to by subsequent nodes. Consequently, columns are considered “same” if they have the same ID. Special identification columns like Loop indexes, Object Entity, Object IDs are given the same ID for each special column. Therefore the ObjectID column will be considered the same from all tables. This behavior is usually expected. If not, it can be altered with these nodes:   ,    and   .Appending records is useful when joining two tables with the same columns. For example for joining two disjunct filtrations or aggregations. Figure&nbsp;801.&nbsp; Figure&nbsp;802.&nbsp; Figure&nbsp;803.&nbsp; Figure&nbsp;804.&nbsp;   Data management &gt; Table Manipulation &gt; Compact Columns🔗 Aggregates all columns with the same title (or title different just in numerical suffix) to the first column using the First Valid rule.This is useful when    or    produces duplicate colums because of their different IDs (please see    for more on Column IDs).For more control on which columns will merge use Copy Column ID.The following example demonstrates how to force two different columns to merge into single one using   . Figure&nbsp;805.&nbsp;After append records there are two columns “MeanObjectIntensity” and “MeanObjectIntensity2” that need to be merged into one. Figure&nbsp;806.&nbsp; Figure&nbsp;807.&nbsp;   Data management &gt; Table Manipulation &gt; Copy Column ID🔗 The resulting table contains all columns from table A with specified columns IDs replaced with IDs from reference table B.This is useful when tables A and B are intended to be merged using    or    node and they contain columns with different IDs (please see    for more on Column IDs) that should be treated as same column.There is a simpler automatic way using   .The following example demonstrates how to force two different columns to merge into a single one using   . Figure&nbsp;808.&nbsp; Figure&nbsp;809.&nbsp; Figure&nbsp;810.&nbsp;   Data management &gt; Table Manipulation &gt; Duplicate Column🔗 Duplicates the selected column.   Data management &gt; Table Manipulation &gt; Join Records🔗 This action is useful for merging incompatible tables:frames and objectscells (objects) and nuclei (objects)It joins records from two or more unrelated (with different number of rows and columns) record sets. Resulting table contains a union of all distinct input table columns (loop indexes, entity and Object Ids are considered the same).Select the type of join (see below) and click    to add a table row where you specify the relation between the joined features taken from two different tables. Figure&nbsp;811.&nbsp;Join Records action results shown in the bottom table. Left table belongs to “Objects” whereas the right table belongs to “Frame”.Inner Join Inner Join outputs a Cartesian product of the related rows (rows where “using columns” have same value):{% raw %}CountRows(R) = CountRows(A) {% endraw %}× CountRows(B) × CountRows(C) × ...,where A, B, C, ... are input tables and R is an output table.A related row (e.g. ZStackIndex = 3) must be present in all tables.Left Join Left Join is an inner join plus all related rows which do not occur in tables to the right. All rows from the leftmost table (A) are in the result table (R).Right Join Right Join is the same as the Left Join but with reversed input cables (... C, B, A).Outer Join Outer join is a union of the left and right join.Please see   for more information.   Data management &gt; Table Manipulation &gt; New Column ID🔗 Makes the specified columns IDs unique (please see    for more on Column IDs).It is useful when nodes like   ,    and others treat a column coming from two different tables as the same one and does not include it twice. This is typically because both columns were made by one node or the is a “system” column like “Loop Index Columns”, “Entity” or “Object ID”. Figure&nbsp;812.&nbsp; Figure&nbsp;813.&nbsp; Figure&nbsp;814.&nbsp; Figure&nbsp;815.&nbsp;Without the    node both source columns are merged (overwritten) into one. Figure&nbsp;816.&nbsp;   Data management &gt; Table Manipulation &gt; Shift Records🔗 Shifts data in selected columns by given number of rows.FillEmpty Rows which are empty after shift are left empty.Original Rows which are empty after shift are filled with original values.Cycle Rows which are empty after shift are filled with values of rows on the other end of columns.   Data management &gt; Statistics &gt; Aggregate Columns🔗 Aggregate Columns action creates a new column with the given name (Title) in which the result of a calculation is placed (Aggregation). The calculation is based on the checked parameters.Title Name of the new column.Aggregation Statistics to be computed.Column Checked columns will be used for calculation.This node uses aggregation statistics. Please see   .   Data management &gt; Statistics &gt; Binning🔗 Creates new bin column and assigns each row to one bin based on the value of the selected column. Then the rows can be grouped based on the bins.Source column Column for binning.New column Name of the new bin column.Column Type Sets the column type either to a Number or a String (sequence of characters).Unit Unit of the new bin column. Add bin Adds a new table row (one bin). Remove selected bin Deletes a row record (one bin). Remove all bins Clears the table (deletes all bins).  Generate bins automatically Opens the Generate Bins  window used to calculates linear or logarithmic bins based on the bin width. Set the lower starting value of the bin set (Min), then the higher ending value (max) and enter the width of the bin. Choose whether the generated bins will be Linear or Logarithmic with a specified base. Set the Label for each newly generated bin. Enter “lo” to start from the lowest generated value or “hi” to start from the lowest generated value + width. Any other mathematical combination of “lo” and “hi” is also possible (see the examples below). Figure&nbsp;817.&nbsp;Linear method generating the middle value of each bin. Figure&nbsp;818.&nbsp;Logarithmic method generating logarithmic bins to base 10. Validate bins If the bins are created improperly, this function automatically corrects the classes so that they follow each other. Copy to clipboard Copies the full table to clipboard. Paste from clipboard Inserts the table data from clipboard into the frequency table.lo, hi, value Fill in the table so that lo  and hi  values represent the bin size and value sets the user defined label to each bin. When lo or hi is empty (null), it is interpreted as negative infinity or positive infinity respectively. Bins should be exclusive otherwise the behavior is not defined.   Data management &gt; Statistics &gt; Binning (simple)🔗 Creates a new column and fills it with bin labels to which the value in the source column falls into. Bins are equidistant intervals between min and max.New column name Name of the new column.Source column Column for binning.Min (incl.) Minimal bin value.Max (excl.) Maximal bin value.Count of bins Number of bins.Class label Sets options how each bin is labeled (Start point: min of each bin, Middle point: middle of each bin, Bin class Id: bin index starting from 1.   Data management &gt; Statistics &gt; Frequency Table🔗 This table can be used to classify the results data to see the number of elements in each defined class. Select the Source column, name the New column and add units (Unit). Fill in the table so that From and To values represent the bin size (range of source values) which will be substituted by the new value specified in the third column.Other tools are similar to the    node.   Data management &gt; Statistics &gt; Generate Distribution🔗 (requires:  )Creates a table with points of Probability density function or Cumulative distribution function. Select the Distribution, its Parameters, left and right interval endpoints and Step. If checked, y- values of the Critical Region are generated to a separate column. Tested Value is generated to a separated column as well.   Data management &gt; Statistics &gt; Statistics🔗 Creates a summary statistics table, as seen in the example below. Figure&nbsp;819.&nbsp;This node uses aggregation statistics. Please see   .   Data management &gt; Statistical tests &gt; ANOVA One-way🔗 (requires:  )Performs One-way ANOVA (analysis of variance). Please see  .Select a column with Sample Data and parameters of the test. The table must be grouped. Every group is one factor (i.e. treatment group). The output results of the analysis are displayed in a one row table.Working example from a Wikipedia page:Create a table with the example data and group it by the “group” column. Figure&nbsp;820.&nbsp;Analysis definition. Figure&nbsp;821.&nbsp;JS Create Table node.{% raw %}return [\n[ 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3 ],\n[ 6, 8, 4, 5, 3, 4, 8, 12, 9, 11, 6, 8, 13, 9, 11, 8, 7, 12 ]\n];{% endraw %} Figure&nbsp;822.&nbsp;Grouped data.Setup the ANOVA and get the results. Figure&nbsp;823.&nbsp;ANOVA One-way node. Figure&nbsp;824.&nbsp;Results.   Data management &gt; Statistical tests &gt; F-test🔗 (requires:  )This test can be used to test:the hypothesis that the means of a given set of normally distributed populations, all having the same standard deviation, are equal.the hypothesis that a proposed regression model fits the data well.the hypothesis that two normal populations have the same variance.Please see  .Select Sample A  from table A, Sample B  from table B and parameters of test. If tables are grouped, then count of the groups in each table must be same. F-test is done for each pair of the groups (i-th group in table A and i-th group in table B). Output is a table with one row of data for each group pair.   Data management &gt; Statistical tests &gt; t-test 1s🔗 (requires:  )This is a location test of whether the mean of a population has a value specified in a null hypothesis (please see  ).Select Sample and parameters of test. If table is grouped, then the t-test is done for each group. Output is a table with one row of data for each group.   Data management &gt; Statistical tests &gt; t-test 2s pair🔗 (requires:  )Paired samples t-tests typically consist of a sample of matched pairs of similar units, or one group of units that has been tested twice (a \"repeated measures\" t-test). A typical example of the repeated measures t-test would be where subjects are tested prior to a treatment, say for high blood pressure, and the same subjects are tested again after treatment with a blood-pressure-lowering medication. By comparing the same patient's numbers before and after treatment, we are effectively using each patient as their own control (please see  ).Select Sample A, Sample B and parameters of test. If table is grouped, then the t-test is done for each group. Output is a table with one row of data for each group.   Data management &gt; Statistical tests &gt; t-test 2s unpair🔗 (requires:  )The independent (unpaired) samples t-test is used when two separate sets of independent and identically distributed samples are obtained, one from each of the two populations being compared. For example, suppose we are evaluating the effect of a medical treatment, and we enroll 100 subjects into our study, then randomly assign 50 subjects to the treatment group and 50 subjects to the control group. In this case, we have two independent samples and would use the unpaired form of the t-test (please see  ).Select Sample A from table A, Sample B from table B and parameters of test. If tables are grouped, then count of the groups in each table must be the same. t-test is done for each pair of groups (i-th group in table A and i-th group in table B). Output is a table with one row of data for each group pair.   Data management &gt; Statistical tests &gt; Z-factor🔗 Calculates   for positive and negative control groups.Labels Select column containing labels (positive, negative and others).Values Select column containing measured control values.Negative Label Enter label of rows with negative control (in Labels column).Positive Label Enter label of rows with positive control (in Labels column).Z-factor Column Check to create column with Z-factor value and enter its name.   Data management &gt; Position &gt; Addition🔗 Vectorally adds two positions. Z positions are optional. The result is two or three new columns (X, Y, (Z)).ResX = X0 + X1, ...   Data management &gt; Position &gt; Difference (vector)🔗 Vectorally subtracts two positions in time. Z positions are optional. The result is two or three new columns (X, Y, (Z)).ResX = X0 - X1, ...   Data management &gt; Position &gt; Distance🔗 Calculates the distance between two points in time. Z positions are optional. The result is two or three new columns (X, Y, (Z)).Dist = SQRT( (X0 - X1)^2 + (Y0 - Y1)^2 + (Z0 - Z1)^2 )   Data management &gt; Position &gt; Optimal path🔗 Sorts given points represented by their positions in the most effective order for scanning.   Data management &gt; Position &gt; Vector Length🔗 Calculates the length of the vector. If Z (Vector Z) column is left blank 2D length is calculated: Figure&nbsp;825.&nbsp;Vector Length in 2D Figure&nbsp;826.&nbsp;Vector Length in 3D   Data management &gt; Position &gt; Vector Orientation🔗 Calculates the heading of the vector. Heading is the angle in degrees between the positive X axis and the  counterclockwise in range &lt;0; 360&gt;. Elevation is calculated as well when the Z (Vector Z) column is given. Elevation is the angle in degrees between the XY plane and the  in range &lt;-90; 90&gt;.   Data management &gt; Position &gt; Stage Transformation🔗 Transforms (recalculates) the position coordinates between the stage (absolute) and the image (relative) system.Position X, Position Y, Position Z Defines the X, Y, Z positions.Transformation Defines the source and destination format of the position.New Column Name of the transformed features.Unit Unit of the transformed features.   Data management &gt; Processing &gt; Find Local Extrema🔗 Only the local extreme values found in the source Column are copied into the new column. Name the New Column and specify its units (Unit). Select which Extrema are taken into account and adjust the Threshold to filter out subtle changes.   Data management &gt; Processing &gt; Rolling Average🔗 Calculates the rolling average value as an average of three consecutive values (current value and the value before and after the current one). Select the Column from which the rolling average will be calculated, name the New Column and specify its units (Unit). Use Window Radius  to expand the number of consecutive values from which the average is calculated.   Data management &gt; Processing &gt; Rolling Median🔗 Calculates the rolling median value as a median of three consecutive values (current value and the value before and after the current one).   Data management &gt; Processing &gt; Rolling Minimum🔗 Calculates the rolling minimum value as a minimum of three consecutive values (current value and the value before and after the current one).   Data management &gt; Processing &gt; Rolling Maximum🔗 Calculates the rolling maximum value as a maximum of three consecutive values (current value and the value before and after the current one).   Data management &gt; Curve Fitting &gt; Curve Fitting🔗 Fits a curve to data using the method of least squares. Fit values are placed into a new column. Select the Dependent Column and Independent Column, choose the Curve type and specify the Outputs.DataIndependent Column Column with independent data, [x]Dependent Column column with dependent data, [y=f(x)]ModelCurve Type of the curve in case of higher polynomial curve its degree n. Table&nbsp;9.&nbsp;Curve typeCurveFormulaParametersMean (constant)  a0Linear  a1, a0Quadratic  a2, a1, a0Higher Polynomial  an, ... , a2, a1, a0Simple ExponentialA, BGaussianA, µ, σ Gaussian curve is calculated using weighted linear least squares, if it cannot be calculated this way, nonlinear least square method is used instead and  is 0.use significant parameters only P-value is often between 0 and 1, higher value shows that parameter is less significant. You can filter out all values higher than given value. This is done iteratively until all p-values are lower than given value.OutputsFitted value Approximated value given by the resulting equation.Equation of Trend Line Resulting equation which can be used in a linechart.R2 Displays goodness of fit, between 0 and 1, higher means better.Coefficients Appends columns with parameters.p-values Appends columns with p-values.   Data management &gt; Curve Fitting &gt; Dose Response🔗 Fits a Dose-Response curve to data using the method of non-linear least squares. Fitted values are placed into a new column.DataDose Column Column with independent data, [x] i.e. Dose.Zero Handling Substitute (with a value entered in the blank field) or discard zero.Response Column Column with dependent data, [y=f(x)] i.e. Response.ModelCurve Type of the curve in case of higher polynomial curve its degree n. Table&nbsp;10.&nbsp;Curve typeCurveFormulaParameters4PL (Symmetrical)  B, T, E, H5PL (Asymmetrical)  B, T, E, H, S You can constrain (fix) any of the parameters. Table&nbsp;11.&nbsp;ParameterMeaningBBottom (minimum of the function).TTop (maximum of the function).EInflection point of the curve.HHill (hill coefficient), gives direction and how steep the response curve is.SGives assymetry around the inflection point.EC50, IC50, LD50, ... are same as E for the 4LP model, for 5PL they are calculated as follows: Figure&nbsp;827.&nbsp; Output columns Adjust the naming of the output columns.   Data management &gt; Curve Fitting &gt; Gauss Mixture🔗 Fits multiple gaussian curves to data using the method of nonlinear least squares.DataDependent Column Column with dependent data, [y=f(x)].Independent Column Column with independent data, [x].ModelNumber of Peaks Number of gaussian curves n, number of outputed curves can be smaller. Table&nbsp;12.&nbsp;Curve typeCurveFormulaParametersMultiple Gaussians  Ai, μi, σiOutputsFitted value Approximated value given by the resulting equation.Equation of mixture Resulting equation containing all curves, can be used in a linechart.Separate equation for each peak Appends column for each fitted curve.Coefficients Appends columns with parameters.   Data management &gt; Curve Fitting &gt; Growth🔗 This node is similar to the    node. It fits a growth curve to data and is used by the    and   .   Data management &gt; Detect &gt; DBSCAN🔗 This Density-Based Spatial Clustering of Applications with Noise (DBSCAN ) action clusters 2D points. Cluster IDs are inserted into a new column. Name the new column (New Column Name ), select the Position X Column  and Position Y Column , set the Max. Distance in Cluster  and the Min. Number of Neighbours . Two points are neighbors if their distance from each other is not greater than the Maximal Distance in Cluster. If any point has at least Minimal Number of Neighbours, this point and all his neighbors are added to the same cluster. If any point is part of a cluster, all of his neighbors are also part of this cluster.For details about this action, please see the  .   Data management &gt; Detect &gt; Grid Points🔗 Detects a square grid and appends rows with coordinates of the missing points. Select columns containing coordinates of the grid points and select what to ignore during the detecting of the missing points.   Data management &gt; Detect &gt; Parse Well Name🔗 Recognizes and parses well names to row and column. Table&nbsp;13.&nbsp;Input ColumnWellRowWellColumnAA01AA0101AA01AAABAB0101   Data management &gt; Detect &gt; TMA Dearraying🔗 Inserts a new column containing new indexes of binary objects of detected TMA (Tissue microarray) cores. Choose the Position X Column and Position Y Column with position of core centers, select the Orientation and then set the indexing start (Index from) or choose the Meander indexing. The settings are shown in the preview scheme below. Select Continuous indexing to not include numbers of missing cores in indexing (so binaries will be numbered from 1 to N, where N is the count of binary objects).This node automatically detects columns and rows of grid of TMA cores. Then it assigns new indexes so that cores in the upper row have smaller indexes than cores in the lower row. In each row, cores are indexed from left to right. Numbers of missing cores are not used (for example, if you have 10 cores in a 4x3 grid, core in the bottom right corner (if not missing) will always have an index of 12).For good results cores should be laying in a square grid which is not rotated. This node can be used on any binary objects laying in a square grid (not just TMA cores).   Data management &gt; Detect &gt; Values Run🔗 (requires:  )Detects run of the same values and returns the column with IDs. Same ID indicates the same values in the Column for consecutive indexes in the Index Column. New Column adjusts the name of the new column.   Data management &gt; Sequences &gt; Difference🔗 Calculates a new column as a difference (subtraction) between the second and first value, third and second, etc. Select the Column from which the difference will be calculated, choose the Loop, name the New Column  and specify its units (Unit). First record is blank when First item is selected.   Data management &gt; Sequences &gt; Integrate🔗 Calculates a new column as a sum of the first and the second value, second and the third value, etc. First row stays the same. Select the Column from which the integration will be calculated, choose the Loop, name the New Column and specify its units (Unit). Enter a Constant if you want to add a number to each sum of the two values.   Data management &gt; Sequences &gt; Position Difference🔗 Computes difference (vector) X, Y, Z columns in every two consecutive rows. Figure&nbsp;828.&nbsp; Figure&nbsp;829.&nbsp; Figure&nbsp;830.&nbsp;   Data management &gt; Sequences &gt; Position Integrate🔗 Adds up X, Y, Z columns in every two consecutive rows. Figure&nbsp;831.&nbsp; Figure&nbsp;832.&nbsp; Figure&nbsp;833.&nbsp;   Data management &gt; Sequences &gt; High Pass Filter🔗  Figure&nbsp;834.&nbsp;Alpha calculation ΔT is a time interval [s] and fc is a cut-off frequency [Hz].Please see   for more details.   Data management &gt; Sequences &gt; Low Pass Filter🔗  Figure&nbsp;835.&nbsp;Alpha calculationΔT is a time interval [s] and fc is a cut-off frequency [Hz].Please see   for more details.   Data management &gt; Sequences &gt; Sequence (Int)🔗 Generates a new column with an integer sequence. Name the New Column and set the Start and Step value of the sequence. Optionally the sequence can respect a selected loop over column value if switched from the default “&lt;rows&gt;” value. New Column adjusts the name of the new column.   Data management &gt; Sequences &gt; Sequence (Exp)🔗 Generates a new column with an exponential sequence. Name the New Column and set the Start and Factor (exponent) value of the sequence. To create an inverted exponential sequence, click Invert factor and the Factor value is automatically changed. Optionally the sequence can respect a selected Loop over column value if switched from the default “&lt;rows&gt;” value.   Data management &gt; JavaScript &gt; JS Create Column🔗 Transforms the source record set into the resulting record set that has a new column. The transformation is to be programmed in JavaScript.See the dedicated documentation:  .   Data management &gt; JavaScript &gt; JS Create Table🔗 Transforms N source record sets into resulting single record set. The transformation is to be programmed in JavaScript.See the dedicated documentation:  .   Data management &gt; JavaScript &gt; JS Scalar Expression🔗 Outputs a scalar record set by evaluating an expression on its scalar inputs. Figure&nbsp;836.&nbsp; ",
     id: 132 }, 
   { title: "Image Operations",
     xmlid: "id|node.imageoperations_sectionx",
     content: "  Arithmetics             Constant         Functions          Logical       Merge       Image Operations &gt; Arithmetics &gt; Add🔗 Performs addition of the connected results (A+B).   Image Operations &gt; Arithmetics &gt; Subtract🔗 Performs subtraction of the connected results (A-B).   Image Operations &gt; Arithmetics &gt; Multiply🔗 Performs multiplication of the connected results (A*B).   Image Operations &gt; Arithmetics &gt; Divide🔗 Performs division of the connected results (A/B).   Image Operations &gt; Arithmetics &gt; Average🔗 Converts the connected channels to the intensity channel. Merged channels are taken as N channels, not 1.   Image Operations &gt; Arithmetics &gt; Minimum🔗 Displays minimum of the connected results.   Image Operations &gt; Arithmetics &gt; Maximum🔗 Displays maximum of the connected results.   Image Operations &gt; Arithmetics &gt; Covariance🔗 Each pixel of the resulting image is calculated using two source images (A and B) as follows: Figure&nbsp;716.&nbsp;where E[X] is mean value over all pixels. The covariance is then calculated as mean pixel value ( ). Resulting image is always floating point image.   Image Operations &gt; Arithmetics &gt; Mask Image🔗 The connected color image is masked by the connected binary image.   Image Operations &gt; Arithmetics &gt; Remove Shading🔗 Applies the input B image to the input A image as the shading correction image. Formula for each pixel (multiplicative shading correction): output = ( mean(A) / mean(A / B) ) * (A / B). A The image to be correctedB Correction imageSee also:   .   Image Operations &gt; Constant &gt; Add🔗 Adds an offset to the connected image.   Image Operations &gt; Constant &gt; Subtract🔗 Subtracts offset from the connected image.   Image Operations &gt; Constant &gt; Multiply🔗 Adds gain to the connected image.   Image Operations &gt; Constant &gt; Divide🔗 Divides the connected image by the divisor.   Image Operations &gt; Constant &gt; Clip🔗 Clips the connected image based on the Min and Max values.   Image Operations &gt; Constant &gt; Constant Value🔗 Each pixel of this action result gets the specified value (Constant).   Image Operations &gt; Functions &gt; Abs🔗 Resulting image is an absolute value of the source.   Image Operations &gt; Functions &gt; Invert🔗 Intensity value of each pixel is calculated as the maximal value in the image minus the current value.   Image Operations &gt; Functions &gt; Exp🔗 Pixel exponentiation with the given Base .   Image Operations &gt; Functions &gt; Inv🔗 Resulting image is an inversion of the source. Each pixel value is reciprocal (1/x) of the corresponding source pixel. Resulting image is always a floating point image.   Image Operations &gt; Functions &gt; Log🔗 Resulting image is a logarithm of the source.Base Logarithm base.   Image Operations &gt; Functions &gt; Pow🔗 Pixel power with the given Exponent .   Image Operations &gt; Functions &gt; Sign🔗 Computes   for each pixel: Figure&nbsp;717.&nbsp;The image is converted to floating-point.   Image Operations &gt; Logical &gt; Greater🔗 Logical function greater than.   Image Operations &gt; Logical &gt; Greater or Equal🔗 Logical function greater than or equal to.   Image Operations &gt; Logical &gt; Equal🔗 Logical function equal to.   Image Operations &gt; Logical &gt; Not Equal🔗 Logical function not equal to.   Image Operations &gt; Merge &gt; Blend🔗 Merges Foreground channel a with Background b channel using Weight channel w.for floating-point weight   Figure&nbsp;718.&nbsp;for integer weight  and max weight value   Figure&nbsp;719.&nbsp;   Image Operations &gt; Merge &gt; Conditional🔗 Merges True channel a with False channel b using Condition channel c. Figure&nbsp;720.&nbsp; ",
     id: 133 }, 
   { title: "Image Processing",
     xmlid: "id|node.imageprocessing_sectionx",
     content: "  Deconvolution    Background         Contrast           Convolution       Denoising          Detection            Insert    Morphology        Linear Morphology       Linear Morphology 3D       Labels &amp; Classes    Transformations             JavaScript       Image Processing &gt; Deconvolution &gt; Deconvolution🔗 Please see   for more information about deconvolution.   Image Processing &gt; Background &gt; Auto Shading Corr.🔗 Performs the automatic shading correction. Choose the type of correction which best represents your image background.   Image Processing &gt; Background &gt; Rolling Ball🔗 This function estimates the background intensity by rolling a ball of the defined radius over/under the image intensities. Figure&nbsp;691.&nbsp;Rolling Ball Background Correction SchemeImage intensity (signal is Bright)Ball of the defined radiusEstimated background Figure&nbsp;692.&nbsp;Radius Set radius in pixels. The value should be bigger than size of the biggest object in the image.Signal is Choose signal intensity - Bright for fluorescence images, Dark for bright-field images.   Image Processing &gt; Background &gt; Shading Correction🔗 This command defines parameters of the background shading correction. Figure&nbsp;693.&nbsp;Background smoothness Defines the smoothness of the curve estimating the background. The lower the number the more smooth the curve is. With higher numbers the local changes become more accented.Correction Select on which background the correction is performed:Light Background (Brightfield)Black Background (Fluorescence)Average Background (DIC, PhC)See also   .   Image Processing &gt; Background &gt; Shading under ROI🔗 Performs the background shading correction only inside the ROI (Region Of Interest) area.Correction Select on which background the correction is performed:Light Background (Brightfield) For Brightfield.Black Background (Fluorescence) For Fluorescence.Average Background (DIC, PhC) For Differential Interference Contrast or Phase Contrast.See also   .   Image Processing &gt; Background &gt; Subtract Background🔗 Subtracts the background of the connected image using the connected binary mask. Select the category of information that the mask contains (Background or Objects).   Image Processing &gt; Background &gt; Subtract Constant🔗 Subtracts a value from the connected result. Fill the value to be subtracted in the edit box.   Image Processing &gt; Background &gt; Ax Shading Correction🔗 Compensates image shading of the current image using a correction image specific for Confocal Microscope AX. Adjust the Strength using the slider.   Image Processing &gt; Contrast &gt; Auto Contrast🔗 Enhances contrast of the connected result automatically based on the Low/High values.Low Pixels with an intensity less than Low are set to zero.High Pixels with an intensity greater than High are set to 255.   Image Processing &gt; Contrast &gt; Contrast🔗  Increases contrast of the image. It changes intensities of the current image. Hue and saturation are not affected. Intensities are rescaled according to the selected method. Figure&nbsp;694.&nbsp;OptionsAuto Sets the Range so that the limits correspond to the minimum and maximum intensities found in the image.Gamma The parameter for the Gamma Correction method. It ranges from 0.05 to 5.00. Set Gamma &lt; 1 to get more information in dark areas or set Gamma &gt; 1 to enhance bright parts of the image.Range Set the low/high contrast intensities. Pixels with intensity greater than the high value set will be changed to a pure white, whereas pixels with intensity smaller than the low value set will be changed to pure black (zero). The range itself will be stretched to fit the available intensity range.OK Closes the dialog window and applies the contrast settings.Cancel Closes the dialog window without applying the contrast settings.Help Opens this help page.Preview Turns the preview on/off.TypeLinear, Logarithmic, Exponential These transformations stretch the intensity interval defined by the range to the full intensity range using the selected curve. All values outside the range are set to pure black and pure white.Equalized Equalization transformation leads to uniform intensity distribution in the specified interval &lt;Low, High&gt;.Gamma correction Gamma correction maps the intensity interval &lt;Low, High&gt; according to exponential equation with the gamma parameter.   Image Processing &gt; Contrast &gt; Gauss-Laplace🔗  This command increases sharpness of the image. However, the scale in which the sharpening is performed is bigger than standard - i.e. large objects are affected, not tiny details. Figure&nbsp;695.&nbsp;Power Set the power value. The possible range of values is from 1.01 to 2.Default Sets the default value to the power text box. The default value is 1.25.   Image Processing &gt; Contrast &gt; Local Contrast🔗  Enhances contrast of the current image while accentuating its details. The contrast is enhanced within both bright and dark areas of the image. Figure&nbsp;696.&nbsp;Degree Contrast multiplicator which is locally proportional to contrast amplification.Radius Size of the neighborhood to be processed.   Image Processing &gt; Contrast &gt; Sharpen🔗  This function sharpens the image. Figure&nbsp;697.&nbsp;Before - after applying the Sharpen function   Image Processing &gt; Contrast &gt; Sharpen Slightly🔗  This function slightly sharpens the image. Figure&nbsp;698.&nbsp;Before - after applying the Sharpen Slightly function   Image Processing &gt; Contrast &gt; Ultra Details🔗 Enhances details on the alpha channel (Alpha) and the standard channel (Details) and removes the noise which originates in the detail enhancement (Noise).   Image Processing &gt; Contrast &gt; Unsharp Mask🔗  Sharpens the image using the unsharp masking technique. Figure&nbsp;699.&nbsp;Power Set the strength of the sharpening effect. The possible range of values is from 0.01 to 1.Area Define the area size which is used during calculating the result. The larger the area is, the smoother the result will be.   Image Processing &gt; Convolution &gt; Common Filters🔗 Convolves the image with the selected common filter.Sobel (east), Sobel (north), Sobel (south), Sobel (west) Edge detection ( )Perwittt (east), Perwittt (north), Perwittt (south), Perwittt (west) Edge detection ( )   Image Processing &gt; Convolution &gt; Gaussian Filters🔗 Applies standard Gaussian filters to the color image. The result is a  .Filter Gaussian  Blurs the image.Gaussian ∂x, Gaussian ∂y First derivative. Detects (emphasizes) edges in the specified axis direction.Gaussian ∂x2, Gaussian ∂y2 Second derivative. Detects (emphasizes) edges in the specified axis direction.Sigma Parameter σ (dispersion) for the Gaussian filter. For the basic Gaussian filter, it determines extent of the blur. For the other filters, it determines the size of edges that are going to be detected.   Image Processing &gt; Convolution &gt; Golay Filter🔗  Smooths and detects edges of color image. Figure&nbsp;700.&nbsp;Count Number of steps used in the algorithm.Filter Type Type of filter.SmoothVertical EdgesHorizontal EdgesEdges Golay filter function uses polynomial (second order) fitting of the image data for smoothing image. Fitting is performed in the neighborhood defined by Kernel parameter. The Vertical Edges detection is derived from the 1st derivation of image data in the X direction. The same is true for Horizontal Edges detection (Y direction). General edge detection is calculated as the sum of the absolute values of the 1st derivations in X and Y directions. Edge detection includes dynamics corrections. Algorithm exactly calculates edge values.   Image Processing &gt; Convolution &gt; Mexican Hat🔗  This function performs filtration on intensity component (or on every selected component - when working with multichannel images) of an image using convolution with 5x5 kernel. Mexican Hat kernel is defined as a combination of Laplacian kernel and Gaussian kernel, it marks edges and also reduce some noise.   Image Processing &gt; Denoising &gt; Denoising🔗 Denoising action can be used to reduce any kind of noise in the image (Gaussian, Poisson noise). The described neighborhood is 3 dimensional - i.e. information is taken from the neighboring frames.Denoising method Original  Original method is based on denoising via pixel large neighborhoods in a spatial and frequency (incl. wavelets coefficients) meaning. It is a one-pass algorithm.Regression Regression algorithm iteratively denoises every pixel according to its local neigborhood. In every iteration, local linear regression is computed for every pixel and the value is replaced by the regressed value. This method belongs to the same family as the Original method.Fusion Fusion method is a fusion between Original and Regression. It takes the better parts of both algorithms.Bayes Bayes method is slow, but it uses a high-quality algorithm. A probabilistic approach is used to compute the most likely estimate from the neighboring pixels.Iterative prediction Iterative prediction is very similar to the Regression method but in the new iteration, we find a new value by voting from extrapolation of neighboring pixels.Denoising Power Define the strength of the denoising algorithm for each channel separately using the slider or the edit box. If the slider is set to zero, denoising is done with an estimated noise variance. Move the slider to the right and denoising calculates with higher noise variance (more noise is present in the image) and vice versa.   Image Processing &gt; Denoising &gt; Denoising (fast)🔗 Fast denoising works the same as    with the exception that described the neighborhood is 2 dimensional - i.e. information is taken only from the current frame.   Image Processing &gt; Denoising &gt; Destriping🔗 Removes stripes created by the VisiTech iSIM device.Strength Defines the strength of the stripe removing.   Image Processing &gt; Denoising &gt; Low Pass Filter🔗 This command defines a filter, which passes only details larger than the set pixel value. Figure&nbsp;701.&nbsp;Keep details from level Set the size of details which will be kept. Smaller details will be suppressed.   Image Processing &gt; Denoising &gt; Median🔗  This function filters small irregularities in images.Radius Select the size of neighborhood used for Median filtering. Reset Sets the dialog default values.   Image Processing &gt; Denoising &gt; Smooth🔗  This command performs smoothing on the color image.Matrix Click the button to change the structuring element used for this operation. See  .Count Number of iterations. Reset Sets the dialog default values.   Image Processing &gt; Denoising &gt; Smooth (fast)🔗 Approximation of the standard Gaussian  filter. Results should be similar to   , but faster.Sigma Parameter σ (dispersion) for the Gaussian filter. For the basic Gaussian filter, it determines the extent of the blur.   Image Processing &gt; Detection &gt; DIC Objects🔗  This command can enhance objects in a DIC image so that the objects can be thresholded reasonably.The Differential interference contrast capturing method produces special kind of images, where objects look three-dimensionally having glow on one side and shadow on the other side. Such objects can not be thresholded and further analysed. The Detect DIC Objects command performs recognition of the objects and makes them bright in order to be easily thresholded. Figure&nbsp;702.&nbsp;Detect DIC Objects dialog for Multichannel imagesAngle Specify the angle of illumination in the edit box.   Image Processing &gt; Detection &gt; Edges🔗  This command enhances the edges in the image.   Image Processing &gt; Detection &gt; Filaments🔗,   Image Processing &gt; Detection &gt; Filaments  Detects and highlights 2D/3D filament structures present in the connected color image (e.g. neurons, blood vessels, ...). Set the diameter range of the filaments in your sample, set how many times this range is evaluated (Steps) and enhance the result using Contrast.   Image Processing &gt; Detection &gt; Gabor Edge Detect🔗     represents a linear filter useful for visualizing specific features of an image. Use the Orientation combo box to select a direction [°] in which the features are visualized. Select a proper direction and adjust the Amplitude of the wavelet to find the sharpest result. Real, Imaginary or Both filters can be applied. Switching from Positive to Negative value highlights the surroundings of the object. Figure&nbsp;703.&nbsp;Orientation Select a direction in degrees in which the features are visualized.Amplitude Adjust the amplitude of the wavelet to find the sharpest result.Filter Apply one of the filters or both.Use Value Switching from Positive to Negative highlights the surroundings of the object.   Image Processing &gt; Detection &gt; Gradient Morpho🔗  Detects edges by morphological transformations of color images. Figure&nbsp;704.&nbsp;Matrix Click the button to change the structuring element used for this operation. See  .Count Number of iterations. Reset Sets the dialog default values. Morphologic gradient is the difference between dilated and eroded images. It enhances edges.   Image Processing &gt; Detection &gt; Peaks🔗  Enhances small light objects.Matrix Click the button to change the structuring element used for this operation. See  .Count Number of iterations. Reset Sets the dialog default values. Detect Peaks command enhances small light objects by “Top Hat” morphologic transformation. The size of objects selected is determined by the size of the used structuring element, which depends both on Matrix and Number parameters. This command enables the specific segmentation of small objects to the exclusion of larger objects and also can help you in the case of non-homogeneous background.   Image Processing &gt; Detection &gt; Regional Maxima🔗  Detects regional maxima. It is a subset of top hat transformations.Matrix Click the button to change the structuring element used for this operation. See  .Count Number of iterations. Reset Sets the dialog default values.   Image Processing &gt; Detection &gt; Regional Minima🔗  The Regional Minima function detects regional minima. It is a subset of top hat transformations.Matrix Click the button to change the structuring element used for this operation. See  .Count Number of iterations. Reset Sets the dialog default values.   Image Processing &gt; Detection &gt; Valleys🔗  Detects edges by morphological transformations on color images.Matrix Click the button to change the structuring element used for this operation. See  .Count Number of iterations. Reset Sets the dialog default values. The Detect Valleys command enhances small dark objects by “Top Hat” morphologic transformation. The size of the selected objects is determined by level of the transformation, which depends on Matrix type and on Number of steps. This command enables the specific segmentation of small objects to the exclusion of larger objects and also can help you in the case of non-homogeneous background.   Image Processing &gt; Insert &gt; Gradient Image🔗 Inserts a grayscale color gradient over the connected image. The gradient is defined by a gradient table where the first column sets the color change points and the second column defines the color of the segment (0 = black, 1 = white). Use decimals to set a specific change position or shade of gray. Angle of the gradient can be adjusted optionally.   Image Processing &gt; Morphology &gt; Open🔗  Performs morphological opening on current color image. Morphological opening is erosion followed by the same number of dilations. The transformation removes small light objects. If the structuring element dimension has an odd value, there are two enhanced pixels in structuring element depicting centers: one for erosion and the other for dilation. Figure&nbsp;705.&nbsp;Matrix Click the button to change the structuring element used for this operation. See  .Count Number of iterations. Reset Sets the dialog default values.   Image Processing &gt; Morphology &gt; Close🔗  Performs morphological closing on the current color image. Figure&nbsp;706.&nbsp;Matrix Click the button to change the structuring element used for this operation. See  .Count Number of iterations. Reset Sets the dialog default values.Morphological closing is a dilation followed by the same number of erosions. Small dark areas are removed by this transformation. If the structuring element dimension has an odd value, there are two enhanced pixels in structuring element depicting centers: one for erosion and the other for dilation.   Image Processing &gt; Morphology &gt; Erode🔗  Performs morphologic erosion on color image. Erosion affects the intensity of color image. Hue and saturation are not affected. Dark areas grow whereas light areas shrink. Figure&nbsp;707.&nbsp;Matrix Click the button to change the structuring element used for this operation. See  .Count Number of iterations. Reset Sets the dialog default values.See Also&nbsp;     ,      ,   ,      Image Processing &gt; Morphology &gt; Dilate🔗  Performs morphological dilation on color image. Dilation of color images changes their intensity. Light areas grow and small dark objects and structures disappear. Hue and saturation are not affected. Figure&nbsp;708.&nbsp;Matrix Click the button to change the structuring element used for this operation. See  .Count Number of iterations. Reset Sets the dialog default values.   Image Processing &gt; Morphology &gt; Fill Holes🔗 Fills holes in the connected color image. Holes which are connected to image borders remain intact. Figure&nbsp;709.&nbsp;Example   Image Processing &gt; Linear Morphology &gt; Linear Open🔗  Removes small light areas in the direction specified by Matrix. Figure&nbsp;710.&nbsp;Matrix Click the button to change the structuring element used for this operation. See  .Count Number of iterations. Reset Sets the dialog default values.   Image Processing &gt; Linear Morphology &gt; Linear Close🔗  Closes color image using a linear structuring element. Figure&nbsp;711.&nbsp;Matrix Click the button to change the structuring element used for this operation. See  .Count Number of iterations. Reset Sets the dialog default values. Linear morphological closing is a dilation followed by erosion using the same linear structural element. The transformation is performed on the intensity component and removes small dark areas in the direction specified by Matrix orientation. Number specifies level of closing.   Image Processing &gt; Linear Morphology &gt; Linear Erode🔗  Erodes color image using a linear structuring element. Figure&nbsp;712.&nbsp;Matrix Click the button to change the structuring element used for this operation. See  .Count Number of iterations. Reset Sets the dialog default values. Linear erosion affects the intensity of color image in one direction. Hue and saturation are not affected. Dark areas linearly grow whereas light areas linearly shrink in the direction defined by Matrix orientation. It is an anisotropic transformation.   Image Processing &gt; Linear Morphology &gt; Linear Dilate🔗  Erodes color image using a linear structuring element. Figure&nbsp;713.&nbsp;Matrix Click the button to change the structuring element used for this operation. See  .Count Number of iterations. Reset Sets the dialog default values. Linear dilation of color images changes their intensity in one direction, specified by Matrix orientation. Light areas linearly grow and small linear dark objects and structures disappear. Hue and saturation are not affected. It is an anisotropic operation.   Image Processing &gt; Linear Morphology 3D &gt; Linear Open Z 🔗 Highlights structures with a linear pattern using the Linear Open function in the Z direction. Please see   .   Image Processing &gt; Linear Morphology 3D &gt; Linear Close Z 🔗 Highlights structures with a linear pattern using the Linear Close function in the Z direction. Please see   .   Image Processing &gt; Linear Morphology 3D &gt; Linear Erode Z 🔗 Highlights structures with a linear pattern using the Linear Erode function in the Z direction. Please see   .   Image Processing &gt; Linear Morphology 3D &gt; Linear Dilate Z 🔗 Highlights structures with a linear pattern using the Linear Dilate function in the Z direction. Please see   .   Image Processing &gt; Labels &amp; Classes &gt; Smooth🔗 This node is used to smooth out rough patches of classes. For every pixel it evaluates the surrounding area and takes the most frequent pixel.Radius Size of the computation element.   Image Processing &gt; Transformations &gt; Add Borders🔗 Adds grayscale color borders to the connected image. Enter the border size [px] for each side (Top, Left, Right, Bottom) and enter the color value (0=black, 255=white).   Image Processing &gt; Transformations &gt; Binning🔗 Reduces size of the input image based on the binning factor and binning method.Binning factor Binning factor of 3x3 will reduce the area of 9 pixels into a single pixel in the resulting image based on the selected method.Binning method The resulting pixel value will be calculated as maximum, minimum, mean or sum. Sum No Noise method brightens the image but does not amplify noise like the sum method does.   Image Processing &gt; Transformations &gt; Crop🔗 Crops the connected image to the specified width and height [px]. Shift the cropping rectangle by filling the Start x and Start y position.   Image Processing &gt; Transformations &gt; Change Canvas🔗 Crops to the center of the connected image by the specified width and height [px].   Image Processing &gt; Transformations &gt; Fit Size🔗 Shrinks the source image so that it fits into a square with a given size. Aspect ratio of the image is maintained as well as all other properties.Size [px] Number of pixels of the longer side of the image.Method Interpolation method used for rescaling.   Image Processing &gt; Transformations &gt; Flip🔗 Flips the source image horizontal and/or vertical. Simply check Horizontal flip  or Vertical flip  to flip the image in the particular direction.   Image Processing &gt; Transformations &gt; Move🔗 Shifts the connected color image horizontally (X Offset) or vertically (Y Offset). Area shifted outside the image borders can be placed to the opposite side if Rotate values around image borders  is checked.   Image Processing &gt; Transformations &gt; Resize🔗 Resizes the image by a given ratio.Width Ratio in X axis.Height Ratio in Y axis.Method Interpolation method used for rescaling.   Image Processing &gt; Transformations &gt; Resize to Ref🔗 Resizes the source image (A) so that it has the same size as the reference image (B).Method Interpolation method used for rescaling.   Image Processing &gt; Transformations &gt; Rotate🔗 Rotates the source image by the specified angle around the center of the image. If you need to change the center of rotation, you can use the    node, rotate the image, and then crop it back to its original size.   Image Processing &gt; JavaScript &gt; JS Preprocess🔗 Transforms source color image into the resulting color image. The transformation is to be programmed in JavaScript.See the dedicated documentation:  . Figure&nbsp;714.&nbsp;See the dedicated documentation:  .   Image Processing &gt; JavaScript &gt; JS Preprocess float🔗 Transforms source color image into the resulting float image. The transformation is to be programmed in JavaScript. Figure&nbsp;715.&nbsp;See the dedicated documentation:  . ",
     id: 134 }, 
   { title: "Measurement",
     xmlid: "id|node.measurement_sectionx",
     content: " 2D Basic         Estimates     Metadata      Field pixel values     Resolution     Object pixel values      Distance     JavaScript     3D Basic        Metadata      Volume voxel values    Distance    Object voxel values    Z-stack measurements   Measurement &gt; Basic Node input/output overview for all Basic nodes. Table&nbsp;7.&nbsp;Measurement nodeInputOutputFieldChannel, Mast?Row per fieldObjectBinary, Channel*Row per objectObject CountBinary+, Channel*Row per fieldParentParent, Child, Channel*Row per parent objectChildrenParent, Child, Channel*Row per child objectCellCells, Nuclei, Spots?, Channel*Row cellwhere:? ... optional,+ ... at least one,* ... zero or more.In order to reduce the number of nodes required for a recipe all nodes include features from more generic measurements. These are repeated in every row if needed. For example:All measurements include frame metadata to be able to include time filename from the same node.Object measurement includes all features from frame measurement, eliminating the need to add a separate field measurement node to obtain data such as the measured area.   Measurement &gt; Basic &gt; Field🔗,   Measurement &gt; Basic &gt; Volume  The Field node measures the intensity features in the whole image field. The output is a table where every row corresponds to one field.Optionally, a binary input (a mask) can be connected in order to reduce the measured area. Only the pixels under the mask are measured. Figure&nbsp;786.&nbsp;Field node.All Basic measurement nodes are set in a similar way with variations in the list of measurement features depending on the node type and their dimension (2D/3D).Select the measurement feature from the Add Feature list. Click on it when you see the + symbol next to its name. It is added to the right list showing all measured features. Use the down arrow next to the feature to further specify the measurement.If a feature displays the channels selection bar , first select on which channel the measurement will be done (green channel is selected in our example) and then  add the feature. The channel can be changed later by clicking on the Change channels button next to the = icon.Adjust the name of the feature in the Custom Name column. This name is then passed in the Analysis Results table.Choose the Channels on which the feature is calculated.Set the resulting Format and the number of decimal units (dec).The   Calculator (Calculated Column) feature is different than the other features as it displays a scientific calculator at the bottom with buttons for arithmetical operations, allowing the calculation of a new column using already inserted features (click on the  icon to insert a feature into the formula).Measurement of some features can be further specified in their Aggregation drop-down menu. Order of a feature can be changed by clicking and dragging the feature to a different row position in the list. To hide the selected feature in the Records table, click on the  icon. To delete a feature, click on the  icon. To remove all features in the list, click   Remove all.ObjectId and ObjectEntity are shown in the Analysis Results as {BinaryLayerName}Id and {BinaryLayerName}Entity. Naming the binary layers as Cell, Nuclei, Parent, etc., so that the columns are named CellId, CellEntity, NucleiId, etc., enhances the clarity for the user. Generally one row record is created per cell. Some nodes generate a varying number of rows compared to a classic measurement (e.g. a histogram produces one row for each pixel value or bin). Loop indexes are dependent on the input file and produce multiple columns in the analysis results table if the input file has multiple loops.   Measurement &gt; Basic &gt; Object🔗,   Measurement &gt; Basic &gt; Object  Measures all features of every object. The output is a table where every row corresponds to one object.Optional channel inputs must be connected when intensity features are measured.Field feature as well as field and global metadata can be measured in the same node. They are repeated for every object. Figure&nbsp;787.&nbsp;The description of the dialog window behavior can be found in the    node.   Measurement &gt; Basic &gt; Object Count🔗,   Measurement &gt; Basic &gt; Object Count  Measures object features on the whole field. The output is a table where every row corresponds to one field.The node measures typical binary features like Object count, Area fraction as well as all other object features as aggregates (mean, sum, max, ...).Optionally, the node can measure more binary layers at once.Optional channel inputs must be connected when intensity features are measured. Figure&nbsp;788.&nbsp;The description of the dialog window behavior can be found in the    node.   Measurement &gt; Basic &gt; Parent🔗,   Measurement &gt; Basic &gt; Parent  Measures parent and children object features. The output is a table where every row corresponds to one parent object.Optional channel inputs must be connected when intensity features are measured.Parent-children relationship&nbsp;Every parent may “have” zero (0) or more children. There are three methods for determining childhood:Child is inside the parent - all child pixels are in the parent.Child is intersecting with the parent - they have at least one common pixel.Child nearest parent - closest parent measured border to border.The node measures features of the parent objects as well as aggregates (mean, sum, max, ...) of the children object features. Figure&nbsp;789.&nbsp; The Parent Id is always named ObjectId in the results. One row is created per parent. Child measurement is aggregated per parent (pick the Aggregation type for each child measurement).The description of the dialog window behavior can be found in the    node.   Measurement &gt; Basic &gt; Children🔗,   Measurement &gt; Basic &gt; Children  Measures parent and children object features. The output is a table where every row corresponds to one children object.Optional channel inputs must be connected when intensity features are measured (see Parent-children relationship in   ).The node measures features of both parent and children objects. The parent features are repeated for its every child. Having all children features is useful in later downstream nodes where the parent ID can be used for grouping, reduction or sorting.Children IDs can be “measured” and used for joining with other nested children nodes. Figure&nbsp;790.&nbsp; The Parent Id is always named ObjectId in the results. One row is created per child. Aggregation can be done using   .The description of the dialog window behavior can be found in the    node.   Measurement &gt; Basic &gt; Cell🔗 Measures object features of cell, nucleus and few features of the cytoplasm. The output is a table where every row corresponds to one cell and nucleus.Optional channel inputs must be connected when intensity features are measured.Cell must be connected to MakeCell node which ensures that every cell has exactly one. It also gives the same ID to every cell and its nucleus.Cytoplasm is internally calculated as complement of the nucleus inside cell. Consequently it is hollow and may be split in more than one object (having same object ID though). Because of this it does not make sense to measure most of the object features.Optionally Spots objects can be connected to count their presence in each compartment.For the count feature the presence in a compartment is defined by the position of the center pixel.For the area and intensity features the intersection of spots and a compartment is used. Figure&nbsp;791.&nbsp;Node InputsCells Layer representing cells (“Cell” output).Nuclei Layer representing nuclei (“Nucleus” output).Spots Layer representing “Spots” inside the nucleus or cytoplasm area.Channel Channel used for intensity based measured features.The description of the dialog window behavior can be found in the    node.   Measurement &gt; Estimates &gt; Shift Estimate🔗 Outputs the X and Y shift estimates (in px and µm) for the checked correction method.   Measurement &gt; Estimates &gt; SNR Estimate🔗 Estimates the   (“SNR”) value present in the connected color result.   Measurement &gt; Metadata &gt; Global🔗 Retrieves global metadata and custom information from Channel A.   Measurement &gt; Metadata &gt; Recorded Data🔗 Shows all recorded data from the connected color image.   Measurement &gt; Metadata &gt; Wellplate🔗 Retrieves well plate information and thumbnail using Channel A and Binaries B1, ..., Bn and stores it in the table.ThumbnailSource Rect Defines size (in microns if calibrated) of the center portion of the frame that is cropped and rendered.Maximum Size Maximum size (in pixels) is a limit to which the rendered image is stretched down if being bigger.LabelingPredefined labels List of possible labels separated by comma. Listed values are used in the “Number Of” aggregation menu (e.g. in    node).   Measurement &gt; Field pixel values &gt; Histogram🔗 Calculates a   of all pixel values in the color image. If the connected color image has multiple channels they are averaged into a single value per pixel. If a binary is connected only pixels under it are taken.Minimum minimum pixel value; 0 or minimum for floats by defaultMaximum maximum pixel value;  or maximum for floats by defaultBin count number of histogram bins;  or 65536 for floats by default   Measurement &gt; Field pixel values &gt; Pixel Values🔗 Reports all pixel values of the color image. If the connected color image has multiple channels they are averaged into a single value per pixel. If a binary is connected only pixels under it are taken.   Measurement &gt; Resolution &gt; Image Pair FRC🔗 Fourier Ring Correlation (FRC) is a technique used to measure the actual resolution of two independent images of the same scene. This method produces a numerical value that represents the resolution, providing a precise and reliable measurement. However, obtaining two independent images of the identical scene can often be challenging. Despite this, FRC is versatile and applicable across various imaging modalities.This node implements the paper  .   Measurement &gt; Resolution &gt; One Image FRC🔗 This node is a modification of the    node so that it can be used for only one image and two images are not needed. This is based on the paper  . Accuracy and reliability is lower than on the    node. It is calibrated for AX and NSPARC images. Trying other modalities is not recommended.   Measurement &gt; Object pixel values &gt; Histogram🔗 Calculates histogram of all pixel values for every binary object.Please see   .   Measurement &gt; Object voxel values &gt; Histogram 🔗 Calculates histogram of all voxel values for every binary object.Please see   .   Measurement &gt; Object pixel values &gt; Pixel Values🔗 Reports all values for every object.   Measurement &gt; Object pixel values &gt; Profile Line🔗 Reports all pixel values along every line. If the object is not a line the order of pixel is undefined.   Measurement &gt; Object intensity &gt; Entropy 🔗 Calculates entropy of voxel values for every binary object.   Measurement &gt; Object intensity &gt; Kurtosis 🔗 Calculates sample   of voxel values for every binary object.   Measurement &gt; Object intensity &gt; Maximum 🔗 Calculates the maximum intensity for each object in the connected color result under the connected 3D binary result. Please see   .   Measurement &gt; Object intensity &gt; Mean 🔗 Calculates the mean intensity for each object in the connected color result under the connected 3D binary result. Please see   .   Measurement &gt; Object intensity &gt; Minimum 🔗 Calculates the minimum intensity for each object in the connected color result under the connected 3D binary result. Please see   .   Measurement &gt; Object intensity &gt; Mode 🔗 Finds voxel value that appears most often in every object.   Measurement &gt; Object intensity &gt; Quantile 🔗 Calculates quantile in the connected color result for each 3D object present in the connected 3D binary result. Please see   .   Measurement &gt; Object intensity &gt; Skewness 🔗 Calculates sample skewness of voxel values for every binary object.   Measurement &gt; Object intensity &gt; Standard Deviation 🔗 Calculates the standard deviation of intensity for each 3D object (“StDevIntensity”) in the connected color result under the connected 3D binary result.   Measurement &gt; Object intensity &gt; Sum 🔗 Calculates the sum of intensity in every voxel of the connected color result (“SumIntensity”) under the connected binary result for each object separately. Please see   .   Measurement &gt; Object intensity &gt; Variance 🔗 Shows the intensity variance for each 3D object in the connected color result under the connected 3D binary result.   Measurement &gt; Object intensity &gt; Uniformity 🔗 Calculates uniformity of color voxel values for every binary object.   Measurement &gt; Distance &gt; Nearest Object🔗,   Measurement &gt; Distance &gt; NearestObjDist  Measures the smallest distance to another 2D/3D object. Please see   .   Measurement &gt; Distance &gt; Pairwise Distance🔗 Calculates the distance between objects in two binary layers. Reference A center to B center calculates distance between the centers of two objects. Figure&nbsp;792.&nbsp;A center to B border, A border to B center calculates distance between the center of one object and the closest point on the border of the other object. Figure&nbsp;793.&nbsp;A border to B border calculates distance between the borders of objects (minimal distance between borders). When the objects are touching, the distance is 0. Figure&nbsp;794.&nbsp;   Measurement &gt; JavaScript &gt; JS Measure Field🔗 Measures one or more field features.See the dedicated documentation:  .   Measurement &gt; JavaScript &gt; JS Measure Objects🔗 Measures one or more object features.See the dedicated documentation:  .   Measurement &gt; Z-stack measurements &gt; Focus criteria 🔗 Calculates the focus criteria (values which estimate the image sharpness) for each slice in a Z-stack and estimates the focus position. For the bright field and fluorescence criteria, the x-y region of the image is divided into 3x3 crops. The focus criterion is calculated for each crop and Z-position, and the overall results are used to estimate the focus position. The calculated focus position corresponds to the value that would be obtained during (non-AI) live auto-focusing if the camera captured the same Z-stack.Type Choose the type of the focus criterion.Pass type Choose how the focus should be evaluated as if the Z-stack was obtained in one of the following modes: Single pass autofocus (Single pass), first pass of two-pass (2 passes: pass #1) autofocus, or second pass of two-pass autofocus (2 passes: pass #2).Output columnsZ coord Z-position of each slice.Validity check Validity of the focus estimation. Can be either OK (the focus position was found), Small range (the focus position is out of range) or Fail (focus position couldn't be estimated).Focus plane If the result of the validity check was OK, this column contains the found focus position. If the focus was found to be out of the Z-range, it contains the Z-value at the corresponding border of the range. If the focusing failed, it contains nothing.Whole frame criteria Focus criteria calculated across Z for the whole image.Crop # criteria Focus criteria calculated across Z for each crop. Crops #1 - #3, #4 - #6 and #7 - #9 correspond, respectively, to the top, middle and bottom of the image.Selected criteria If the result is OK, this column contains the focus criteria calculated across Z for the crop which had the most impact on the Z-position. Otherwise, this column is empty.   Measurement &gt; Whole volume &gt; Object Count 🔗 Counts the number of binary objects.   Measurement &gt; Whole volume &gt; Total Voxel Count 🔗 Counts the number of voxels.   Measurement &gt; Whole volume &gt; Total Object Volume 🔗 Shows the total volume of all objects present in the connected binary result.   Measurement &gt; Whole volume &gt; Volume Fraction 🔗 Volume of the objects in the connected binary result is divided by the total volume of the analysed image.   Measurement &gt; Whole volume &gt; Measured Volume 🔗 Calculates volume of the whole 3D sample.   Measurement &gt; Volume intensity &gt; Costes Backround 🔗 Calculates Costes Background of all voxel values. If the connected color volume has multiple channels they are averaged into a single value per volume. If a binary is connected only voxels under it are taken into account.It is a background estimation of two channels based on the Costes method. This method assumes that the correlation of values in both channels should be zero for background pixels. It is used mainly in colocalization analyses.   Measurement &gt; Volume intensity &gt; Entropy 🔗 Calculates entropy of all voxel values. If the connected color volume has multiple channels they are averaged into a single value per voxel. If a binary is connected only voxels under it are taken.Please see   .   Measurement &gt; Volume intensity &gt; Kurtosis 🔗 Calculates sample   of all voxel values. If the connected color volume has multiple channels they are averaged into a single value per voxel. If a binary is connected only voxels under it are taken.Please see   .   Measurement &gt; Volume voxel values &gt; Histogram 🔗 Calculates histogram of all voxel values. If the connected color volume has multiple channels they are averaged into a single value per voxel. If a binary is connected only pixels under it are taken.Please see   .   Measurement &gt; Volume intensity &gt; Maximum 🔗 Finds maximum voxel value in every object. If the connected color volume has multiple channels they are averaged into a single value per voxel. If a binary is connected only voxels under it are taken.   Measurement &gt; Volume intensity &gt; Mean 🔗 Calculates arithmetic mean of all voxel values. If the connected color volume has multiple channels they are averaged into a single value per voxel. If a binary is connected only voxels under it are taken.Please see   .   Measurement &gt; Volume intensity &gt; Minimum 🔗 Finds minimum voxel value. If the connected color volume has multiple channels they are averaged into a single value per voxel. If a binary is connected only voxels under it are taken.   Measurement &gt; Volume intensity &gt; Mode 🔗 Finds voxel value that appears most often. If the connected color volume has multiple channels they are averaged into a single value per voxel. If a binary is connected only voxels under it are taken.Please see   .   Measurement &gt; Volume intensity &gt; Quantile 🔗 Calculates n-th quantile of all voxel values. If the connected color volume has multiple channels they are averaged into a single value per voxel. If a binary is connected only voxels under it are taken.Please see   .   Measurement &gt; Volume intensity &gt; Skewness 🔗 Calculates sample skewness of all voxel values. If the connected color volume has multiple channels they are averaged into a single value per voxel. If a binary is connected only voxels under it are taken.Please see   .   Measurement &gt; Volume intensity &gt; Standard Deviation 🔗 Calculates sample standard deviation of all voxel values. If the connected color volume has multiple channels they are averaged into a single value per voxel. If a binary is connected only voxels under it are taken.Please see   .   Measurement &gt; Volume intensity &gt; Sum 🔗 Calculates sum of all voxel values. If the connected color volume has multiple channels they are averaged into a single value per voxel. If a binary is connected only voxels under it are taken.Please see   .   Measurement &gt; Volume intensity &gt; Uniformity 🔗 Calculates uniformity of all voxel values. If the connected color volume has multiple channels they are averaged into a single value per voxel. If a binary is connected only voxels under it are taken.Please see   .   Measurement &gt; Volume intensity &gt; Variance 🔗 Calculates sample variance of all voxel values. If the connected color volume has multiple channels they are averaged into a single value per voxel. If a binary is connected only voxels under it are taken.Please see   .   Measurement &gt; Volume metadata &gt; Time 🔗 Displays the acquisition time.   Measurement &gt; Volume metadata &gt; Volume Center 🔗 Calculates the volume center (X, Y, Z in µm) of the connected color image.   Measurement &gt; Volume metadata &gt; Volume Center Pixels 🔗 Calculates the volume center (X, Y, Z in px) of the connected color image.   Measurement &gt; Volume metadata &gt; Volume Size 🔗 Displays the volume size.   Measurement &gt; Volume metadata &gt; Volume Size Pixels 🔗 Displays the volume size in pixels.   Measurement &gt; Volume metadata &gt; Recorded Data 🔗 Gives access to the metadata such as Exposure Time, Plate Name, Well Row, Slide Barcode, etc.   Measurement &gt; Volume ratiometry &gt; Ratio 🔗 Calculates the mean of ratios between corresponding pixels of the two input channels (nominator, denominator) under the input binary mask in 3D.   Measurement &gt; Volume ratiometry &gt; Pearson Coeff 🔗 Calculates Pearson correlation coefficient for a volume image. For more information please see   .   Measurement &gt; Volume ratiometry &gt; Mander Coeff 🔗 Calculates Manders overlap for a volume image. For more information please see   .   Measurement &gt; Object size &gt; Eq Diameter 🔗 Calculates the Equivalent Diameter (“EqDiameter”) for each 3D object in the connected binary result. It represents a sphere with the same volume as the measured object. For more information please see   .   Measurement &gt; Object size &gt; Object Voxel Count 🔗 Calculates the number of voxels in each 3D object.   Measurement &gt; Object size &gt; Volume 🔗 Calculates the volume of each object in the connected 3D binary result.   Measurement &gt; Object size &gt; Surface 🔗 Calculates the surface of each 3D object in the connected binary result. Computation method of this node is based on David Legland: Computation of Minkowski Measures on 2D and 3D binary images. DOI: 10.5566/ias.v26.p83-92.   Measurement &gt; Object shape &gt; Major Axis Length 🔗 Calculates the Major Axis Length for each object in the connected 3D binary result. Please see   .   Measurement &gt; Object shape &gt; Minor Axis Length 🔗 Calculates the Minor Axis Length for each object in the connected 3D binary result. Please see   .   Measurement &gt; Object shape &gt; Minor 2 Axis Length 🔗 Calculates the Minor 2 Axis Length for each object in the connected 3D binary result. Please see   .   Measurement &gt; Object shape &gt; Elongation 🔗 Calculates the elongation of each object in the connected 3D binary result. Please see   .   Measurement &gt; Object shape &gt; Orientation 🔗 Please see   .   Measurement &gt; Object shape &gt; Pitch 🔗 Please see   .   Measurement &gt; Object shape &gt; Sphericity 🔗 Please see   .   Measurement &gt; Object ratiometry &gt; Pearson Coeff 🔗 Please see   .   Measurement &gt; Object ratiometry &gt; Manders Coeff 🔗 Please see   .   Measurement &gt; Object ratiometry &gt; Ratio 🔗 Please see   .   Measurement &gt; Object parenting &gt; Aggregate Children 🔗 Defines the child/parent emplacement and specifies which records from the connected nodes are shown. Order of the records can be changed by the arrow buttons.This node uses aggregation statistics. Please see    and   .   Measurement &gt; Object parenting &gt; Children🔗 Please see   .   Measurement &gt; Object parenting &gt; Children 🔗 Please see   .   Measurement &gt; Object parenting &gt; Child Id 🔗 Please see   .   Measurement &gt; Object parenting &gt; Child Distance 🔗 Please see   .   Measurement &gt; Object parenting &gt; Nearest Child 🔗 Please see   .   Measurement &gt; Object parenting &gt; Object ID 🔗 Please see   .   Measurement &gt; Object parenting &gt; Parent Id 🔗 Measures the inserted 3D features using the parent-child hierarchy. Use the first drop-down menu to select the hierarchy corresponding to your sample (Child is inside parent, Child is intersecting parent or Child's nearest parent). Then add feature(s) to be measured.   Measurement &gt; Object parenting &gt; Parent Distance 🔗 Please see   .   Measurement &gt; Object position &gt; Center 🔗 Shows the X, Y and Z distance from the top left corner to the center of gravity of each object.   Measurement &gt; Object position &gt; CenterPx 🔗 Shows the X, Y and Z distance in pixels from the top left corner to the center of gravity of each object.   Measurement &gt; Object position &gt; CenterAbs 🔗 Shows absolute coordinates of the center of gravity of each object in the scope of the stage XYZ range.   Measurement &gt; Object position &gt; Centroid 🔗 Shows the X, Y and Z coordinate of the object's centroid.   Measurement &gt; Object position &gt; CentroidPx 🔗 Shows the X, Y and Z coordinate of the object's centroid in pixels.   Measurement &gt; Object position &gt; CentroidAbs 🔗 Shows absolute coordinates of the centoid of each object in the scope of the stage XYZ range.   Measurement &gt; Object position &gt; Bounds 🔗 Shows the bounds of each 3D object. Please see   .   Measurement &gt; Object position &gt; BoundsPx 🔗 Shows the bounds of each 3D object in pixels. Please see   .   Measurement &gt; Object position &gt; BoundsAbs 🔗 Shows the absolute bounds of each 3D object. Please see   . ",
     id: 135 }, 
   { title: "ND Processing &amp; Conversions",
     xmlid: "id|node.ndprocessingconversions_sectionx",
     content: "  Stack reduction                 Stack reduction Binary     Processing     Binary to Color      Bitdepth      Channels      Labels &amp; Classes     Render to RGB       Render to Table     RGB      Golden Sample     Volume Contrast    Python Scripting     ND Processing &amp; Conversions &gt; Stack reduction &gt; Average🔗 Reduces N source frames into one resulting frame. Each pixel value in the resulting frame is an average of corresponding pixels in N source frames.Based on the type:All Frames All frames in the loop are reduced into one frame (all to 1).Rolling For each frame in the loop reduces surrounding N frames into the current one (all to all).Piecewise Reduces every consecutive N frames into one frame (all to all/N). Loop Loop to be reduced.Frames Number of frames to reduce. Disabled for All Frames. ND Processing &amp; Conversions &gt; Stack reduction &gt; Best Focus Plane🔗 Finds the best focus plane in the whole loop and effectively reduces the loop into single frame.Focus Criterion Select the method matching the acquisition modality.Loop Loop to be reduced.Channel In case there are more channels (All) select the one to use for criterion calculation.   ND Processing &amp; Conversions &gt; Stack reduction &gt; EDF🔗 (requires:  )Creates an Extended Depth of Focus (EDF) image from the selected Z-Stack Loop similarly to the   . For more information about EDF, please see  .   ND Processing &amp; Conversions &gt; Stack reduction &gt; Integrate🔗 Integration uses multiple source frames to create one resulting image by integrating their pixel intensities. This is useful especially for low-signal (dark) scenes.All Frames All frames in the loop are reduced into one frame (all to 1).Rolling For each frame in the loop reduces surrounding N frames into the current one (all to all).Piecewise Reduces every consecutive N frames into one frame (all to all/N).Loop Loop to be reduced.Frames Number of frames to reduce. Disabled for All Frames.   ND Processing &amp; Conversions &gt; Stack reduction &gt; Max IP🔗 Displays ND document in the maximum intensity projection. See   for more information.   ND Processing &amp; Conversions &gt; Stack reduction &gt; Max IP Ref🔗 Reduces the specified loop by taking a pixel from Channel stack A from the frame where the Reference channel stack Ref has maximum intensity.   ND Processing &amp; Conversions &gt; Stack reduction &gt; Median🔗 Performs   on the connected result based on the specified parameters.All Frames All frames in the loop are reduced into one frame (all to 1).Rolling For each frame in the loop reduces surrounding N frames into the current one (all to all).Piecewise Reduces every consecutive N frames into one frame (all to all/N).Loop Loop to be reduced.Frames Number of frames to reduce. Disabled for All Frames.   ND Processing &amp; Conversions &gt; Stack reduction &gt; Quantile Image🔗 Performs   on the connected result based on the specified parameters.   ND Processing &amp; Conversions &gt; Stack reduction &gt; Min IP🔗 Displays ND document in the minimum intensity projection. See   for more information.   ND Processing &amp; Conversions &gt; Stack reduction &gt; Select Frame🔗 This action is used for selecting a specific frame in the source image.Loop Loop where the frame will be selected.Select Frame Specifies the selection from the current loop.Start Starting frame.Count Number of frames.Step Step size.   ND Processing &amp; Conversions &gt; Stack reduction &gt; Select Single Frame🔗 Selects one frame (Index) from the specified dimension (Loop).   ND Processing &amp; Conversions &gt; Stack reduction &gt; Select Single Binary🔗 Returns binary from a selected frame (Index) chosen from the specified dimension (Loop).   ND Processing &amp; Conversions &gt; Stack reduction &gt; Shading Image🔗 Captures a shading image of the selected Type on the selected Loop. See also   .   ND Processing &amp; Conversions &gt; Stack reduction &gt; Stitch Multi Points🔗 (requires:  )Stitches frames of the connected image into a large image. Select the stitching method from the Stitching via drop-down menu (see  ). Optionally the Precise stitching (Image Registration) can be checked but be aware that it is more computationally demanding.Optionally use the Automatic Shading Correction and choose the type which best represents your sample - Brightfield, DIC like or Fluorescence with offset enabling to heighten brightness level of the corrected image to make objects in dark areas more visible.   ND Processing &amp; Conversions &gt; Stack reduction Binary &gt; And🔗 Reduces N source frames into one resulting frame. Each binary pixel in the resulting frame means that every frame has this binary pixel. The function behaves like Min IP but on the binary.All Frames All frames in the loop are reduced into one frame (all to 1).Rolling For each frame in the loop reduces surrounding N frames into the current one (all to all).Piecewise Reduces every consecutive N frames into one frame (all to all/N).Loop Loop to be reduced.Frames Number of frames to reduce. Feature is disabled for All Frames.   ND Processing &amp; Conversions &gt; Stack reduction Binary &gt; Or🔗 Reduces N source frames into one resulting frame. Each binary pixel in the resulting frame means that at least one frame has this binary pixel. The function behaves like Min IP but on the binary.All Frames All frames in the loop are reduced into one frame (all to 1).Rolling For each frame in the loop reduces surrounding N frames into the current one (all to all).Piecewise Reduces every consecutive N frames into one frame (all to all/N).Loop Loop to be reduced.Frames Number of frames to reduce. Feature is disabled for All Frames.   ND Processing &amp; Conversions &gt; Processing &gt; Align🔗 Moves the frames with respect to each other in order to stabilize the motion of the image.Align to Previous / First frame.Loop Loop to stabilize.Channel In case there are more channels (All) select the one to use for calculation.Smooth Movement Correction  Correction for Heavily Noised Images  Super Resolution Alignment     ND Processing &amp; Conversions &gt; Processing &gt; Equalize Intensity🔗 This command enhances the dynamic range of generally any current ND2 file which contains at least Z dimension. It analyses the image, calculates values similar to auto-contrast values common for the whole time sequence and then processes all frames of the ND2 file. This method is robust to noise and preserves original trends of histogram.You can select a method used to equalize the intensity. Figure&nbsp;721.&nbsp;Histogram Stretching Select which method is used to equalize the document. The histogram is stretched using selected methods. The shape of the histogram is preserved.Histogram Shape Transformation to the Current One This method transforms the shape of the histogram of the whole document according to shape of the histogram of the selected area.After you press the OK button, you will be prompted to select according to which area the image will be equalized. Draw the rectangle and confirm with a right-mouse click. The document is processed afterwards.   ND Processing &amp; Conversions &gt; Binary to Color &gt; Convert🔗 Connected binary image is converted into a color image. Pixels of the binary objects are equal to one and background pixels become zeros.   ND Processing &amp; Conversions &gt; Binary to Color &gt; Convert Using Table🔗 Creates a new channel with a binary filled with the measured object feature (Object value ). Example&nbsp;9.&nbsp;Detect objects on a channel (with threshold) and measure circularity (or any other object feature). Then use this action on the thresholded result and connect the tabular input with the circularity result. Set Object value  to Circularity. Each binary object now gets a circularity value. Figure&nbsp;722.&nbsp;Convert Using Table Example  Figure&nbsp;723.&nbsp;Bottom right area of the image showing the binary layer with the assigned circularity value (0,16 in this example)Background Value added to the background (everything except the binary).Object value Object feature which is written to the object linked through Entity  and ObjectId .   ND Processing &amp; Conversions &gt; Binary to Color &gt; Convert Using Table 🔗 Creates a new channel with a binary filled with the measured object feature (Object value).Background Value added to the background (everything except the binary).Object value Object feature which is written to the object linked through Entity and Object3DId . ND Processing &amp; Conversions &gt; Bitdepth &gt; Change Bit Depth🔗,  ND Processing &amp; Conversions &gt; Bitdepth &gt; Change Bit Depth  Changes the bit-depth of the connected result. Figure&nbsp;724.&nbsp;Output Bit Depth Sets a new color depth of your image. 8-bit/16-bit integer or a floating-point. Intensity values can be rescaled.See  .Re-scale intensity values If checked the pixel values are mapped to the new range. E.g. when you convert an 8bit image to 16bit and you do not rescale the values, it results in a completely black image. If the check box was selected, the 8bit 255 value would become 65535 in 16bit etc.   ND Processing &amp; Conversions &gt; Bitdepth &gt; Convert to Ref🔗 Changes the bit-depth of the connected result to the bit-depth of the connected reference image. Intensity values can be rescaled.   ND Processing &amp; Conversions &gt; Channels &gt; Merge Channels🔗 Merges all connected results into one.   ND Processing &amp; Conversions &gt; Channels &gt; Split Channels🔗 Splits the connected results back to the state before the channel merge.   ND Processing &amp; Conversions &gt; Channels &gt; Channel to Intensity🔗 Creates an intensity channel from the connected color source.   ND Processing &amp; Conversions &gt; Labels &amp; Classes &gt; Binaries to Color🔗 Converts multiple binary inputs into a color class image where every pixel belongs to exactly one class defined by its value (zero being background). Binaries are taken one-by-one from A parameter each setting a value (A -&gt; 1, B -&gt; 2, ...) to pixels under the corresponding binary. If binaries overlap the later is preserved.   ND Processing &amp; Conversions &gt; Labels &amp; Classes &gt; Color to Binaries🔗 Converts class image into multiple binaries based on the pixel values.Class count Number of binaries created.   ND Processing &amp; Conversions &gt; Render to RGB &gt; Color🔗 Connected color image is rendered into a single RGB image.   ND Processing &amp; Conversions &gt; Render to RGB &gt; Binary🔗 Connected binary image is rendered into a single RGB image.   ND Processing &amp; Conversions &gt; Render to RGB &gt; Overlay🔗 Overlay of the connected color (bottom) and binary layer (top) is rendered into a single RGB image.   ND Processing &amp; Conversions &gt; Render to RGB &gt; Graph🔗 Graph is rendered into a single RGB image.   ND Processing &amp; Conversions &gt; Render to Table &gt; Render Frame🔗 Renders Channel A and Binaries B1, ..., Bn and stores it in the table as a row. If Filter is connected to a table it renders only frames present in the table.Source Rect Defines the size (in microns if calibrated) of the center portion of the frame that is cropped and rendered.Maximum Size Maximum Size (in pixels) is a limit to which the rendered image is stretched down if being bigger.   ND Processing &amp; Conversions &gt; Render to Table &gt; Render Objects🔗 Rendered Size Size of the rendered image.Format &amp; Quality Specifies the Image file format (jpg or png) and image quality in [%].Auto-contrast Binary Opacity    ND Processing &amp; Conversions &gt; RGB &gt; RGB to Intensity🔗 Converts the connected RGB result into an intensity channel.   ND Processing &amp; Conversions &gt; RGB &gt; RGB to HSI🔗 Converts the connected RGB result to its HSI representation.   ND Processing &amp; Conversions &gt; RGB &gt; HSI to RGB🔗 Converts the connected HSI result to its standard RGB representation.   ND Processing &amp; Conversions &gt; Volume Contrast &gt; Volume Contrast🔗 Creates a Volume Contrast image from the connected color image. Set the Wavelength of the connected sample image ( reset button returns the default value) and define the Well surface and Background level values. Recommended values can be added using the button on the right.   ND Processing &amp; Conversions &gt; Golden Sample &gt; Match🔗 After connecting this node to the template image (Image) to be matched and the golden sample (Template), it can be used for adjusting the   parameters such as the minimal similarity, minimal/maximal angle, maximal number of matches, maximal overlap, or maximal downsampling.   ND Processing &amp; Conversions &gt; Golden Sample &gt; Draw🔗 Adjusts the parameters for the template matching binary layer such as the X and Y center position, width/height, angle, and units.   ND Processing &amp; Conversions &gt; Python Scripting &gt; Python🔗,   ND Processing &amp; Conversions &gt; Python Scripting &gt; Python    3.12 is   inside NIS-Elements in such a way that NIS-Elements can call the python interpreter and pass python code to it.The python program and all its modules live in NIS-Elements folder so that it does not contaminate host environment that may be using different version of Python:{% raw %}C:\\Program Files\\NIS Elements\\Python\\\n{% endraw %}Furthermore, two bat files pip.bat and python.bat exist in NIS-Elements folder to enable interacting with the python.Installing packages using pipPython packages can be managed using pip from the NIS-Elements folder.Change directory to NIS-Elements.{% raw %}CD \\Program Files\\NIS Elements\n{% endraw %}Install package using pip.bat.{% raw %}pip.bat install &lt;package&gt; \n{% endraw %}or (alternatively pip can be called using python.bat){% raw %}python.bat -m pip install &lt;package&gt;\n{% endraw %}Run the interpreter and try to import the package.{% raw %}python.bat\n&gt;&gt;&gt; import &lt;package&gt;\n{% endraw %}Upgrading pip Be careful and call .\\python.exe from the NIS-Elements Python folder to be sure to upgrade the local NIS-Elements pip not the system pip.{% raw %}CD \\Program Files\\NIS Elements\\Python\n.\\python.exe -m pip install --upgrade pip\n{% endraw %}Troubleshooting pip When pip.bat is not working:Solution 1: instead use {% raw %}python.bat -m pip install &lt;package&gt;{% endraw %}Solution 2: bootstrap pipCall the bootstrappip.bat from NIS-Elements\\Python folder.Check pip.bat from C:\\Program Files\\NIS Elements\\ and if successful end here. Otherwise continue with the steps below.Go to NIS Elements Python folder C:\\Program Files\\NIS Elements\\PythonDelete pip*.exe (pip.exe, pip3.exe, pip3.12.exe) from Scripts folder if present.Delete pip and pip-XX.Y.Z.dist-info (e.g. pip-23.2.1.dist-info) from Lib\\site-packages if present.From C:\\Program Files\\NIS Elements\\Python call {% raw %}.\\python.exe -m ensurepip --default-pip{% endraw %}Check pip.bat from C:\\Program Files\\NIS Elements\\Python stdout - where print() text goes NIS-Elements installs hook to both python stderr and stdout and routes it to the Log file. To see it go to   ).Python nodes in GA3 There are two Python nodes. 2D node operates on frame data and a 3D node operates on volume data. They share the same look and behavior and appear empty when inserted into GA3. Figure&nbsp;725.&nbsp;The nodes allow for any number and type of inputs and outputs. Inputs and outputs can be added and removed by buttons on the toolbar. The tooltip on both inputs and outputs suggests how to access it in the code (e.g. inp[0], out[1]). Figure&nbsp;726.&nbsp;The default script looks like this:{% raw %}# IMPORTANT: 'limnode' must be imported like this (not from nor as)\nimport limnode\n\n# defines output parameter properties\ndef output(inp: tuple[limnode.AnyInDef], out: tuple[limnode.AnyOutDef]) -&gt; None:\n    pass\n\n# return Program for dimension reduction or two-pass processing\ndef build(loops: list[limnode.LoopDef]) -&gt; limnode.Program|None:\n    return None\n\n# called for each frame/volume\ndef run(inp: tuple[limnode.AnyInData], out: tuple[limnode.AnyOutData], ctx: limnode.RunContext) -&gt; None:\n    pass\n\n# child process initialization (when outproc is set)\nif __name__ == '__main__':\n    limnode.child_main(run, output, build) \n{% endraw %}There are three functions that definewhat is the node output output(...),how each frame/volume is processed run(...) andhow the run is called build(...)All important details can be seen in the limnode.py inside C:\\Program Files\\NIS Elements\\Python\\Lib\\site-packages.output() function The purpose of this function is to define all aspects of output parameters by modifying out tuple items.Output() is typically called before the run of GA3 after some user change upstream the graph. The information provided is used by the downstream nodes in GUI and run (for example table columns defined here are listed dependent nodes).In GA3 output parameters can have assigned input or can be create new output. When they have assigned input, they inherit name, color and the number of components from that input. The output changes as the input gets reconnected. New outputs however, must be fully defined.For assigning all output items have assign(...) method which takes input of the same type. For creating new output there are dedicated makeNew*(...) methods. All these methods return self and can be safely concatenated.By default the node tries to assign to every output the first input of the same type. It this is not possible a new output is made.Here is a summary of the methods:{% raw %}class OutputChannelDef:\n    def assign(self, param: InputChannelDef) -&gt; OutputChannelDef:\n        ...\n    def makeNewMono(self, name: str, color: AnyColorDef) -&gt; OutputChannelDef:\n        ...\n    def makeNewRgb(self, name: str|None = None) -&gt; OutputChannelDef:\n        ...\n    # change bitdepth\n    def makeFloat(self) -&gt; OutputChannelDef:\n        ...\n    def makeUInt16(self) -&gt; OutputChannelDef:\n        ...\n    def makeUInt8(self) -&gt; OutputChannelDef:\n        ...\n\nclass OutputBinaryDef:\n    def assign(self, param: InputBinaryDef) -&gt; OutputBinaryDef:\n        ...\n    def makeNew(self, name: str, color: AnyColorDef) -&gt; OutputBinaryDef:\n        ...\n    # change bitdepth\n    def makeInt32(self) -&gt; OutputBinaryDef:\n        ...\n    def makeUInt8(self) -&gt; OutputBinaryDef:\n        ...\n\nclass OutputTableDef:\n    def assign(self, param: InputTableDef) -&gt; OutputTableDef:\n        ...\n    # new empty table\n    def makeEmpty(self, name: str) -&gt; OutputTableDef:\n        ...\n    # optional InputTableDef to copy columns from or if None add just loopCols\n    def makeNew(self, name: str, param: InputTableDef|None = None) -&gt; OutputTableDef:\n        ...\n    # to add all necessary loop columns\n    def withLoopCols(self) -&gt; OutputTableDef:\n        ...\n    # to add Entity and ObjectID columns\n    def withObjectCols(self) -&gt; OutputTableDef:\n        ...\n    # and arbitrary data columns based on type\n    def withIntCol(self, title: str, unit: str|None = None, feature:str|None = None, id: str|None = None) -&gt; OutputTableDef:\n        ...\n    def withFloatCol(self, title: str, unit: str|None = None, feature:str|None = None, id: str|None = None) -&gt; OutputTableDef:\n        ...\n    def withStringCol(self, title: str, unit: str|None = None, feature:str|None = None, id: str|None = None) -&gt; OutputTableDef:\n        ...\n{% endraw %}Channels and binaries Example of assigning an input channel to output and changing it to float:{% raw %}def output(inp: tuple[limnode.AnyInDef], out: tuple[limnode.AnyOutDef]) -&gt; None:\n    out[0].assign(inp[0]).makeFloat()\n{% endraw %}Example of creating a new YFP (yellow) 16bit channel:{% raw %}def output(inp: tuple[limnode.AnyInDef], out: tuple[limnode.AnyOutDef]) -&gt; None:\n    out[0].makeNewMono('YFP', \"#FFFF00\").makeUInt16() # color can be a RGB tuple too: (255, 255, 0)\n{% endraw %}Tables Example of adding two columns to an output table:{% raw %}\n# by making new the table has only loop columns and the two columns added to it\ndef output(inp: tuple[limnode.AnyInDef], out: tuple[limnode.AnyOutDef]) -&gt; None:\n    unit_x, unit_y, _ = inp[0].units # inp[0] is InputChannelDef or InputBinaryDef\n    out[0].makeNew(\"Records\").withFloatCol(\"X coord\", unit_x).withFloatCol(\"Y coord\", unit_y)\n\n# by assigning the table gets all columns from inp[0] and a new Ratio column of type float\ndef output(inp: tuple[limnode.AnyInDef], out: tuple[limnode.AnyOutDef]) -&gt; None:\n    out[0].assign(inp[0]).withFloatCol(\"Ratio\")    \n{% endraw %}build() function It is called before run to enquire hew to call the run method. By default it returns None to indicate that run(...) will be called for each frame/volume once.Other two possibilities are for:stack reduction, andtwo-pass algorithmsFor stack reduction return ReduceProgram(...) with appropriate loop overZStack() or overTimeLapse() method.{% raw %}def build(loops: list[limnode.LoopDef]) -&gt; limnode.Program|None:\n    return limnode.ReduceProgram(loops).overZStack()\n{% endraw %}For two-pass algorithms return TwoPassProgram(...) with appropriate loop overZStack() or overTimeLapse() method.{% raw %}def build(loops: list[limnode.LoopDef]) -&gt; limnode.Program|None:\n    return limnode.TwoPassProgram(loops).overZStack()\n{% endraw %}run() function It is called for every frame/volume (depending on build(...) return value) to fill the data of each out item.Channels and binaries For OutputChannelData and OutputBinaryData the data property is a numpy ndarry of shape (z, y, x, comps) and dtype defined by output() function. By default it is filled with zeros.{% raw %}# to simply multiply the input by factor of two\ndef run(inp: tuple[limnode.AnyInData], out: tuple[limnode.AnyOutData], ctx: limnode.RunContext) -&gt; None:\n    out[0].data[:] = inp[0].data[:] * 2\n{% endraw %}Tables For OutputTableData the data property is a table (type LimTableDataBase defined in limtabledata.py). Table columns are defined by output function(). By default the table is empty (there is no data in any column).{% raw %}# for more columns at once\ndef run(inp: tuple[limnode.AnyInData], out: tuple[limnode.AnyOutData], ctx: limnode.RunContext) -&gt; None:\n    out[0].withColData([\"X coord\", \"Y coord\"], [ [0.0, 0.5], [0.0, 0.5] ])  # list, list[list]\n\n# for inp[0] being an assigned (has same columns) InputTableData\ndef run(inp: tuple[limnode.AnyInData], out: tuple[limnode.AnyOutData], ctx: limnode.RunContext) -&gt; None:\n    a, b = inp[0].colArray([\"MeanOfDiO\", \"MeanOfDiI\"])\n    out[0].withColDataFrom(inp[0]).withColData(\"Ratio\", a/b)\n{% endraw %}Each item has prefilled recdata (of type LimTableDataBase) with per-frame recorded data (e.g. AcqTime, X, Y, Z, ...) which can be altered. The columns in colArray() and withColData() are regular experssions (columns are looked up using re.search(colname)). Thus, for exact match use \"^colname$\" (see colIndexByPattern()in limtabledata.py).The context The context ctx provides more information about the current run:{% raw %}@dataclass(kw_only=True)\nclass RunContext:\n    inpFilename: str                            # full input filename\n    outFilename: str                            # full output filename (typically same as inpFilename except in Cluster Computing)\n    inpParameterCoordinates: tuple[tuple[int]]  # loop coordinates for every input parameter \n    outCoordinates: tuple[int]                  # output loop coordinates\n    reducingCoordinateIndexes: tuple[int]|None  # loop of loops being reduced or None when not reducing\n    finalCall: bool                             # when out should be filled (when false out.data and recdata are set to None)\n\n    @property\n    def shouldAbort(self) -&gt; bool:              # abort is requested\n        ...\n{% endraw %}ctx.shouldAbort should be checked whenever possible. However, it is not possible to abort calls into libraries. In such cases switch out of process execution ON.Reduce When ReduceProgram was returned in build() the run() function must fill the out items only when ctx.lastCall == True.Before lastCall it is expected to accumulate the result like in this maximum intensity example:{% raw %}# IMPORTANT: 'limnode' must be imported like this (not from nor as)\nimport limnode, numpy as np\n\ntmp = None\n\n# defines output parameter properties\ndef output(inp: tuple[limnode.AnyInDef], out: tuple[limnode.AnyOutDef]) -&gt; None:\n    out[0].makeNewMono(\"DiY\", (0, 0, 255))\n\n# return Program for dimension reduction or two-pass processing\ndef build(loops: list[limnode.LoopDef]) -&gt; limnode.Program|None:\n    return limnode.ReduceProgram(loops).overZStack()\n\n# called for each frame/volume\ndef run(inp: tuple[limnode.AnyInData], out: tuple[limnode.AnyOutData], ctx: limnode.RunContext) -&gt; None:\n    global tmp\n    if tmp is None:\n        tmp = np.zeros(inp[0].data.shape, inp[0].data.dtype)\n    tmp = np.maximum(tmp, inp[0].data)\n    if ctx.finalCall:\n        out[0].data[:] = tmp\n\n# child process initialization (when outproc is set)\nif __name__ == '__main__':\n    limnode.child_main(run, output, build)\n{% endraw %}Two-passes When TwoPassProgram was returned in build() the run() function must fill the out items only when ctx.lastCall == True. The run can monitor the frame/volume being processed in ctx.inpParameterCoordinates and ctx.reducingCoordinateIndexes. During first pass the node analyses data and then during second pass if fills the out items.In the example below the node accumulates a maximum intensity projection in the first pass and during the second pass it subtracts current frame from it:{% raw %}import limnode, numpy as np\n\ntmp = None\n\n# defines output parameter properties\ndef output(inp: tuple[limnode.AnyInDef], out: tuple[limnode.AnyOutDef]) -&gt; None:\n    out[0].assign(inp[0])\n\n# return Program for dimension reduction or two-pass processing\ndef build(loops: list[limnode.LoopDef]) -&gt; limnode.Program|None:\n    return limnode.TwoPassProgram(loops).overZStack()\n\n# called for each frame/volume\ndef run(inp: tuple[limnode.AnyInData], out: tuple[limnode.AnyOutData], ctx: limnode.RunContext) -&gt; None:\n    global tmp\n    if tmp is None:\n        tmp = np.zeros(inp[0].data.shape, inp[0].data.dtype)\n    if ctx.finalCall:\n        out[0].data[:] = tmp - inp[0].data[:]\n    else:\n        tmp = np.maximum(tmp, inp[0].data)   \n\n# child process initialization (when outproc is set)\nif __name__ == '__main__':\n    limnode.child_main(run, output, build) \n{% endraw %} All numpy.ndarray data properties are valid only during the particular run. If the algorithm has to to store it in a variable it has to make a copy of it.tmp = numpy.copy(unp[0].data[0, :, :, 0])Out of process run Many times the required python modules are incompatible with NIS-Elements or lengthy operation dictate the need for out of process execution. In such scenario all three functions output, build and run are called in a separate process. Specifically the C:\\Program Files\\NIS Elements\\Python\\pythonw.exe is and the script is passed to it. All calls are forwarded to that process.For details see limnode.py.Examples Using scikit-image for Otsu segmentation This example will show how to install an additional package (skimage) and use it for basic segmentation.Install the package using pip.bat and check that it is installed and can be imported using python.bat:{% raw %}CD \\Program Files\\NIS Elements\npip.bat install scikit-image\npython.bat\n&gt;&gt;&gt; import skimage\n{% endraw %}Open the C:\\Program Files\\NIS-Elements\\Images\\agnor.tif.In NIS-Elements GA3 Editor:Drag the Python node (in the ND Processing &amp; Conversions section) into the graph.Open the node settings dialog by clicking on the ... and add one color input  and one binary output .Connect the input pin to the blue channel.Connect the output pin to the SaveBinaries node.Paste the code below inside the dialog. Figure&nbsp;727.&nbsp;{% raw %}# IMPORTANT: 'limnode' must be imported like this (not from nor as)\nimport limnode, numpy\nfrom skimage import filters\n\n# defines output parameter properties\ndef output(inp: tuple[limnode.AnyInDef], out: tuple[limnode.AnyOutDef]) -&gt; None:\n    out[0].makeNew(\"otsu\", (0, 255, 255))\n\n# return Program for dimension reduction or two-pass processing\ndef build(loops: list[limnode.LoopDef]) -&gt; limnode.Program|None:\n    return None\n\n# called for each frame/volume\ndef run(inp: tuple[limnode.AnyInData], out: tuple[limnode.AnyOutData], ctx: limnode.RunContext) -&gt; None:\n    image = inp[0].data[0, :, :, 0]\n    threshold = filters.threshold_otsu(image)\n    binary = image &lt; threshold\n    out[0].data[0, :, :, 0] = binary.astype(numpy.uint8)\n\n# child process initialization (when outproc is set)\nif __name__ == '__main__':\n    limnode.child_main(run, output, build) \n{% endraw %}The code:Imports filters from the skimage package.In the output() function defines the output to be a new binary named “otsu” and cyan color (0, 255, 255) or “#00FFFF”.In the run() function:takes the image from the inp array,calculates the threshold calling threshold_otsu,creates the binary by directly comparing the values from the image with the threshold value (in this example objects are dark), andsets the binary data into the out array while converting to the proper format (uint8 or int32 for binary IDs).The resulting thresholded binary image should look like this: Figure&nbsp;728.&nbsp;Using Cellpose for segmentation Cellpose is well known package for cellular segmentation (see  ).First install the cellpose module:{% raw %}C:\\Program Files\\NIS Elements&gt;pip.bat install cellpose[gui]\n{% endraw %}In NIS-Elements open an image, insert python node and set it up for segmentation (one channel input and one binary output) like so: Figure&nbsp;729.&nbsp;Because the libraries used by cellpose collide with NIS-Elements, make sure to switch ON the Run out of process.Then edit the python code as follow:{% raw %}# IMPORTANT: 'limnode' must be imported like this (not from nor as)\nimport limnode, numpy\nfrom cellpose import models\n\nmodel = models.Cellpose(model_type='cyto3')\n\n# defines output parameter properties\ndef output(inp: tuple[limnode.AnyInDef], out: tuple[limnode.AnyOutDef]) -&gt; None:\n    out[0].makeNew(\"cell\", (0, 255, 255))\n\n# return Program for dimension reduction or two-pass processing\ndef build(loops: list[limnode.LoopDef]) -&gt; limnode.Program|None:\n    return None\n\n# called for each frame/volume\ndef run(inp: tuple[limnode.AnyInData], out: tuple[limnode.AnyOutData], ctx: limnode.RunContext) -&gt; None:\n    masks, flows, styles, diams = model.eval(inp[0].data[0, :, :, 0])\n    out[0].data[0, :, :, 0] = masks.astype(numpy.uint8)\n\n# child process initialization (when outproc is set) \nif __name__ == '__main__':\n    limnode.child_main(run, output, build) \n{% endraw %}And see the results: Figure&nbsp;730.&nbsp;All imports and processing defined in global scope is often run by GA3 editor and can result in frequent freezing of GA3 editor. To make GA3 editor more responsible, try to hide these in functions.Here is short example how to rewrite Cellpose segmentation to make editor more responsive.{% raw %}# IMPORTANT: 'limnode' must be imported like this (not from nor as)\nimport limnode\n\nmodel = None\n\n# defines output parameter properties\ndef output(inp: tuple[limnode.AnyInDef], out: tuple[limnode.AnyOutDef]) -&gt; None:\n    out[0].makeNew(\"cell\", (0, 255, 255))\n\n# return Program for dimension reduction or two-pass processing\ndef build(loops: list[limnode.LoopDef]) -&gt; limnode.Program|None:\n    return None\n\n# called for each frame/volume\ndef run(inp: tuple[limnode.AnyInData], out: tuple[limnode.AnyOutData], ctx: limnode.RunContext) -&gt; None:\n    import numpy\n    from cellpose import models\n    \n    global model\n    if model is None:\n        model = models.Cellpose(model_type='cyto3')\n    \n    masks, flows, styles, diams = model.eval(inp[0].data[0, :, :, 0])\n    out[0].data[0, :, :, 0] = masks.astype(numpy.uint8) \n\n# child process initialization (when outproc is set) \nif __name__ == '__main__':\n    limnode.child_main(run, output, build) {% endraw %}All imports except limnode are moved to function 'run' as its only place where they are needed. Global variable “model” is initially assigned to None and is reassigned in “run”.Using the BaSiCPy for shading removal BaSiCPy is a library \"for background and shading correction of optical microscopy images\" (see  ).First install the basicpy module:{% raw %}C:\\Program Files\\NIS Elements&gt;pip.bat install basicpy\n{% endraw %}Setup the node for processing by adding Channel input and output. It is not necessary to run BaSiC in different process.The python con be written like so:{% raw %}# IMPORTANT: 'limnode' must be imported like this (not from nor as)\nimport limnode, numpy\nfrom basicpy import BaSiC\n\nbasic = BaSiC(get_darkfield=True)\nfitted: bool = False\nimgcache: list[numpy.ndarray] = []\n\n# defines output parameter properties  \ndef output(inp: tuple[limnode.AnyInDef], out: tuple[limnode.AnyOutDef]) -&gt; None:  \n   pass\n\n# return Program for dimension reduction or two-pass processing\ndef build(loops: list[limnode.LoopDef]) -&gt; limnode.Program|None:\n   return limnode.TwoPassProgram(loops).overMultiPoint()  \n\n# called for each frame/volume\ndef run(inp: tuple[limnode.AnyInData], out: tuple[limnode.AnyOutData], ctx: limnode.RunContext) -&gt; None:\n   global imgcache, fitted, basic\n   if ctx.finalCall:\n       if not fitted:\n           stk = numpy.stack(imgcache, axis=0)\n           basic.fit(stk)\n           imgcache = []\n           fitted = True\n       out[0].data[0, :, :, 0] = basic.transform(inp[0].data[0, :, :, 0])[0]\n   else:\n       imgcache.append(numpy.copy(inp[0].data[0, :, :, 0]))\n       fitted = False\n\n# child process initialization (when outproc is set)  \nif __name__ == '__main__':\n   limnode.child_main(run, output, build)   \n{% endraw %}The result looks like this: Figure&nbsp;731.&nbsp; ",
     id: 136 }, 
   { title: "NIS.ai",
     xmlid: "id|node.nis.ai_sectionx",
     content: "  Preprocessing        Transformations     Segmentation      Trained files    Measurement    Evaluation       NIS.ai &gt; Preprocessing &gt; Clarify.ai🔗 This function removes out of focus blur from the source images using neural networks. It is intended for widefield images and works best for thick samples. It is a preferred choice for under-sampled images, whereas deconvolution is a preferred method for well-sampled images.See  .Clarify.ai requires valid image metadata (similar to deconvolution). It is a parameterless method which does not increase the resolution and does not denoise the image however it can be combined with   . Check the Denoise.ai check box next to a channel to perform denoising first before clarifying. Check this check box only for very noisy images with SNR value smaller than 20.Modality To handle the out-of-focus planes correctly, it is important to know how exactly the image sequence has been acquired. Select the proper microscopic modality from the combo box.Pinhole size Depending on the Modality setting, set the pinhole/slit size value and choose the proper units.Magnification Specify magnification of the objective used to capture the image sequence.Numerical Aperture Enter the numerical aperture of the objective.Immersion Refractive Index Enter the refraction index of the immersion medium used. Predefined refraction indexes of different media can be selected from the pull-down menu.Calibration Enter the image calibration in μm/px.Output Check if you need to create new document. Otherwise the clarifying is applied to the original image.Channels Select which channels will be clarified and which will be denoised. You can also adjust the emission wavelength. To revert the changes, click   .Preview If checked, the clarifying preview is shown in the original image.OK Confirms the settings and performs the clarifying.Cancel Closes the window without executing any process.   NIS.ai &gt; Preprocessing &gt; Restore.ai🔗 Opens the Restore.ai dialog window. This function is designed to be used when denoise and deconvolution processes are combined. It can be applied on all types of fluorescence images (widefield, confocal, 2D/3D, etc.).Modality To handle the out-of-focus planes correctly, it is important to know how exactly the image sequence has been acquired. Select the proper microscopic modality from the combo box.Magnification Specify magnification of the objective used to capture the image sequence.Numerical Aperture Enter the numerical aperture of the objective.Refraction Index Enter refraction index of the immersion medium used. There are some predefined refraction indexes of different media in the nearby pull down menu.Calibration Enter the image calibration in μm/px.Channels Image channels produced by your camera are listed within this table. You can decide which channel(s) shall be processed by checking the check boxes next to the channel names. The emission wavelength value may be edited (except the Live De-Blur method). Brightfield channels are omitted automatically.   NIS.ai &gt; Preprocessing &gt; Denoise.ai🔗 Performs image denoising with the use of neural networks. This function is used especially for static scenes because moving objects may get blurred.   NIS.ai &gt; Preprocessing &gt; Cells Presence.ai🔗 Detects whether cells are present in the brightfield image or not. Works even for out of focus images.Network Selects the trained network used for the cells presence detection. The Recommended option should be used in most cases, as it is trained on a larger dataset and generally delivers better results. However, in rare situations, you can switch to the Legacy network if necessary.Node outputs:Verdict Verdict is 1 if cells are present, 0 if they are not present.Confidence Confidence of the detection, ranging from 0 (not confident) to 1 (very confident).   NIS.ai &gt; Preprocessing &gt; Cells Focus.ai🔗    NIS.ai &gt; Preprocessing &gt; Cells Localization.ai🔗 Detects cells and outputs a binary image with dots on the detected cell centers.Works only on images with bright field modality and in a narrow Z range around focus (+/- 25 µm).   NIS.ai &gt; Transformations &gt; Enhance.ai🔗 For the function description please see  .Trained AI Selects the trained network from a file (click Browse to locate the *.eai file).Details... Opens metadata associated with training of the currently selected neural network.   NIS.ai &gt; Transformations &gt; Convert.ai🔗 For the function description please see  .Trained AI Selects the trained network from a file (click Browse to locate the *.cai file).Details... Opens metadata associated with training of the currently selected neural network.   NIS.ai &gt; Segmentation &gt; Segment.ai🔗 For the function description please see  .Trained AI Selects the trained network from a file (click Browse to locate the *.sai file).Details... Opens metadata associated with training of the currently selected neural network.Advanced &gt;&gt; Reveals post-processing tools and restrictions used for enhancing the results of the neural network.   NIS.ai &gt; Segmentation &gt; Segment Objects.ai🔗 For the function description please see  .Trained AI Selects the trained network from a file (click Browse to locate the *.oai file).Details... Opens metadata associated with training of the currently selected neural network.Advanced &gt;&gt; Reveals post-processing tools and restrictions used for enhancing the results of the neural network.   NIS.ai &gt; Segmentation &gt; Homogeneous Area/Cells.ai🔗 Uses Cells Localization.ai to segment the image into areas with cells and into homogeneous areas without cells. The resulting binary image is equal to 0 at the cells and to 1 at the homogeneous areas. The segmentation fails if less than 10 cells are found.Channel Select the channel for segmentation.Inversion Check to invert the resulting binary (change 0s to 1s and vice versa).   NIS.ai &gt; Trained files &gt; Select Trained File.ai🔗 Selects an appropriate trained AI file according to the sample objective magnification. Output is expected to be used as a dynamic input parameter for another AI node. Paths to the trained AI files, either relative or absolute, can be defined using a standard regular expression.   NIS.ai &gt; Measurement &gt; Quality Estimate.ai🔗 (requires:  )Estimates the   (“SNR”) value as it is used with Autosignal.ai ( ).   NIS.ai &gt; Evaluation &gt; Segmentation Accuracy🔗 Calculates the average precision to evaluate AI on objects. This node has two inputs - GT  (Ground Truth) and Pred  (Prediction). It compares the ground truth binary layer (A) and predicted binary layer (B) generated by segmentation using AI. It also pairs the objects from both layers and classifies them (based on the IoU threshold) into:true positives (TP) matched correctly,false positives (FP) incorrectly segmented objects andfalse negatives (FN) incorrectly missed objects.Based on these numbers it calculates:precision = TP / (TP + FP),recall = TP / (TP + FN) andF1 = 2 x precision x recall / (precision + recall)IoU Threshold Defines a threshold above which two overlapping objects are considered correctly matched - threshold of quantity: Figure&nbsp;846.&nbsp;.   NIS.ai &gt; Evaluation &gt; Object Segmentation Accuracy🔗 Calculates the average precision to evaluate AI on objects. This node has two inputs - GT (Ground Truth) and Pred (Prediction). It compares the ground truth binary layer (A) and predicted binary layer (B) generated by segmentation using AI. It also pairs the objects from both layers and classifies them (based on the IoU threshold) into:true positives (TP) matched correctly,false positives (FP) incorrectly segmented objects andfalse negatives (FN) incorrectly missed objects.Based on these numbers it calculates:precision = TP / (TP + FP),recall = TP / (TP + FN) andF1 = 2 x precision x recall / (precision + recall)IoU Threshold Defines a threshold above which two overlapping objects are considered correctly matched - threshold of quantity: Figure&nbsp;847.&nbsp;.(requires:  ) ",
     id: 137 }, 
   { title: "Results &amp; Graphs",
     xmlid: "id|node.results.graphs_sectionx",
     content: "  Graphs          Layout      Wellplate            Tables       Reports     Legacy Graphs    Results &amp; Graphs &gt; Graphs &gt; Barchart🔗 This graph is used for visualizing Y of ordinal X. Settings of each tab are described below:GeneralGraph Title Sets the title of the graph shown in the top.Categorical Factors Nesting specifies the order in which the columns are to be arranged (e.g. if all bars for column A and then all bars for column B, or if the bars for the columns should alternate, ...). Group labels and column titles can be turned on/off. Grouped records specifies how the bars are colored - by column or grouping. Padding specifies how much to separate the groups given by nesting.Annotations Specifies whether the X and Y data labels are shown.Legend Determines the visibility and the position of the graph legend.DataDisplay Data Selects what data should be displayed (All data, Current frame, ...), i.e. the default value.All Columns This tab is used to pair the graph axes with the variables available in the table input. Select a data variable and click on the arrow  next to the selected axis to assign it to this axis. Multiple variables can be assigned to the Y axes. To move the variable from the axis list back to the All Columns list, click  or  to remove all. With a variable selected in the X/Y Axis list, Series style such as color and values. Error Bar section sets the error bar properties for the selected column. Choose an error column from the drop-down menu and optionally set its Color, Line width, and Whisker width.InteractionName Defines the graph name in the layouts.Icon Defines the graph icon in the layouts.Display Data Selection Allows the user to change the set Display Data directly in the chart (a drop-down menu appears at the top if something was checked). When having multiple charts/tables in the layout, they can Synchronize with each other. Select the current frame for a table and the next chart will also show only data from the current frame.Column Selection Select which columns can be interactively selected in graph top toolbar for each axis.Autoscale Scales graph by all data in the accumulated table.Show button  Adds the  Scale to fit the data button to the top toolbar.Data selection Facilitates the interactive data selection in the graph scene.Tools Adds interactive tools to the graph side bar.AxesAxis Label Defines the visibility and text of the axis label.Labels Defines the format and display precision of the tick labels.Minimum In Auto mode, the minimal value for the range is set automatically. If switched to Fixed, a minimal value from which the data are visualized can be entered into the edit box.Maximum In Auto mode, the maximal value for the range is set automatically. If switched to Fixed, a maximal value to which the data are visualized can be entered into the edit box.Scale Specifies the scale type - linear or logarithmic.Reversed Range Reverses the values range from “min - max” to “max -min”.Major Step In Auto mode, the major step value for the ticks and grid lines is set automatically. If switched to Fixed, a major step value used for the ticks and grid lines visualization can be entered into the edit box. If Tick is checked, a short tick is added to the axis next to each major step value. Checking Grid adds a grid line.Minor Step In Auto mode, the minor step value for the ticks and gridlines is set automatically. If switched to Fixed, a minor step value used for the ticks and gridlines visualization can be entered into the edit box. If Tick is checked, a short tick is added to the axis next to each minor step value. Checking Grid adds a grid line. Results &amp; Graphs &gt; Graphs &gt; Colormap🔗 Provides a graphical representation of data where the individual values contained in a two-dimensional matrix are represented as colors from selected color gradient map. This graph does not support a grouped source table.Settings of each graph tab are described below:GeneralGraph Title Sets the title of the graph shown in the top.Colors Defines the used palette and default background color for missing values.Annotations Specifies whether the labels are shown or not.DataDisplay Data Selects what data should be displayed (All data, Current frame, ...), i.e. the default value.X Axis, Y Axis, Color Select default columns and enable interactive change in the graph top bar.Column Selection Select which columns can be interactively selected in graph top toolbar for each axis.InteractionName Defines the graph name in the layouts.Icon Defines the graph icon in the layouts.Display Data Selection Allows the user to change the set Display Data directly in the chart (a drop-down menu appears at the top if something was checked). When having multiple charts/tables in the layout, they can Synchronize with each other. Select the current frame for a table and the next chart will also show only data from the current frame.Autoscale Scales graph by all data in the accumulated table.Show button  Adds the  Scale to fit the data button to the top toolbar.Data selection Facilitates the interactive data selection in the graph scene.Tools Adds interactive tools to the graph side bar.AxesAxis Label Defines the visibility and text of the axis label.Labels Defines the format and display precision of the tick labels. Results &amp; Graphs &gt; Graphs &gt; Fitplot🔗 This graph is used for visualizing fitted curve.Settings of each graph tab are described below:GeneralGraph Title Sets the title of the graph shown in the top.Legend Determines the visibility and the position of the graph legend.DataDisplay Data Selects what data should be displayed (All data, Current frame, ...), i.e. the default value.Columns with Fitted Equation This tab is used for selecting fitted equations to plot. Select a data variable and click on the arrow to assign it to Displayed Columns. Multiple variables can be assigned. To move the variable from the Displayed Columns list back to the Columns with Fitted Equation list, click  or  to remove all.InteractionName Defines the graph name in the layouts.Icon Defines the graph icon in the layouts.Display Data Selection Allows the user to change the set Display Data directly in the chart (a drop-down menu appears at the top if something was checked). When having multiple charts/tables in the layout, they can Synchronize with each other. Select the current frame for a table and the next chart will also show only data from the current frame.Data selection Facilitates the interactive data selection in the graph scene.Error bar button visible  Adds the  Error bars visible button to the top toolbar.Legend button visible  Adds the  Legend visible button to the top toolbar.Tools Adds interactive tools to the graph side bar. Results &amp; Graphs &gt; Graphs &gt; Histogram🔗 This graph is used for visualizing distribution of data.Settings of each graph tab are described below:GeneralGraph Title Sets the title of the graph shown in the top.Bins For numerical values set the bin count, eventually you can set the minimum and maximum. For string values enter the list used as a filter.DataAll Columns This tab is used to select data to plot. Select a data variable and click on the arrow to assign it to Displayed Columns. Multiple variables can be assigned. To move the variable from the Displayed Columns list back to the Columns with Fitted Equation list, click  or  to remove all.InteractionName Defines the graph name in the layouts.Icon Defines the graph icon in the layouts.Display Data Selection Allows the user to change the set Display Data directly in the chart (a drop-down menu appears at the top if something was checked). When having multiple charts/tables in the layout, they can Synchronize with each other. Select the current frame for a table and the next chart will also show only data from the current frame.Tools Adds interactive tools to the graph side bar. Results &amp; Graphs &gt; Graphs &gt; Linechart🔗 This graph is used for visualizing Y as function of X.Settings of each graph tab are described below:GeneralGraph Title Sets the title of the graph shown in the top.Palettes &amp; Markers Defines palettes and markers and grouped source table behavior.Annotations Specifies whether the X and Y data labels are shown.Legend Determines the visibility and the position of the graph legend.DataAll Columns This tab is used to pair the graph axes with the variables available in the table input. Select a data variable and click on the arrow  next to the selected axis to assign it to this axis. Multiple variables can be assigned to the Y axes. To move the variable from the axis list back to the All Columns list, click  or  to remove all. With a variable selected in the X/Y Axis list, Series style such as color and values. Error Bar section sets the error bar properties for the selected column. Choose an error column from the drop-down menu and optionally set its Color, Line width, and Whisker width.InteractionName Defines the graph name in the layouts.Icon Defines the graph icon in the layouts.Display Data Selection Allows the user to change the set Display Data directly in the chart (a drop-down menu appears at the top if something was checked). When having multiple charts/tables in the layout, they can Synchronize with each other. Select the current frame for a table and the next chart will also show only data from the current frame.Column Selection Select which columns can be interactively selected in graph top toolbar for each axis.Autoscale Scales graph by all data in the accumulated table.Show button  Adds the  Scale to fit the data button to the top toolbar.Data selection Facilitates the interactive data selection in the graph scene.Tools Adds interactive tools to the graph side bar.AxesAxis Label Defines the visibility and text of the axis label.Labels Defines the format and display precision of the tick labels.Minimum In Auto mode, the minimal value for the range is set automatically. If switched to Fixed, a minimal value from which the data are visualized can be entered into the edit box.Maximum In Auto mode, the maximal value for the range is set automatically. If switched to Fixed, a maximal value to which the data are visualized can be entered into the edit box.Scale Specifies the scale type - linear or logarithmic.Reversed Range Reverses the values range from “min - max” to “max -min”.Major Step In Auto mode, the major step value for the ticks and grid lines is set automatically. If switched to Fixed, a major step value used for the ticks and grid lines visualization can be entered into the edit box. If Tick is checked, a short tick is added to the axis next to each major step value. Checking Grid adds a grid line.Minor Step In Auto mode, the minor step value for the ticks and gridlines is set automatically. If switched to Fixed, a minor step value used for the ticks and gridlines visualization can be entered into the edit box. If Tick is checked, a short tick is added to the axis next to each minor step value. Checking Grid adds a grid line. Results &amp; Graphs &gt; Graphs &gt; Scatterplot🔗 This graph is used for visualizing a cloud of points.Settings of each graph tab are described below:GeneralGraph Title Sets the title of the graph shown in the top.Palettes &amp; Markers Defines palettes and markers and grouped source table behavior.Annotations Specifies whether the X and Y data labels are shown.Legend Determines the visibility and the position of the graph legend.DataAll Columns This tab is used to pair the graph axes with the variables available in the table input. Select a data variable and click on the arrow  next to the selected axis to assign it to this axis. Multiple variables can be assigned to the Y axes. To move the variable from the axis list back to the All Columns list, click  or  to remove all. With a variable selected in the X/Y Axis list, Series style such as color and values. Error Bar section sets the error bar properties for the selected column. Choose an error column from the drop-down menu and optionally set its Color, Line width, and Whisker width.InteractionName Defines the graph name in the layouts.Icon Defines the graph icon in the layouts.Display Data Selection Allows the user to change the set Display Data directly in the chart (a drop-down menu appears at the top if something was checked). When having multiple charts/tables in the layout, they can Synchronize with each other. Select the current frame for a table and the next chart will also show only data from the current frame.Column Selection Select which columns can be interactively selected in graph top toolbar for each axis.Autoscale Scales graph by all data in the accumulated table.Show button  Adds the  Scale to fit the data button to the top toolbar.Data selection Facilitates the interactive data selection in the graph scene.Tools Adds interactive tools to the graph side bar.AxesAxis Label Defines the visibility and text of the axis label.Labels Defines the format and display precision of the tick labels.Minimum In Auto mode, the minimal value for the range is set automatically. If switched to Fixed, a minimal value from which the data are visualized can be entered into the edit box.Maximum In Auto mode, the maximal value for the range is set automatically. If switched to Fixed, a maximal value to which the data are visualized can be entered into the edit box.Scale Specifies the scale type - linear or logarithmic.Reversed Range Reverses the values range from “min - max” to “max -min”.Major Step In Auto mode, the major step value for the ticks and grid lines is set automatically. If switched to Fixed, a major step value used for the ticks and grid lines visualization can be entered into the edit box. If Tick is checked, a short tick is added to the axis next to each major step value. Checking Grid adds a grid line.Minor Step In Auto mode, the minor step value for the ticks and gridlines is set automatically. If switched to Fixed, a minor step value used for the ticks and gridlines visualization can be entered into the edit box. If Tick is checked, a short tick is added to the axis next to each minor step value. Checking Grid adds a grid line. Results &amp; Graphs &gt; Graphs &gt; Scatterplot 4D🔗 This graph is used for visualizing a cloud of points. Points can differentiate by size and color.Settings of each graph tab are described below:GeneralGraph Title Sets the title of the graph shown in the top.Palettes &amp; Markers Defines palettes and markers and grouped source table behavior.Annotations Specifies whether the X and Y data labels are shown.Legend Determines the visibility and the position of the graph legend.DataX Axis, Y Axis, Color, Size Select default columns and enable interactive change in graph top bar.Column Selection Select which columns can be interactively selected in graph top toolbar for each axis.InteractionName Defines the graph name in the layouts.Icon Defines the graph icon in the layouts.Display Data Selection Allows the user to change the set Display Data directly in the chart (a drop-down menu appears at the top if something was checked). When having multiple charts/tables in the layout, they can Synchronize with each other. Select the current frame for a table and the next chart will also show only data from the current frame.Autoscale Scales graph by all data in the accumulated table.Show button  Adds the  Scale to fit the data button to the top toolbar.Data selection Facilitates the interactive data selection in the graph scene.Tools Adds interactive tools to the graph side bar.AxesAxis Label Defines the visibility and text of the axis label.Labels Defines the format and display precision of the tick labels.Minimum In Auto mode, the minimal value for the range is set automatically. If switched to Fixed, a minimal value from which the data are visualized can be entered into the edit box.Maximum In Auto mode, the maximal value for the range is set automatically. If switched to Fixed, a maximal value to which the data are visualized can be entered into the edit box.Scale Specifies the scale type - linear or logarithmic.Reversed Range Reverses the values range from “min - max” to “max -min”.Major Step In Auto mode, the major step value for the ticks and grid lines is set automatically. If switched to Fixed, a major step value used for the ticks and grid lines visualization can be entered into the edit box. If Tick is checked, a short tick is added to the axis next to each major step value. Checking Grid adds a grid line.Minor Step In Auto mode, the minor step value for the ticks and gridlines is set automatically. If switched to Fixed, a minor step value used for the ticks and gridlines visualization can be entered into the edit box. If Tick is checked, a short tick is added to the axis next to each minor step value. Checking Grid adds a grid line. Results &amp; Graphs &gt; Layout &gt; Display🔗 Specifies which results are displayed in the Main panel -    (Left, Right inputs), Side panel -    (Side input) or Report panel (Report input). Results &amp; Graphs &gt; Layout &gt; Horizontal🔗 Organizes input result panes or layouts side-by-side horizontally. Results &amp; Graphs &gt; Layout &gt; Stacked🔗 Organizes input result panes one over the other. Results &amp; Graphs &gt; Wellplate &gt; Bars🔗 Displays well plate with one or more bars in each well. Any numerical column aggregated per well is suitable as input.GeneralGraph Title Sets the title of the graph shown in the top.Data Range Overrides the overall minimum/maximum.Log Y Displays the bars in a logarithmic scale. DataDisplay Data Selects what data should be displayed (All data, Current frame, ...), i.e. the default value.Columns Selects the features for the graph. InteractionName Defines the graph name in the layouts.Icon Defines the graph icon in the layouts.Display Data Selection Allows the user to change the set Display Data directly in the chart (a drop-down menu appears at the top if something was checked). When having multiple charts/tables in the layout, they can Synchronize with each other. Select the current frame for a table and the next chart will also show only data from the current frame.Column Selection Select which columns can be interactively selected in graph top toolbar for each axis.Selection visible Enables the user to select the data columns.Shown columns Shown columns to force visible. The text is regular pattern that must match. Show takes precedence over hide.Hidden columns Hidden columns to force not visible. The text is regular pattern that must match. Use “*” to select all columns in conjunction with show.Data selection Facilitates the interactive data selection in the graph scene. Results &amp; Graphs &gt; Wellplate &gt; Barstack🔗 Displays well plate with bars stacked one over the other in each well. Any numerical column aggregated per well is suitable as input.GeneralGraph Title Sets the title of the graph shown in the top.Data Range Overrides the overall minimum/maximum.DataDisplay Data Selects what data should be displayed (All data, Current frame, ...), i.e. the default value.Columns Selects the features for the graph.InteractionName Defines the graph name in the layouts.Icon Defines the graph icon in the layouts.Display Data Selection Allows the user to change the set Display Data directly in the chart (a drop-down menu appears at the top if something was checked). When having multiple charts/tables in the layout, they can Synchronize with each other. Select the current frame for a table and the next chart will also show only data from the current frame.Column Selection Select which columns can be interactively selected in graph top toolbar for each axis.Selection visible Enables the user to select the data columns.Shown columns Shown columns to force visible. The text is regular pattern that must match. Show takes precedence over hide.Hidden columns Hidden columns to force not visible. The text is regular pattern that must match. Use “*” to select all columns in conjunction with show.Data selection Facilitates the interactive data selection in the graph scene. Results &amp; Graphs &gt; Wellplate &gt; Boxplot🔗 Displays the well plate with a box plot in each well. Any box plot aggregated column per well is suitable as input.TableGraph Title Sets the title of the graph shown in the top.Data Range Overrides the overall minimum/maximum.DataDisplay Data Selects what data should be displayed (All data, Current frame, ...), i.e. the default value.Columns Selects the features for the graph.InteractionName Defines the graph name in the layouts.Icon Defines the graph icon in the layouts.Display Data Selection Allows the user to change the set Display Data directly in the chart (a drop-down menu appears at the top if something was checked). When having multiple charts/tables in the layout, they can Synchronize with each other. Select the current frame for a table and the next chart will also show only data from the current frame.Column Selection Select which columns can be interactively selected in graph top toolbar for each axis.Selection visible Enables the user to select the data columns.Shown columns Shown columns to force visible. The text is regular pattern that must match. Show takes precedence over hide.Hidden columns Hidden columns to force not visible. The text is regular pattern that must match. Use “*” to select all columns in conjunction with show.Data selection Facilitates the interactive data selection in the graph scene. Results &amp; Graphs &gt; Wellplate &gt; Dosing🔗 Displays the well plate with dosing information: concentrations and positive/negative controls. It uses concentration, compound, group and control columns from a table typically produced by the    node.GeneralGraph Title Sets the title of the graph shown in the top.DataDisplay Data Selects what data should be displayed (All data, Current frame, ...), i.e. the default value.InteractionName Defines the graph name in the layouts.Icon Defines the graph icon in the layouts.Display Data Selection Allows the user to change the set Display Data directly in the chart (a drop-down menu appears at the top if something was checked). When having multiple charts/tables in the layout, they can Synchronize with each other. Select the current frame for a table and the next chart will also show only data from the current frame. Results &amp; Graphs &gt; Wellplate &gt; Heatmap🔗 Displays the well plate heat map. Any numerical aggregated column per well is suitable as input.GeneralGraph Title Sets the title of the graph shown in the top.Data Range Overrides the overall minimum/maximum.Log colors Shows color in a logarithmic scale.DataDisplay Data Selects what data should be displayed (All data, Current frame, ...), i.e. the default value.Columns Selects the features for the graph.Interaction Name Defines the graph name in the layouts.Icon Defines the graph icon in the layouts.Display Data Selection Allows the user to change the set Display Data directly in the chart (a drop-down menu appears at the top if something was checked). When having multiple charts/tables in the layout, they can Synchronize with each other. Select the current frame for a table and the next chart will also show only data from the current frame.Column Selection Select which columns can be interactively selected in graph top toolbar for each axis.Selection visible Enables the user to select the data columns.Shown columns Shown columns to force visible. The text is regular pattern that must match. Show takes precedence over hide.Hidden columns Hidden columns to force not visible. The text is regular pattern that must match. Use “*” to select all columns in conjunction with show. Results &amp; Graphs &gt; Wellplate &gt; Image🔗 Displays the well plate with image and optionally binary overlay thumbnails. Any image thumbnail column is suitable. The thumbnail is typically produced by the    node.GeneralGraph Title Sets the title of the graph shown in the top.DataDisplay Data Selects what data should be displayed (All data, Current frame, ...), i.e. the default value.InteractionName Defines the graph name in the layouts.Icon Defines the graph icon in the layouts.Display Data Selection Allows the user to change the set Display Data directly in the chart (a drop-down menu appears at the top if something was checked). When having multiple charts/tables in the layout, they can Synchronize with each other. Select the current frame for a table and the next chart will also show only data from the current frame.Selections visible Enables the user to select color channels and/or binary layers.Data selection Data selection enables the user to select a row with a link into the image (selects objects or moves to the relevant frame). Results &amp; Graphs &gt; Wellplate &gt; Labeling🔗 Displays the well plate with labeling information: positive/negative controls, detection and custom labels. Uses detection and labeling columns from a table typically produced by the    node.GeneralGraph Title Sets the title of the graph shown in the top.DataDisplay Data Selects what data should be displayed (All data, Current frame, ...), i.e. the default value.InteractionName Defines the graph name in the layouts.Icon Defines the graph icon in the layouts.Display Data Selection Allows the user to change the set Display Data directly in the chart (a drop-down menu appears at the top if something was checked). When having multiple charts/tables in the layout, they can Synchronize with each other. Select the current frame for a table and the next chart will also show only data from the current frame.Selections visible Enables the user to select color channels and/or binary layers.Data selection Data selection enables the user to select a row with a link into the image (selects objects or moves to the relevant frame). Results &amp; Graphs &gt; Wellplate &gt; Linechart🔗 Displays the well plate with a chart visualizing Y as a function of X.GeneralGraph Title Sets the title of the graph shown in the top.DataDisplay Data Selects what data should be displayed (All data, Current frame, ...), i.e. the default value.X-axis column Select data for the x axis column.Y-axis column Select data for the y axis column.InteractionName Defines the graph name in the layouts.Icon Defines the graph icon in the layouts.Display Data Selection Allows the user to change the set Display Data directly in the chart (a drop-down menu appears at the top if something was checked). When having multiple charts/tables in the layout, they can Synchronize with each other. Select the current frame for a table and the next chart will also show only data from the current frame.Column Selection Select which columns can be interactively selected in graph top toolbar for each axis.Data selection Data selection enables the user to select a row with a link into the image (selects objects or moves to the relevant frame). Results &amp; Graphs &gt; Wellplate &gt; Violin🔗 Displays the well plate with violin plot in each well. Any violin aggregated column per well is suitable as input.GeneralGraph Title Sets the title of the graph shown in the top.DataDisplay Data Selects what data should be displayed (All data, Current frame, ...), i.e. the default value.Columns Selects the features for the graph.InteractionName Defines the graph name in the layouts.Icon Defines the graph icon in the layouts.Display Data Selection Allows the user to change the set Display Data directly in the chart (a drop-down menu appears at the top if something was checked). When having multiple charts/tables in the layout, they can Synchronize with each other. Select the current frame for a table and the next chart will also show only data from the current frame.Column Selection Select which columns can be interactively selected in graph top toolbar for each axis.Selections visible Enables the user to select color channels and/or binary layers.Shown columns Shown columns to force visible. The text is regular pattern that must match. Show takes precedence over hide.Hidden columns Hidden columns to force not visible. The text is regular pattern that must match. Use “*” to select all columns in conjunction with show.Data selection Data selection enables the user to select a row with a link into the image (selects objects or moves to the relevant frame). Results &amp; Graphs &gt; Tables &gt; Summary🔗 Table view for showing summary information in the form of two columns: header and value. The input to the summary table is a table which gets transposed in such a way that the column names become header and the first row values become values.DataDisplay Data Data to be displayed.Shown columns Shown columns to force visible. The text is regular pattern that must match. Show takes precedence over hide.Hidden columns Hidden columns to force not visible. The text is regular pattern that must match. Use “*” to select all columns in conjunction with show.Sections Sections define the first rows of sections. Section is separated visually by an extra space.Header suffix Header suffix for all headers (e.g. “:”).InteractionName Defines the graph name in the layouts.Icon Defines the graph icon in the layouts.Display Data Selection Allows the user to change the set Display Data directly in the chart (a drop-down menu appears at the top if something was checked). When having multiple charts/tables in the layout, they can Synchronize with each other. Select the current frame for a table and the next chart will also show only data from the current frame. Results &amp; Graphs &gt; Tables &gt; Table🔗 Table view for showing the data. Table columns can be resized, sorted, moved, filtered if enabled in the node. Statistics can be enabled as well.GeneralTable Title Sets the title of the table shown in the top.Show row index column Show row index column in front of each row.Enable sorting Enable sorting on all columns.Enable resizing Enable resizing on all columns.Enable moving Enable moving on all columns.Enable filtering Enable filtering on all columns.DataDisplay Data Selects what data should be displayed (All data, Current frame, ...), i.e. the default value.Shown columns Shown columns to force visible. The text is regular pattern that must match. Show takes precedence over hide.Hidden columns Hidden columns to force not visible. The text is regular pattern that must match. Use “*” to select all columns in conjunction with show.InteractionTab Title Sets the title of the table shown in the top.Tab Icon Defines the tab icon in the layouts.Display Data Selection Allows the user to change the set Display Data directly in the chart (a drop-down menu appears at the top if something was checked). When having multiple charts/tables in the layout, they can Synchronize with each other. Select the current frame for a table and the next chart will also show only data from the current frame.Statistics Selects the statistics rows to be calculated.Data selection Data selection enables the user to select a row with a link into the image (selects objects or moves to the relevant frame). Results &amp; Graphs &gt; Tables &gt; Table with Stats🔗 This node works like node   , but you need to connect a table with measured statistics to it (e.g. from the    node). Results &amp; Graphs &gt; Tables &gt; Object Catalog🔗 Creates a catalog of objects. The node has to be connected to the    node. Results &amp; Graphs &gt; Reports &gt; Report🔗 Puts the connected results into a report which is defined in this node and shown in the Analysis Results tab. For details about defining the report, please see  .   Results &amp; Graphs &gt; Reports &gt; HTML Report🔗 Creates report as HTML document using   template (MD), Python code and optional CSS. Currently the node is lacking interactive editing aids and this documentation is rudimentary. Therefore it should be used by advanced users only. Figure&nbsp;837.&nbsp;The report connects to one or more table inputs (including layouts).The MD template contains Markdown (may contain HTML) for static content and more or less sections enclosed in {% raw %}{{{% endraw %} {% raw %}}}{% endraw %} which are replaced by dynamic content.For every {% raw %}{{ arg0, ..., argN, kwarg0=value0, ..., kwargM=valueM }}{% endraw %} section {% raw %}printToHtml(arg0, ..., argN, kwarg0=value0, ..., kwargM=valueM){% endraw %} is called.By default it calls the {% raw %}limreport.defaultPrintToHtml(){% endraw %} but can be overridden.Typically {% raw %}arg0{% endraw %} is the name of the input a for A, b for B etc. If the input is a layout the {% raw %}arg1{% endraw %} should be the pane index counting from the left. Remaining parameters depends on the type of the input (graph, wellplate, table, summary).Examples of MD template:Display name of the experiment into the heading (#), a is a layout, 0 is the first pane (summary), 'Experiment name' is column name: Figure&nbsp;838.&nbsp;Display dosing and labeling side-by-side, b is layout 1 is labeling pane and 2 is dosing pane (image at the top). Figure&nbsp;839.&nbsp;Display all available features Dose response fit, a is layout 2 is fit plot pane, cycle will cycle over all available features. Figure&nbsp;840.&nbsp;Displaying a table, a is layout, 2 is table pane, number of rowHeaderCols=1 and custom table html style. Figure&nbsp;841.&nbsp; Figure&nbsp;842.&nbsp;Displaying small thumbnails, d is a table generated using    node. Figure&nbsp;843.&nbsp;  Figure&nbsp;844.&nbsp;Below is the default python which can be used as it is.{% raw %}import importlib\nimport limreport, limhtml\n\n# useful for development\nimportlib.reload(limreport)\nimportlib.reload(limhtml)\n\n# called for every {{ ... }} in MD\ndef printToHtml(*args, **kwargs):\n    return limreport.defaultPrintToHtml(*args, **kwargs)\n\n# if present called on final HTML \ndef processHtml(html):\n    # chromio v.83 doesn't support break-after, break-before: avoid-page\n    # wrap &lt;hN&gt;&lt;/hN&gt;&lt;element&gt;&lt;/element&gt; into:\n    # &lt;div class=\"lim-no-page-break-inside\"&gt;&lt;hN&gt;&lt;/hN&gt;&lt;element&gt;&lt;/element&gt;&lt;/div&gt;\n    parser = limhtml.AvoidPageBreaks()\n    parser.feed(html)\n    return parser.output(){% endraw %}For further info see the following files:{% raw %}C:\\Program Files\\NIS Elements\\Python\\Lib\\site-packages\\limhtml.py\nC:\\Program Files\\NIS Elements\\Python\\Lib\\site-packages\\limreport.py\nC:\\Program Files\\NIS Elements\\Python\\Lib\\site-packages\\limtabledata.py{% endraw %} Results &amp; Graphs &gt; Legacy Graphs &gt; Barchart🔗 This graph is used for visualizing Y of ordinal X. Settings of each tab are described below.GeneralTitle Sets the title of the graph shown in the top.Inside Sets the inside color of the graph (color behind the visualized data).Outside Sets the outside color of the graph (frame around the visualized data).Default Dark Returns the colors to the default dark scheme.Axes Sets the color of the axes.Text Sets the color of the texts.Default Light Returns the colors to the default light scheme.Series Sets the color scheme for the grouped data.Display Values Shows data values directly in the graph.Hidden Hides the legend.Inside Graph Shows the legend inside the graph. Choose the legend position in the drop-down menu next to this option.Below Graph Shows the legend below the graph.Background Sets the background color of the legend.Text Sets the color of the legend text.DataAll Columns This tab is used to pair the graph axes with the variables available in the table input. Select a data variable and click on the arrow  next to the selected axis to assign it to this axis. Multiple variables can be assigned to the Y axes.   button opens the Data Series dialog setting up the data series graph properties such as the color, line type, stroke type, line width, fill, marker type, and values type. Only the checked properties are visualized in the graph.   button opens the Error Bar dialog setting up the error bar properties for the selected column. Choose an error column from the drop-down menu and optionally set its color, line width, and whisker width.  buttons move the variable in the axis list up/down. button moves the variable from the axis list back to the All Columns list.X Axis/Y Axis/Left Y Axis/Right Y Axis/Color Axis/Size Axis/Category AxisTitle Sets the title of the axis that is currently being set.Reversed Range Reverses the range of the category axis.Visible Shows/hides the labels for the axis currently being set.Format Specifies the axis label format for the axis currently being set.Precision Specifies the axis label numeral precision.Step Specifies the step size of the category axis.Minimum In Auto mode, the minimal value for the range is set automatically. If switched to Fixed, a minimal value from which the data are visualized can be entered into the edit box. Scale specifies the scale type - linear or logarithmic.Maximum In Auto mode, the maximal value for the range is set automatically. If switched to Fixed, a maximal value to which the data are visualized can be entered into the edit box. Reversed Range reverses the values range from “min - max” to “max -min”.Major Step In Auto mode, the major step value for the ticks and gridlines is set automatically. If switched to Fixed, a major step value used for the ticks and gridlines visualization can be entered into the edit box. If Tick is checked, a short tick is added to the axis next to each major step value. Grid color specifies the color of the grid line.Minor Step In Auto mode, the minor step value for the ticks and gridlines is set automatically. If switched to Fixed, a minor step value used for the ticks and gridlines visualization can be entered into the edit box. If Tick  is checked, a short tick is added to the axis next to each minor step value. Grid color specifies the color of the grid line. Results &amp; Graphs &gt; Legacy Graphs &gt; Colormap🔗 Provides a graphical representation of data where the individual values contained in a two-dimensional matrix are represented as colors from selected color gradient map. This graph does not support grouped source table A.All tabs are closely described in the    node.Typical UsecasesHeatmap of a property related to grid type patterns (wellplate) where x, y axes are mapped into columns and rows.Tracking results with fixed number of tracks and time points can color code speed or other track properties at given time. Results &amp; Graphs &gt; Legacy Graphs &gt; Linechart🔗 This graph is used for visualizing Y as function of X.All tabs are closely described in the    node. Results &amp; Graphs &gt; Legacy Graphs &gt; Scatterplot🔗 This graph is used for visualizing a cloud of points. If the source table A is grouped the graph shows them in colors.All tabs are closely described in the    node. Results &amp; Graphs &gt; Legacy Graphs &gt; Scatterplot XY Color🔗 This 3D graph is used for visualizing a cloud of points. The third axis is visualized using a color gradient. This graph does not support grouped source table A.All tabs are closely described in the    node. Results &amp; Graphs &gt; Legacy Graphs &gt; Scatterplot XY Size🔗 This 3D graph is used for visualizing a cloud of points. The third axis is visualized using disks of different size. This graph does not support grouped source table A.All tabs are closely described in the    node. Results &amp; Graphs &gt; Legacy Graphs &gt; Statistical Box🔗 Depicts groups of numerical data through their quartiles as a  . It shows minimum, first quartile, mean, third quartile and maximum. It can optionally show outliers. If the source table A is grouped the graph shows one box for each group.All tabs are closely described in the    node. ",
     id: 138 }, 
   { title: "Segmentation",
     xmlid: "id|node.segmentation_sectionx",
     content: "  Threshold        Spot detections     Special detections        Interactive     JavaScript      Segmentation &gt; Threshold &gt; Threshold🔗 Thresholds the connected result using a simple slider and creates a binary. Figure&nbsp;732.&nbsp;,  Pick threshold from image - single pixel, ,  Select the picking tool and click on objects in the image you wish to detect. Color values of each of the clicked pixel(s) are used to adjust the low and the high threshold values so that the clicked pixel would stay inside the thresholded interval. To reset the threshold values back to default click .The histogram can be zoomed using the top-right icons: Set to current threshold range Zooms to the current range. Set to actual (min, max) range Zooms to the minimum - maximum range. Set to full range Zooms to the full domain.Smooth, Clean, Fill Holes, Separate These binary processing functions can be activated and their value adjusted using the arrows or directly by typing the value into the edit box. Fill Holes processing can only be turned on/off whereas the values of other processings are applied as a radius in µm or px.Size, Circularity Filter the detected binary objects using the Size or Circularity restriction filters.For more information about thresholding please see  .   Segmentation &gt; Threshold &gt; RGB Threshold🔗 Thresholds the connected RGB result and creates a binary. Figure&nbsp;733.&nbsp; Pick threshold from image - single pixel,  Pick threshold from image - small circle,  Pick threshold from image - large circle Select a picking tool and click on objects in the image you wish to detect. Color values of each of the clicked pixel(s) are used to adjust the low and the high threshold values so that the clicked pixel would stay inside the thresholded interval. To reset the threshold values back to default click .The histogram can be zoomed using the top-right icons: Set to current threshold range Zooms to the current range. Set to actual (min, max) range Zooms to the minimum - maximum range. Full Range Zooms to the full domain.Smooth, Clean, Fill Holes, Separate These binary processing functions can be activated and their value adjusted using the arrows or directly by typing the value into the edit box. Fill Holes processing can only be turned on/off whereas the values of other processings are applied as a radius in µm or px.Size, Circularity Filter the detected binary objects using the Size or Circularity restriction filters.For more information about RGB thresholding please see  .   Segmentation &gt; Threshold &gt; HSI Threshold🔗 Thresholds the connected RGB result based on Hue, Saturation and Intensity and creates a binary result. Figure&nbsp;734.&nbsp; Pick threshold from image - single pixel,  Pick threshold from image - small circle,  Pick threshold from image - large circle Select a picking tool and click on objects in the image you wish to detect. Color values of each of the clicked pixel(s) are used to adjust the low and the high threshold values so that the clicked pixel would stay inside the thresholded interval. To reset the threshold values back to default click .Smooth, Clean, Fill Holes, Separate These binary processing functions can be activated and their value adjusted using the arrows or directly by typing the value into the edit box. Fill Holes processing can only be turned on/off whereas the values of other processings are applied as a radius in µm or px.Size, Circularity Filter the detected binary objects using the Size or Circularity restriction filters.For more information about HSI thresholding please see  .   Segmentation &gt; Threshold &gt; Simple Threshold🔗 Thresholds the connected result using a simple slider and creates a binary. Figure&nbsp;735.&nbsp; Pick threshold from image - single pixel,  Pick threshold from image - small circle,  Pick threshold from image - large circle Select a picking tool and click on objects in the image you wish to detect. Color values of each of the clicked pixel(s) are used to adjust the low and the high threshold values so that the clicked pixel would stay inside the thresholded interval. To reset the threshold values back to default click .The histogram can be zoomed using the top-right icons: Set to current threshold range Zooms to the current range. Set to actual (min, max) range Zooms to the minimum - maximum range. Full Range Zooms to the full domain.For more information about thresholding please see  .   Segmentation &gt; Threshold &gt; Multilevel Otsu🔗 Calculates the threshold values of Multilevel Otsu's thresholding and thresholds the connected image to multiple binary layers. Select the number of output binary layers using the Levels  option.For more information please see  . Used algorithm is time-consuming. With increasing number of layers the accuracy is decreasing to improve its speed. However accuracy is not decreased for two or three levels. When connected to an RGB image, the algorithm calculates thresholds for image converted to a grayscale (intensity) channel.   Segmentation &gt; Spot detections &gt; Bright Spots🔗 This function opens the Spot Detection dialog window in the Bright Spots mode. It is used mainly for detecting circular objects with similar sizes. For more information please see  . General analysis 3 spot detection uses the Different Sizes object detection method.   Segmentation &gt; Spot detections &gt; Dark Spots🔗 This function opens the Spot Detection dialog window in the Dark Spots mode. It is used mainly for detecting circular objects with similar sizes. For more information please see  . General analysis 3 spot detection uses the Different Sizes object detection method.   Segmentation &gt; Special detections &gt; Homogeneous Area🔗 Opens the Homogeneous Area Detection dialog window suitable for detecting objects on a homogeneous background and objects which are difficult to segment. Advantage of this function is that the objects do not need to be distinguished by intensity because the function works with edges. This is useful especially for DIC and phase-contrast images. Figure&nbsp;736.&nbsp;Channel Channel on which the detection will be performed.Inversion Inverts the detected binary.Compactness The higher the value, the more compact object border is created.Edge Based Method, Variance Based Method Try which method works best for your image. In some cases, the Variance Based Method may bring better results.Threshold Use this slider to find the optimal threshold for your objects.   Segmentation &gt; Special detections &gt; IHC Classification🔗 Performs automatic   (IHC) cell diagnosis on the connected color image creating two binary images - one Positive (segmented from bright blue colors) and one Negative (segmented from dark brown colors). The third segmented class is the background (usually white) which is not shown as an output.Conditions for the inclusion into these three classes vary from image to image and it is impossible to make implicit segmentation conditions. Therefore, the K-means algorithm (k = 3, unsupervised clustering) is used.First, the image pixels are converted to an alpha-beta-intensity color space (related to the HSL model). Then the initialization is done on a:pixel with the greatest deflection in the color plane that predominates in the image,dark pixel,light pixel.The K-means algorithm iterates over pixels, assigns it to the nearest cluster (in a converted color model), and creates new centers until it changes. The returned binaries then contain pixels in the first (positive) and second (negative) class according to this clustering. Figure&nbsp;737.&nbsp;Immunohistochemical staining example   Segmentation &gt; Special detections &gt; Labels to Binary🔗 Connected color image is converted into a binary image. Touching color areas are separated into different binary objects. Figure&nbsp;738.&nbsp;Example of use (RGB Input and Binary output) Figure&nbsp;739.&nbsp;   Segmentation &gt; Special detections &gt; Labels to Binary 🔗 Connected color image is converted into a 3D binary image. Touching color areas are separated into different binary objects.   Segmentation &gt; Special detections &gt; Tight Borders🔗 Performs the segmentation of binary result using the tight borders detection.Threshold Detects pixels with intensities higher than the specified threshold value. The higher the value the less objects are detected.Delete Branches with Maximal Length(px) Deletes branches with length smaller than the length specified.Connect Free Endings Connects free endings of the neighboring branches.Vanish Objects with EqDia less than Objects with EqDia (equivalent diameter of a circular object) smaller than the defined value will be deleted.But Keep Objects with Border Stronger Than Objects with their border stronger than the defined value will be kept.   Segmentation &gt; Interactive &gt; Draw Rectangle🔗 Draws a rectangle ROI either by hand (use the  tool) or based on the defined position (X, Y) and size (Width, Height). Rectangle must be valid  ensures that the drawn rectangle is not empty.   Segmentation &gt; Interactive &gt; Draw Objects🔗 Opens a set of tools enabling the user to draw any objects into the current image. Figure&nbsp;740.&nbsp; Draw rectangle Click and drag in the image to draw a rectangle. Draw square Click and drag in the image to draw a square. Draw circle Click and drag in the image to draw a circle. Draw Line Click and drag in the image to draw a line. Draw Polyline Click points in the image to define the polyline. Use the secondary mouse button or a double-click to place the last node of the polygon and close the object. Draw circle by defining 3 points Click three points in the image which will be placed on the perimeter of the defined circle. Fit circle into N points Click as many points you like and confirm the definition by the secondary mouse click. All the points are enclosed within the circle. Draw ellipse Click and drag in the image to draw a circle. Then click again on its edge and drag to create an ellipse. Draw ellipse defined by 5 points Click 5 different points in the image. The system automatically interpolates these points and finishes the ellipse. Draw polygon Click points in the image to define the polygon. Use the secondary mouse button or a double-click to place the last node of the polygon and close the object. Auto detect Click on an object in the image to automatically detect and draw its borders. Remove last object Deletes the object defined at last. Clear all Deletes all objects drawn in the image.   Segmentation &gt; JavaScript &gt; JS Segmentation🔗 Transforms the source color image into the binary result with transformation programmed in JavaScript. Up to 6 color/binary/table inputs can be added.See the dedicated documentation:  . ",
     id: 139 }, 
   { title: "Sources &amp; Reference",
     xmlid: "id|node.sourcesreference_sectionx",
     content: "  Source Image       Source Binary       Source Table    Merge Tables     Parameters      Import        Export         Temporary         Wizard      Save to Document     NSPARC   Deep Sim      Sources &amp; Reference &gt; Source Image &gt; All Channels🔗 Inserts a single node with an overlay of all channels present in the current image.   Sources &amp; Reference &gt; Source Image &gt; Channels🔗 Produces picture outputs representing channels of the source document:Composition of all channels named “All”.Composition of all fluorescence channels named “Fluo”.Individual channels.Matching rules can be edited by the user for the correct mapping of channels from a different document. Matching of channels is done in following order:Matching by channel name - searching the name in a given list of comma separated names.Matching by channel type - if channel type is Mono/Fluo, matching continues by comparing excitation wavelengths, and if still no match, by emission wavelengths.Unmatched channels are assigned to the rest of the channels with the same channel type (no checking of wavelengths in this step). If there are no such channels available, missing channel is added.It is possible to assign a custom description to each channel (e.g. nuclei, cell, spots).   Sources &amp; Reference &gt; Source Image &gt; Combined Channels🔗 Produces picture outputs representing both, the “All” channel composition and the individual channels.   Sources &amp; Reference &gt; Source Image &gt; Selected Channels🔗 Inserts a parental node with one channel selectable from the list of available channels (Selected) and the second channel with all the remaining channels together (Other).   Sources &amp; Reference &gt; Source Image &gt; MxN pixels🔗 Creates a new color image (channel) using the specified dimensions (Width, Height) and bit depth and fills it with the specified value.   Sources &amp; Reference &gt; Deep Sim &gt; Reconstruction🔗 Inserts a Deep SIM image.   Sources &amp; Reference &gt; Deep Sim &gt; ZStack Reconstruction🔗 Inserts a Deep SIM Z-Stack image.   Sources &amp; Reference &gt; Source Binary &gt; Binaries🔗 Inserts all binary layers found in the source image and selected in the drop-down menu.   Sources &amp; Reference &gt; Source Binary &gt; Selected Binaries🔗 Inserts a parental node with one output containing the binary layer selectable from the list of available binary layers (Selected) and a second output with all the remaining binaries together (Other).   Sources &amp; Reference &gt; Source Binary &gt; Single Binary🔗 Inserts a binary node using a segmentation selected in the drop-down menu.   Sources &amp; Reference &gt; Source Binary &gt; ROI🔗 Inserts a ROI node using ROI(s) from the current image.   Sources &amp; Reference &gt; Source Table &gt; Load Results🔗 Inserts a table node showing the selected Result from the chosen Analysis and Module.   Sources &amp; Reference &gt; Merge Tables &gt; Load &amp; Concatenate🔗 Loads the result table specified by the Result name  from database and appends the content of the connected table to it. When the result table does not exist, the connected table is set as the result. It is useful when some analysis (statistics, classification) has to be calculated over all runs, not just the current one. In such a case use the result R to perform the analysis on.Result name Name of the result table from database.Result matching Sets the synchronizing rule for appending connected table's content (truncate when different columns / synchronize columns).Add extra column Adds extra columns for distinguishing analysis runs by Filename or Run Index .   Sources &amp; Reference &gt; Merge Tables &gt; Concatenate HDF5🔗 Loads the result table from a HDF5 file given by the Filename parameter and appends records from node's input table. Table location inside the HDF5 file is defined by the current analysis name and node's output name.   Sources &amp; Reference &gt; Parameters &gt; Input Folder🔗 Defines the folder used as an input. Click ... and select the folder.   Sources &amp; Reference &gt; Parameters &gt; Input Number🔗 Defines the input value.   Sources &amp; Reference &gt; Parameters &gt; Input Text🔗 Defines the input text.Sources &amp; Reference &gt; Import &amp; Export common features Import and Export nodes share some folder path handling and naming conventions.When exporting files, the path specified in the Folder (or Filename in the case of CSV) dictates where the files will be saved. The path can be either absolute or relative:Absolute Path If the path starts with a drive letter (C:\\ ...), the file will be exported to that absolute path.Relative Path If the path is relative (., folder\\, or .\\folder), the file will be exported relative to the destination file (dst). Typically, dst is the same as src, except in the case of the GA Executor where it might differ.When using the name field as a format template, placeholders can be included in the name string to customize the output filenames. These placeholders will be replaced as follows:{fn}: The complete filename (e.g. 02AU.nd2).{fnb}: The base part of the filename (e.g. 02AU-enhance.ai.nd2 becomes 02AU-enhance.ai). This does not apply to CSV files, as they produce a single file by concatenating rows.Additional placeholders are also available:{n:xx}: Sequential number with xx indicating the number of digits.{l:xx} or {li:xx}: Loop counter where l is the loop type and xx is the number of digits.{ln}: Loop name where l is the loop type (only available for p, w, m, and s).{lb}: Loop barcode where l is the loop type (only available for p and s).Example:{fnb}_M{m:2}_T{t:2} will generate “02AU_M01_T01”, “02AU_M01_T02”, ... and append the appropriate filename extension.Different loop types and their corresponding identifiers are used in placeholders:p Platew Wellm XY Multi-Pointt Time-lapsez Z-Stackc Counts Slider Regionx Temporaryh ChannelList separator&nbsp;Microsoft Excel should use the same List separator value when opening a CSV file so that the CSV file is correctly opened.In Microsoft Windows 11, the list separator is set in Time &amp; Language &gt; Language &amp; region &gt; Administrative language settings &gt; Formats &gt; Additional settings &gt; Numbers &gt; List separator.   Sources &amp; Reference &gt; Import &gt; Color Image🔗 Imports a color image from the specified path.   Sources &amp; Reference &gt; Import &gt; Binary from ND2🔗 Imports a binary/Global ROI from the specified nd2 file.   Sources &amp; Reference &gt; Import &gt; Binary from CSV🔗 Loads a binary layer consisting of rectangular objects representing bounding rectangles or centers of original binary objects. The main purpose of this node is the visualization of objects predicted by AI nodes. Optionally the objects' probability can be taken into account and objects can be filtered according to it.The source table is assumed to be accumulated for the whole dimension and the first column represents the dimension index. The following columns are mandatory and represent the bounding rectangles' positions in pixels:“xmin” - left edge“xmax” - right edge“ymin” - top edge“ymax” - bottom edgeThe “label” column is also mandatory and stands for binary layers name. The “prob” column filled with objects probabilities is optional.Source table example: Table&nbsp;14.&nbsp;tindexxminyminxmaxymaxlabelprob010253601033373sperm0.99980366478372491sperm0.9998111863511198358sperm0.99971250926318sperm0.999726626577272sperm0.9995Filename Source table location.Loop The table dimension representation.Label The desired label searched in source table data.Minimum probability Filter for “prob” column (when present).Rectangle, Center Selects one of the binary layer types being loaded (bounding rectangles or centers of original binary objects).   Sources &amp; Reference &gt; Import &gt; Table from CSV🔗 Imports a table from a comma separated values file (.csv). A data type (number or text) is determined for every column. The header (if present) can force the column type and a specific column meaning. Following header syntax is recognized:{% raw %}Name{% endraw %} or {% raw %}Keyword{% endraw %} {% raw %}[Unit]{% endraw %} : {% raw %}Type{% endraw %}Name Any name except colon.Keyword For special columns like _Entity, _ObjId or _ObjId3d.Unit Any unit. This is an optional item.Type Is int for integer; number, real, double for real numbers and string, text for strings. This is an optional item.  Table&nbsp;15.&nbsp;Example_ObjIdSize [m] : numberDescription : text11258reference object Filename Source table location.Locale If left empty, the system locale is used. Otherwise the entered two-letter   is utilized.If the CSV is saved in Czech or French, where the decimal separator is a comma, and by default, the locale in Windows is set to US, where the decimal separator is a dot, the numbers will not be recognized and will be read as text. This can be resolved by specifying \"cs\" or \"fr\" in the Locale, and the import will use this information to correctly recognize the numbers.Column separator Is used to delimit columns (comma “,” (default), tabulator “TAB” or semicolon “;”).First row is header with column names Check this item when the first row is a header (not actual data).   Sources &amp; Reference &gt; Import &gt; Table from HDF5🔗 Loads a table from  .Table can be:an ordinary 2D dataset (row x columns) containing atomic datatypes or variable length strings,a table previously exported using “Table to HDF5” (group of column datasets).When the format is not recognized table is not loaded.Filename Path to the file containing the table.Location in file Group or dataset in the file e.g. {% raw %}/Ga3Tables/Records{% endraw %}.   Sources &amp; Reference &gt; Export &gt; Color to Single TIFs🔗 Exports TIFF images to the specified folder with the given name. This action can be used for having a reference when GA3 is used more than once JOBS. Figure&nbsp;845.&nbsp;Folder Path where the TIFF images will be saved.Name Name of the TIFF files.Numbering If numbering is set to None, a single file is created. This file is overwritten with every frame of the new data. This is useful for single frame or reduced files from the ND Processing nodes. The other options create one TIFF file per frame:Append sequential number: {% raw %}C:\\seq00000.tiff{% endraw %}Append loop fields {% raw %}C:\\seq00000z000.tiff{% endraw %}Use name as format template   Sources &amp; Reference &gt; Export &gt; Binary to SVGs🔗 Exports objects from the connected binary layer into .svg images.Folder Destination folder for saving the created file.Name Created file name.Numbering If numbering is set to None, a single file is created. This file is overwritten with every frame of the new data. The two other options create one .svg file per frame:Append sequential number: {% raw %}C:\\seq00000.svg{% endraw %}Append loop fields {% raw %}C:\\seq00000z000.svg{% endraw %}Use name as format templateFill Opacity Sets fill opacity of the exported object.Stroke Opacity Sets stroke opacity of the exported object.   Sources &amp; Reference &gt; Export &gt; Split to Multipoint🔗 Creates new nd2 file from cropped regions of the original one represented as a multipoint loop. Frames are ordered according to the selected column from the input table. Other loops (Timelapse, ZStack) are preserved.Folder Destination folder for saving the created file.Name Created file name.Name Suffix Suffix of the created files in case of saving to separate files (other loops than Timelapse and ZStack exist).Multipoint Index Column Column from the input table determining the cropped regions order in the resulting multipoint loop.Center X[px] Column Column from the input table determining the center X of the cropped region in pixels.Center Y[px] Column Column from the input table determining the center Y of the cropped region in pixels.Width Width of the cropped region.Height Height of the cropped region. Preview creates a filename with a “Preview” prefix.   Sources &amp; Reference &gt; Export &gt; Table to CSV🔗 Exports the table into a .csv ( ) file.Filename Name of the file and the path where it will be saved.Delete the file before each run Deletes the file content with each GA3 run.Delete Deletes the file content.Delimiter Choose a field separator of the .csv file.   Sources &amp; Reference &gt; Export &gt; Table to HDF5🔗 Stores a table into a  .Filename Path to the file containing the table.Location in file Group or dataset in the file e.g. {% raw %}/Ga3Tables/Records{% endraw %}.Delete the table before each run Sets whether the table is cleared before it is written to or appended. Preview creates a filename with “Preview” prefix.   Sources &amp; Reference &gt; Export &gt; Result to Image🔗 Exports the table into a .png image.Folder Path where the images will be saved.Name Name for the image files.Numbering If numbering is set to None, a single file is created. This file is overwritten with every frame of the new data. This is useful for single frame or reduced files from the ND Processing nodes. The other options create one PNG file per frame:Append sequential number: {% raw %}C:\\seq00000.png{% endraw %}Append loop fields {% raw %}C:\\seq00000z000.png{% endraw %}Use name as format template   Sources &amp; Reference &gt; Temporary &gt; Load Last Color🔗 Loads data previously saved with the    node. The data is shared in the specified loop and kept private in other loops.Store name Name of the storage used to distinguish the temporary stores.Loop Loop where the data is shared (&lt;none&gt; to share in all loops).Set Invalid As Specifies the substitution when the storage is invalid (Zero Image / Maximum Intensity Image / Current Image).   Sources &amp; Reference &gt; Temporary &gt; Save Last Color🔗 Saves the data to temporary storage. The data is shared in the specified loop and kept private in other loops. The B input must be connected to the    node to ensure that it is executed before the Save.Store Name Name of the storage used to distinguish the temporary stores.Loop Loop where the data is shared (&lt;none&gt; to share in all loops).   Sources &amp; Reference &gt; Temporary &gt; Load Last Binary🔗 Loads data previously saved with the    node. The data is shared in the specified loop and kept private in other loops.Store Name Name of the storage used to distinguish the temporary stores.Loop Loop where the data is shared (&lt;none&gt; to share in all loops).Set Invalid As Specifies the substitution when the storage is invalid (Empty Binary / Full Binary / Current Binary).   Sources &amp; Reference &gt; Temporary &gt; Save Last Binary🔗 Saves the data to temporary storage. The data is shared in the specified loop and kept private in other loops. The B input must be connected to the    node to ensure that it is executed before the Save.Store Name Name of the storage used to distinguish the temporary stores.Loop Loop where the data is shared (&lt;none&gt; to share in all loops).   Sources &amp; Reference &gt; Temporary &gt; Load Last Table🔗 Loads data previously saved with    node. The data is shared in the specified loop and kept private in other loops.Store Name Name of the storage used to distinguish the temporary stores.Loop Loop where the data is shared (&lt;none&gt; to share in all loops).Set Invalid As Specifies the substitution when the storage is invalid (Empty Table / Current Table).   Sources &amp; Reference &gt; Temporary &gt; Save Last Table🔗 Saves the data to temporary storage. The data is shared in the specified loop and kept private in other loops. The B input must be connected to the    to ensure that it is executed before the Save.Store Name Name of the storage used to distinguish the temporary stores.Loop Loop where the data is shared (&lt;none&gt; to share in all loops).   Sources &amp; Reference &gt; Wizard &gt; Visible Channels🔗 Defines which channels (the connected ones) are shown in the wizard, whether autoscale LUTs is turned on and whether a pseudocolor is used for their visualization.   Sources &amp; Reference &gt; Wizard &gt; Visible Binaries🔗 Defines which binary layers (the connected ones) are shown in the wizard, whether the object IDs are shown and how the binary layer transparency is set.   Sources &amp; Reference &gt; Wizard &gt; Visible Tables🔗 Defines which tables (the connected ones) are shown in the wizard.   Sources &amp; Reference &gt; Save to Document &gt; Save Pictures🔗 Saves all connected channels into the result document. Connected inputs are synchronized by the “ToBeStored” property of the channel outputs present in the analysis.   Sources &amp; Reference &gt; Save to Document &gt; Save Binaries🔗 Saves all connected binaries into the result document. Connected inputs are synchronized by the “ToBeStored” property of the binary outputs present in the analysis.   Sources &amp; Reference &gt; Save to Document &gt; Save Tables🔗 Saves all connected tables into the result document. Connected inputs are synchronized by the “ToBeStored” property of the table outputs present in the analysis.   Sources &amp; Reference &gt; NSPARC &gt; Array Components🔗 Loads array components (25 components each) visible only in the preview. Cannot be stored.   Sources &amp; Reference &gt; NSPARC &gt; Reassign🔗 Calls the reassign function and (modified) array components collapse back to mono channels visible in the preview. It can be stored.Sources have to be added explicitly and the source node needs a GUI for channel mapping or binary selection. ",
     id: 140 }, 
   { title: "Tracking",
     xmlid: "id|node.tracking.group_sectionx",
     content: "  Object Position           Tracking       Tracking Features      Tracks     Results      Tracking &gt; Object Position &gt; Time &amp; Center🔗,   Tracking &gt; Object Position &gt; Time &amp; Center  Shows the time and center of the 2D/3D object for tracking.   Tracking &gt; Object Position &gt; Time &amp; CenterAbs🔗,   Tracking &gt; Object Position &gt; Time &amp; CenterAbs  Shows the time and center of the 2D/3D object for tracking in absolute (stage) coordinates.   Tracking &gt; Object Position &gt; Time &amp; Centroid🔗,   Tracking &gt; Object Position &gt; Time &amp; Centroid  Shows the time and the center of gravity of the 2D/3D object for tracking.   Tracking &gt; Object Position &gt; Time &amp; CentroidAbs🔗,   Tracking &gt; Object Position &gt; Time &amp; CentroidAbs  Shows the time and the center of gravity of the 2D/3D object for tracking in absolute (stage) coordinates.   Tracking &gt; Tracking &gt; Track Objects🔗,   Tracking &gt; Tracking &gt; Track Objects  Tracks 2D/3D binary objects that must overlap between frames/volumes. In an ambiguous situations, the biggest intersection wins. This is suitable especially for slowly moving (compared to the framerate) big cells (where the center is not much representative).   Tracking &gt; Tracking &gt; Track Particles🔗,   Tracking &gt; Tracking &gt; Track Particles  Tracks non overlapping 2D/3D objects represented by time and position. Please see   explaining the available parameters.   Tracking &gt; Tracking Features &gt; Feature Derivative🔗 Calculates derivation of the value in time ((Fi - Fi-1) / (dT)), while requiring Diff Time.Please see  .   Tracking &gt; Tracking Features &gt; Tracking Features &gt; Motion Features🔗 Returns a table with motion features.List of Motion FeaturesΔTime  Figure&nbsp;848.&nbsp;Distance  Figure&nbsp;849.&nbsp;Elevation  Figure&nbsp;850.&nbsp;Acceleration Normal  Figure&nbsp;851.&nbsp;Turning Rate  Figure&nbsp;852.&nbsp;Path Speed  Figure&nbsp;853.&nbsp;Straightness  Figure&nbsp;854.&nbsp;ΔPosition  Figure&nbsp;855.&nbsp;Speed  Figure&nbsp;856.&nbsp;Acceleration  Figure&nbsp;857.&nbsp;ΔHeading  Figure&nbsp;858.&nbsp;Pitching Rate  Figure&nbsp;859.&nbsp;Line Length  Figure&nbsp;860.&nbsp;Velocity  Figure&nbsp;861.&nbsp; Heading  Figure&nbsp;862.&nbsp; Acceleration Tangential  Figure&nbsp;863.&nbsp;ΔElevation  Figure&nbsp;864.&nbsp;Path Length  Figure&nbsp;865.&nbsp;Line Speed  Figure&nbsp;866.&nbsp;   Tracking &gt; Tracking Features &gt; Track Features🔗 Returns a table with track features. Input of this node is a table after    node is applied. Output of this node are rows (number of rows is equal to the number of groups on the input - i.e. tracks).List of Track FeaturesSegments  Figure&nbsp;867.&nbsp;Speed  Figure&nbsp;868.&nbsp;Acceleration  Figure&nbsp;869.&nbsp;Straightness  Figure&nbsp;870.&nbsp;Length  Figure&nbsp;871.&nbsp;Heading  Figure&nbsp;872.&nbsp; Line Length  Figure&nbsp;873.&nbsp;Duration  Figure&nbsp;874.&nbsp;Elevation  Figure&nbsp;875.&nbsp;Line Speed  Figure&nbsp;876.&nbsp;   Tracking &gt; Tracks &gt; Accumulate Tracks🔗 Creates a table with all frames and groups them by the track. Enables filtering based on the number of segments (Min segment count ).   Tracking &gt; Tracks &gt; MSD🔗 Creates a table with points of MSD ( ) curve. Select the Track ID, Time and Position columns. Before analyzing using this node use Accumulate Tracks (  ) to group by “Track ID”.MSD Curve is generated for each group separately. If you want to have one curve for each track, group the table by Track ID. If you want one curve in total, ungroup the table.Mean square displacement can also be used from the Intra-Nuclear Single Particle Tracking ( ).   Tracking &gt; Results &gt; Export Tracking🔗 Populates the standard NIS-Elements Tracking Control ( ) with the connected tracking table. Tracking data are saved in the assigned document.(requires:  )Tracking operates frame-by-frame (whole timelapse is not needed – useful for JOBs). In order to get all frames for the inspection or measurement, use the    node. ",
     id: 141 }, 
   { title: "N-STORM Acquisition and Analysis",
     xmlid: "id|nstorm.externalx",
     content: " Please follow the links below which lead to separate documentations:   All linked documentations are installed to the NIS-Elements directory (typically “C:\\Program Files\\NIS-Elements\\Docs”) only if “N-STORM” is selected during the installation procedure. ",
     id: 142 }, 
   { title: "N-STORM Z-Calibration (Agilent and LU4)",
     xmlid: "id|nstorm.zcalibrationx",
     content: "         N-STORM uses the astigmatism method of 3D localization published by Huang et al. ( ). Molecules appear stretched in the x or y axis, with the orientation and magnitude of the stretch dependent on z position. A calibration of Gaussian width in x (Wx) and y (Wy) as a function of z position is generated by translating diffraction limited spots in the z dimension using a piezo stage insert (figure below). 100 nm TetraSpeck beads are used to z calibrate the three reporter wavelengths and also calculate a warp to correct registration between channels (correction for chromatic aberrations for both xy and z). Figure&nbsp;1336.&nbsp;N-STORM z calibrationThe calibration steps are: collect 20 periods at the starting (in focus) position, move piezo to -800 nm at period 21, step piezo up in 10 nm increments to +800 nm, return piezo to starting (in focus) position for final 20 periods. The cylindrical lens changes the image of molecules' axial ratios as a function of z. The sequence can be performed simultaneously for all three reporter wavelengths (total of 603 frames). Connect the Mad City Labs Nano-Z100 piezo stage to the PC via USB only (no more analog connectors). In case the MCL NanoDrive module is installed make sure to go to Devices &gt; Device Manager and disconnect the MCL NanoDrive module. Otherwise the piezo stage will not move. Figure&nbsp;1337.&nbsp;Properly mounted calibration sampleThe setup for calibration with 35 mm glass bottom dish is shown. Other sample vessels and holders can be used, but must be secured similarly.Make sure the insert is securely mounted to the piezo by the 6 small screws, then turn the unit on. Place the calibration sample on the stage and secure it with clips (figure above).Insert the cylindrical lens.Focus on the beads using any of the reporter wavelengths. Identify a field with a dispersed, uniform distribution of beads within the 256x256 region used for STORM. Avoid sparse fields and bead clumps (if beads are too close they will overlap when elongated which can lead to miss localizations). Figure&nbsp;1338.&nbsp;100 nm Tetraspeck bead fieldA good 256x256 pixel field has a sufficient number of beads which are evenly distributed. There are few clumps or overlapping beads.Setup the desired reporter wavelengths by clicking on Settings in the N-STORM with the Agilent Acquisition window. Label the three reporter channels and select the appropriate lasers (figure below, red boxes). The color for displaying localizations in each channel can also be set here. If a laser is not selected as reporter wavelength it will not be available in the Z-calibration dialog (see the next step). Figure&nbsp;1339.&nbsp;Setting up the channels and lasers for calibrationLabel and select the appropriate lasers here. Cycle settings and activation probes are ignored for calibration. Also, 405 nm is not used as a reporter and cannot be calibrated.Click “Z Calibration” in the N-STORM with the Agilent Acquisition window (figure below, left). The window will change to calibration mode (figure below, right) and the piezo will immediately home to the center of its range. You will therefore have to refocus by moving up 50 µm. Figure&nbsp;1340.&nbsp;Calibration settingsAcquisition dialog is switched to calibration mode by clicking “Z Calibration” (left, red circle). The reporter wavelengths to be calibrated are checked and path/filenames specified. Run Now button begins the calibration routine outlined in the “N-STORM z calibration” figure.Check the boxes corresponding to the reporters to be calibrated, and specify the path and name of the calibration file (figure above, right).As a final step, go live using the camera and refocus on the beads. Adjust laser power to give a good signal to noise ratio in each channel. Also check for the out of focus levels where the beads are elongated. At the same time you should avoid saturation in the in-focus frames. Use the “Piezo Z move” arrows (figure above, green box) to set the step size and fine tune focus with the piezo. The goal is to get beads as round as possible (axial ratio ~ 1). Zooming in helps in this regard.Press Run Now to acquire the calibration (figure above, red arrow). Connect the Mad City Labs Nano-Z100 piezo stage to the PC via USB only (no more analog connectors). In case MCL NanoDrive module is installed, make sure to go to Devices &gt; Device Manager and disconnect the MCL NanoDrive module. Otherwise the piezo stage will not move. Figure&nbsp;1341.&nbsp;Properly mounted calibration sampleThe setup for calibration with 35 mm glass bottom dish is shown. Other sample vessels and holders can be used, but must be secured similarly.Make sure the insert is securely mounted to the piezo by the 6 small screws, then turn the unit on. Place the calibration sample on the stage and secure it with the clips (figure above).Insert the cylindrical lens.Focus on the beads using any of the reporter wavelengths. Identify a field with a dispersed, uniform distribution of beads within the 256x256 region used for STORM. Avoid sparse fields and bead clumps (if beads are too close they will overlap when elongated which can lead to miss localizations). Figure&nbsp;1342.&nbsp;100 nm Tetraspeck bead fieldA good 256x256 pixel field has a sufficient number of beads which are evenly distributed. There are few clumps or overlapping beads.Setup the desired reporter wavelengths by clicking on Settings in the N-STORM window. Select Multi-color Continuous Mode and label the three reporter channels and select the appropriate lasers (figure below, red boxes). The color for displaying localizations in each channel can also be set here. If a laser is not selected as reporter wavelength it will not be available in the Z-calibration dialog and if it is not labelled correctly it will appear wrong in the z-calibration dialog (see the next step). Figure&nbsp;1343.&nbsp;Setting up the channels and lasers for calibrationLabel and select the appropriate lasers here. Cycle settings and activation probes are ignored for calibration. Also, 405 nm is not used as a reporter and cannot be calibrated.Click on Advanced in the N-STORM window and then on Z Calibration in the panel that appears below (figure below, left). Select 3D STORM. The window will change to calibration mode (figure below, right) after a message that will ask you to put in the cylindrical lens, the piezo will immediately home to the center of its range (you will therefore have to refocus by moving up 50 µm) and all the lasers go to 0% while a live window appears. For refocusing select one laser channel and set the laser power to 5-10%. Figure&nbsp;1344.&nbsp;Calibration settingsAcquisition dialog is switched to calibration mode by clicking Z Calibration (left, red circle). The reporter wavelengths to be calibrated are checked and path/filenames specified. Run Now button begins the calibration routine outlined in the “N-STORM z calibration” figure.Check the boxes corresponding to the reporters to be calibrated, and specify the path and name of the calibration file (figure above, right).As a final step, adjust laser power to give a good signal to noise ratio in each channel. Check for this also out of focus levels where the beads are elongated. At the same time you should avoid saturation in the in-focus frames. Use the “Piezo Z move” arrows (figure above, green box) to set the step size and fine tune focus with the piezo. The goal is to get beads as round as possible (axial ratio ~ 1). For the fine adjustment you can use the Axial Ratio test tool (figure above, red box and snapshot below). By pressing Test Ax the N-STORM Graph Ax window opens up. You can see the live axial ratio for all selected colors. By selecting the range and the Auto Scroll period number you can adjust the graph. Bring the axial ratio of all colors as close as possible to 1 by moving up and down with the piezo (red box in the figure below). The differences you can see between the colors are due to chromatic aberrations that can be corrected later on. Figure&nbsp;1345.&nbsp;Press Run Now to acquire the calibration (figure above - Calibration settings, red arrow). Open the N-STORM Analysis Module and load the calibration ND2 file.Set the Identification S6ettings (figure below, left - Identification of beads for 3D calibration) for each channel low enough that beads are picked up in the out of focus frames. It is important to realize that in order to get a calibration of +/- 400 nm around focus only the most central 80 frames are needed (figure below). In these 80 frames it is important to get as many but also as reliable identifications as possible. Therefore, you should select a frame between 40 and 50 and base your identification settings on this frame. Make sure that you get around 50% of the beads identified. Recheck your settings on frame 160. Also here you should have around 50%. Figure&nbsp;1346.&nbsp;Reasonable calibrationFinally, check your settings on frame 60, 100 and 140. In these 3 frames you should get as many beads identified as possible and as little false positive as possible. Use the “Test” function to confirm this for all channels prior to running the analysis (figure below, right). Run a 3D analysis (figure below, red arrow) of the calibration data. Figure&nbsp;1347.&nbsp;Identification of beads for 3D calibrationSelect the minimum height for each channel. In this example, a minimum height of 4000 for the 561 channel (left) results in identification of molecules in period 21 (right), corresponding to a z position of -800 nm.To z calibrate, click on Z Calibration in the Identification Settings window (figure below - Generating the z calibration, red arrow). By pressing the Graph button in the Z-calibration dialog and then holding shift while selecting Axial Ratio vs. Period (frame average) a graph will pop up that will show the changes in the axial ratio as function of frame number of all acquired channels (up to 3, figure below, lower panel). Use this graph to judge if the calibration will be OK. Please check for this if the (almost) linear behavior of the curves covers a range of at least 600 nm (in the case below it is around 800 nm, from frame 60 to 140 = 8 frames of 10 nm steps = 800 nm). Figure&nbsp;1348.&nbsp;Judging the quality of the Z-calibration dataIf correct, press Auto Calibrate in the Z Calibration window (figure below, green arrow). There is a pause while the calibration is generated, after which the Auto Calibrate button is grayed out. New values and graphs will populate the dialog if calibration is successful (figure above, top right panel). The software automatically sets the range of the calibration depending on the quality of the data (figure below, red box). The difference between start and end point should be in the range of 600-1200 nm. Usually it will be around 800-900 nm. Figure&nbsp;1349.&nbsp;Generating the z calibrationThe Z Calibration window on the right is opened through the Identification Settings window (red arrow). Pressing Auto Calibrate (green arrow) calibrates the system. Clicking OK and OK again applies the calibration to the system.If the calibration fails, the following error message will pop up. Figure&nbsp;1350.&nbsp;In the location where your acquired z-calibration file is stored you will now find a file with time and date as file name with the ending .log. At the very bottom of the file you will find an error message. Please see the Error messages during 3D calibrations chapter for the meaning of different error messages and to find suggestions how to avoid the errors.If successful, evaluate the result of the calibration by checking the newly accessible calibration graphs (see typical graphs below). Note they are only available right after performing the auto calibration. The Z-calibration graph should display two crossing curves as shown below. Ideally the fitted yellow and blue curves are symmetric. Adjusting the correction collar may be needed if the curves are not symmetric. The axial frame average vs. z position graph should show a linear behavior over the central 800 nm (see example below). Any back curling should be avoided. If starting and end points should be around aspect ratio of 1. If they are far off (like in the shown example) please rerun the calibration and focus the starting point better. Finally, the average per frame of Wx vs. Wy should show a c-like shape. Again back curling should be avoided by all means. These three types of graphs are separately available per channel of the calibration data. Figure&nbsp;1351.&nbsp;If the results are not satisfactory, press Cancel to revert back to the original calibration. To accept and apply the new calibration to the system OK must be clicked twice, first in the Z Calibration window then in the Identification Settings window (figure above - Generating the z calibration). Please see the N-STORM Analysis help file (see  ) for a description of the parameters displayed at the end of calibration and description of calibration graphs.After z-calibration is complete, you can now correct for chromatic aberrations, using the same data set. For this click Calibrate under XY Warp in the ID Settings window (figure below, yellow arrow). A window opens displaying the result of the warp (figure below, right). Click OK to apply the 3D Warp to the system. This step can be skipped if the warping will not be used. Figure&nbsp;1352.&nbsp;Calibrating the system warpClicking “Calibrate” (yellow arrow) calculates the warp. The result window (right) opens when the warp is successful. The cylindrical lens introduces a small distortion to the image. For this reason, it is necessary to generate a separate warp if channel registration is to be corrected for 2D experiments collected without the cylindrical lens. The workflow is similar to the steps described above for 3D, and is summarized below.Remove the cylindrical from the light path.Bring the beads into focus with the piezo and collect a calibration file for all channels to be warped as in steps 3-8 above. Note, that there will be no asymmetric stretch of beads without the cylindrical lens but the beads will blur instead.Open and analyze the calibration file, this time using 2D analysis parameters (figure below). Also you should choose your identification settings depending on an in-focus frame (one of the first or last 20 frames of the acquisition or around frame 100 of the acquisition. These are the frames that should be in focus). As the 2D warp calibration only has to correct in 2D the warp correction is based on the frame where the beads are best focused (axial ratio closest to 1). Therefore, the identification settings can be more strict than in the 3D case where also out of focus beads need to be fitted. Figure&nbsp;1353.&nbsp;Analysis for 2D warpBy unchecking “3D” in the Identification Settings window (red arrow), screening parameters are changed for analysis of 2D experiments collected without the cylindrical lens.Click Calibrate under XY Warp (figure above, yellow arrow). A warp for 2D is generated and a window opens displaying the result of the warp (figure below). Click OK to apply the 2D Warp to the system. This step can be skipped if warping will not be used. Figure&nbsp;1354.&nbsp;2D warp resultNote that the window now specifies that the 2D warp is generated (green circles). The z-calibration algorithm automatically judges how good the acquired data of the z-calibration data file is. The quality of the data set depends both on the quality of the acquisition but also on the quality of the analysis. Thereby, selecting strict identification settings for the analysis of the z-calibration dataset can result in a better calibration curve.In the following section the checkpoints and the criteria that the software applies are described. You will also find some suggestions how to resolve the situation if you get error messages following a failure at one or the other checkpoint. Note that only the first criterion that fails will generate the error message. Therefore several improvement steps might be needed before a calibration succeeds.What are the criteria the software uses to judge if the z-calibration is OK or not:Let`s have a look at the average per frame graph of a rather good (almost ideal) calibration data set. Vertical axis is the average per frame axial ratio. The horizontal axis is the period number (or for a single channel acquisition). The points (A, B, C, D, E, F, G) will help to define the constraints. Figure&nbsp;1355.&nbsp;The first 20 frames of a z-calibration are acquired near the focal plane (data between points A and B on the graph). It is called the “first plateau”. Similarly the “second plateau” is the last 20 frames (data between points F and G). All points between B and F are acquired during the piezo scan in the middle between the two plateaus. In the scan extremes (between B and C and between E and F) optical aberrations are often so bad that the data is highly distorted (for example Airy disks or bad noise, etc). These two regions are called “garbage” and the software tries to exclude them from calculation.The points between C and E form a nice curve that is easy to follow and fit. Somewhere in the middle of that region there is point D where the axial ratio is closest to 1.The small blue letters in the graph (a, b, c, d, e, f) represent the following constraints: Figure&nbsp;1356.&nbsp;a) Absolute vertical position of first plateau The vertical position of every point between A and B must be within the range of 0.625 &lt; a &lt; 1.6 (note: 1 / 1.6 = 0.625).b) Flatness of the first plateau The vertical range of the points between A and B must be less than 0.1 to avoid a warning and less than 0.2 to avoid an error.c) Horizontal distance (in frames) between the beginning of the good data C and the midpoint D It must be at least 30 frames which equals 300 nm.d) Horizontal distance (in frames) between the beginning the midpoint D and the end of the good data E It also must be at least 30 frames.e) Same as “b” only for the second plateau (between F and G). f) Same as “a” only for the second plateau (between F and G). For multichannel acquisitions, each channel is evaluated independently. As you can imagine, satisfying all these requirements gets more difficult when:you do 3 channels at once (because of chromatic aberrations),you have an imperfect objective with asymmetrical PSF,you start your acquisition far from the focal point,you have excessive drift or vibrations,piezo is not working properly.Possible error messages:Error! Channel[1 - Alexa 647] one of detected scan edges[58 120] is closer to detected midpoint[95] than min[30]The horizontal (frame) position of your point C was 58, E was 120, and D was 95. Therefore D was not far enough from E (25 only instead of the required 30). Therefore criterion d is violated. A similar effect could happen for point C and criterion c.Solution: Your starting point is probably not well focused. If you start your calibration at an out of focus position the midpoint (axial ratio =1) will not be centered. Alternatively, your piezo might not move linearly.Error! Channel[1 - 647nm] Average Ax Range[0.202741] is above tolerance [0.200000]The flatness of the first plateau is not good enough. The variation in axial ratios during the first 20 steps is too high. Criterion b (or e) is violated.Solution: Check if your sample is well clamped. Also check if the anti-vibration works properly or if there is any other reason for vibrations (e.g. camera or other instrumentation on the system, air draft, construction work, etc.). Finally, also check if the beads on your sample are not floating and that all beads are in the same focal plane.Warning! Channel[1 - 647nm] Average Ax Range[0.102741] is above tolerance[0.100000]Same as error number 2 but this time only a warning is displayed. It means that your calibration is OK but you should check if there is a way to improve the situation.Error! Channel[1 - 647nm] Min[1.000000] or Max[1.620727] Average Ax in starting plateau is outside of tolerance range[0.625000 - 1.600000]One or several of the points in the first plateau is outside the tolerance range. This leads to a violation of criterion a (or f in case of the second plateau).Solution: Check if you start the z-calibration procedure in an in-focus position. If the starting plateau is OK but the end plateau not, you are most likely suffering from a drift. Please check if you sample is clamped well, that there is no airflow on the system and that there are no temperature fluctuations. Also check that frame averaging is disabled. Finally, make sure that the beads in your sample are not floating and that all the beads are in the same focal plain. Why are my beads clumped together? Beads aggregate during storage. They must be vortexed or sonicated before and after diluting to ensure a good distribution on the coverslip.Should I calibrate with beads in water? Beads for calibration are imaged on the surface of the coverslip throughout the range of calibration. Therefore, the imaging medium has a negligible effect on the optical path length (spherical aberration) and imaging beads in air is acceptable. Calibration can be performed in water, but care must be taken that beads are not floating (which creates non-uniformity in axial ratios for a given z position).The piezo does not move. Is it broken? Check that the piezo controller is connected to the PC by USB and turned on and that the MCL NanoDrive module is disabled (if applicable) under the Manage Devices settings. Also, very long USB cables sometimes cause communication issues for the Nano-Z100 controller. A short cable may restore piezo movement in this instance.Why is there little to no stretch in the beads/molecules when I focus up and down? Make sure the cylindrical lens is in. If there is still no stretch, adjust the correction collar (turning it clockwise usually increases stretch, at the cost of enhancing optical aberrations that can hurt calibration). See the following question “Can I backup my z calibration” for more information on the correction collar.How many beads should be in the field for calibration? Enough to generate good statistics, while at the same time avoiding clumps and/or overlap. This is usually accomplished with 25-100 beads in the 256x256 pixel region.When I try to calibrate a warp I get an error that there is an “Insufficient number of calibration point pairs”. What does this mean? A warp is a non-linear transformation of one channel with respect to another. This means the registration shift is different throughout the field. The same beads are imaged with different wavelengths to act as fiducials mapping the registration offset between channels. To help ensure that the entire field is sampled a minimum of 10 beads must be successfully identified in both channels that are mapped to each other (there a three warps generated; 647 to 488, 561 to 488, 647 to 561). Lowering identification settings often results in more matching pairs. If the calibration still fails, move to a different field.Do I need to manually set maximum displacement and turn off drift correction when analyzing calibration data? Earlier versions of the N-STORM analysis required max displacement set to zero and drift correction manually turned off by the user for calibration, and then turned back on for experimental data. Analysis modules with N-STORM with Agilent version 1.0.9 and later (analysis module b85 or higher) recognize calibration files and automatically set these parameters.Can I backup my z calibration? Yes, you can. The z calibration can be saved as a text file. It is highly recommended to save a backup copy of the current system calibration with your original data. This text file is also useful to move calibration to an offline workstation. The calibration is saved and re-loaded through the Z Calibration dialog (figure below). Figure&nbsp;1357.&nbsp;Saving and loading z calibrationsClick Save (green arrow) in the Z Calibration window to save the calibration as a text file. A saved calibration is re-loaded by clicking Load (red arrow). It is necessary to click OK twice.Where should the correction collar be set on my objective? If your N-STORM system is equipped with a 100x 1.49 NA TIRF objective it will have a correction collar. The position of this collar has a large impact on the z calibration. Generally, the position for 23 ºC, 0.17 mm coverslip thickness is a good place to start. The collar should be set such that there is pronounced stretch in the molecules when they move out of focus. At the same time, it can be used to offset optical artifacts. These typically appear as holes in the diffraction limited spots at the extreme out of focus range (around -800 or +800 nm, figure below). The goal of collar adjustment is to maximize molecule stretch while minimizing the appearance of these holes, which will cause a reversal in the measured axial ratio and decrease system calibration range. Figure&nbsp;1358.&nbsp;Molecule shapes and aberrations during z calibrationZoomed and cropped images of beads from opposite ends of a z calibration. At the -800 nm piezo position, the molecules appear stretched vertically with no significant aberration. Holes resulting from optical aberration are observed at the +800 nm piezo position. Correction collar adjustment can help balance the stretch and aberration. ",
     id: 143 }, 
   { title: "Ca 2+  Ion Concentration Measurement",
     xmlid: "id|option.CalciumCalibrationx",
     content: " The Calcium and FRET module provides methods for an efficient calcium ion concentration measurement.See also  .Ratio Experiment CommonsUsually, Fura-2 ratiometric fluorescent dye which binds to free intracellular calcium is used.Fura-2 is excited at 340 nm and 380nmRatio of Emission of 340 and 380 correlated to the amount of intracellular calciumThe ratio of the emissions at 340 and 380 wavelengths is directly correlated to the amount of intracellular calciumWhy Use a Ratio? Because it is not as sensitive to changes in:Dye concentrationAuto fluorescent backgroundFocus DriftAmbient Lamp ProcedureDefine ROIs for the Ca 2+ measurement.Perform   either during an ND2 experiment or on an existing ND2 file.Turn Ratio View ON (  ).Go to the Time Measurement window and display the ratio graph in the Channels  mode.Press the Define Calcium Calibration  button within the time measurement control panel to pick the calibration values from the graph.A wizard starts. Follow its instructions - pick the R min a R max values from the ratio graph.The [Ca2+] graph display becomes available to be displayed in the time measurement window.The [Ca2+] image channel view is created within the ND2 file. It visualizes the Ca2+ ion concentration via a color scale.Ratio View Properties - Calcium Calibration settings&nbsp;If you prefer to modify the Calcium Calibration by hand, run the    command. In the window that appears select the Use [Ca2+] calibration check box (or it is already selected if the calibration procedure was performed beforehand). The Calcium Calibration tab appears. Figure&nbsp;653.&nbsp; All constants necessary for calcium ion concentration calculation can be defined manually within the Ratio Properties window, either for each ROI separately or globally (the Use per ROI Calibration option). Use the Pick values from the time graph to pick R min / R max constants from the Time Measurement graph - the time measurement must have been performed before. The F min / F max values will be computed automatically.[Ca2+] Concentration of Calcium in nanoMoles.Kd Dissociation constant: 135nM at 20ºC, 224nM at 37ºCFmin The 380 intensity in the moment of R min.Fmax The 380 intensity in the moment of R max.R Ratio of 340/380 intensity at the current time pointviscosity “Constant” between 0.7 and 1.Rmin The minimum ratio value of the experimentRmax The maximum ratio value of the experimentThe calibration settings can be saved for further use by the Save button (and loaded later by the Load button). ",
     id: 144 }, 
   { title: "Introduction to FRET",
     xmlid: "id|p2c10s1x",
     content: " What is FRET?A non-radiative transfer of energy from a donor molecule to an acceptor molecule.Förster/Fluorescence Resonance Energy TransferFluorescence emission from an acceptor molecule due to non radiative energy transfer when a donor molecule was excited.Conditions for FRETDonor's emission must overlap acceptor's excitationDonor and acceptor molecules must be &lt;100 Å (angstroms) apart.Donor and acceptor molecules' dipole orientation must be parallel. ",
     id: 145 }, 
   { title: "FRET Example",
     xmlid: "id|p2c10s6x",
     content: " This “how to” expects you to have correctly captured Donor-only, Acceptor-only, and FRET-sample images.See also:  .SummaryUse the Create FRET Image from Files dialog to build a new .ND2 file of the 4 component images from the donor only sample.Use the Create FRET Image from Files dialog to build a new .ND2 file of the 4 component images from the acceptor only sample.Using the FRET Calibration dialog, use the .ND2 files from steps 1 &amp; 2 to draw ROIs and generate FRET Coefficients.Use the Create FRET Image from Files dialog (which now has all of the coefficients imported automatically into it from step #3) to generate a FRET .ND2 file from the component images of the FRET sample (there are 2 sets of images to try).Determine the mean FRET efficiency of the cells using the ROI statistics dialog. ProcedureFirst, create an .ND2 file from the donor only images (control images) using the Create FRET image from Files dialog. See   . Figure&nbsp;654.&nbsp;An .ND2 FRET image will be created for the donor-only sample. Keep it open and available. Figure&nbsp;655.&nbsp;Next, create an .ND2 file from the acceptor only images (control images) using the Create FRET image from Files dialog Figure&nbsp;656.&nbsp;An .ND2 FRET image will be created for the acceptor-only sample. Keep it open and available. Figure&nbsp;657.&nbsp;Open the FRET Calibration dialog (  0), and choose the appropriate .ND2 to use for the donor and acceptor only images. All open documents appear in the drop-down menu. Figure&nbsp;658.&nbsp;Create ROIs and a background ROI in the donor only sample .ND2 image. Use the Define ROI and Define Background ROI buttons from the dialog window. Figure&nbsp;659.&nbsp;Create ROIs and a background ROI in the acceptor only sample .ND2 image and click OK. Figure&nbsp;660.&nbsp;Hitting OK will open the    dialog and fill the FRET coefficients fields. Confirm it with OK.Now re-open the Create FRET Image from Files dialog. Note the calibrations have been imported. Use the dialog to create an .ND2 file of the FRET sample image Figure&nbsp;661.&nbsp;A new .ND2 file of the FRET image will be created, with tabs for all individual components. There will also be tabs corr FRET for corrected FRET and FRET eff for FRET efficiency. Figure&nbsp;662.&nbsp;Clicking on the FRET eff tab will give a pseudocolored image with a scale Figure&nbsp;663.&nbsp;You can use ROI statistics (see   ) to visualize FRET efficiencies. You can also draw ROIs around cells of interest if necessary Figure&nbsp;664.&nbsp;Intensity Profile (see   ) will also output FRET efficiency along a line when the FRET eff tab on the image is selected. Figure&nbsp;665.&nbsp; ",
     id: 146 }, 
   { title: "Changes from 5.40",
     xmlid: "id|p2c11s2x",
     content: " Main changes in GA3 are described below:GA3 Editor windowNon-modal window NIS-Elements inspection functions are always available (LUTs, Binary Layers, Histogram, Image info, Profile line, 3D Object Meas, etc.).Document can be modified without closing the editor. Preview will be switched OFF automatically. When switched back ON the preview will be recalculated.Several crops of a big document can be made and switched between from within the editor.Node visualization The node and its output require a smaller amount of space.The new node visualization provides a clear and distinct visual separation between various output types (Color, Binary, Table, Graph, etc.) and 2D/3D nodes.Node list with a 2D/3D switch A more meaningful category list.Switch between 2D and 3D nodes to prevent accidental mixing.All nodes on the screen at once (collapsible).Efficient search with the capability to insert nodes, eliminating the need to view the node list. Figure&nbsp;685.&nbsp; Figure&nbsp;686.&nbsp;Sections Visually group nodes (color and name).Independent zoom. The section can be minimized to create extra space for editing other parts of the GA3 and maximized when needed. Figure&nbsp;687.&nbsp; NodesSave nodes New save nodes (color image, binary, tables) which cannot be deleted.What needs to be saved should be connected to these nodes. Figure&nbsp;688.&nbsp;Measurement (2D only) The individual measurement features are grouped into nodes, including field, object, object count, parent, and cell.Renaming, ordering, visibility, joining, calculation is available inside the node.New    node to simplify cell measurements (must be connected to   ). Figure&nbsp;689.&nbsp;Result nodes New results nodes (graphs, table and well-plate views) enhance the after-run interactivity.New graphs currently support feature selection and range.Legacy graphs continue to be supported under the Legacy Graphs  category.New table allows for efficient filtering. Figure&nbsp;690.&nbsp;Layouts The introduction of horizontal (  ), stacked (  ), and display (  ) layout nodes enables the creation of application-like results displays.The    and    menu functions activate the menu and side panel which is filled with results specified in the    node. ",
     id: 147 }, 
   { title: "Basic Control",
     xmlid: "id|p2c11s3x",
     content: "   Connecting nodes&nbsp;The connections between nodes are established through lines, determining the sequence of the analysis. Inputs are represented by shapes like circles, diamonds, or squares with letters (A, B, C, ...), which can be moved and joined to other nodes to create a connection. Certain nodes may have optional inputs that are not mandatory to use.Defining parameters of nodes&nbsp;Click on the node parameters button  or double-click on the node name to open a dialog for defining the node parameters. To enable/disable a node, reveal the context menu over the node and check/uncheck the first radio button.Previewing and running the analysis&nbsp;After building your analysis, turn on the Preview switch to calculate the analysis preview. If everything worked well, you can click  Run to execute the analysis. Analysis is performed, its definition and results are saved into   Analysis Explorer (  ) and a Analysis Results are shown. If there is a problem with the analysis definition, an information icon  is shown - hover over the icon to read the error message.Results storing&nbsp;Results of all executed analyses are saved into   Analysis Explorer (  ). Clicking on the arrow  of the analysis definition reveals analysis records (date of execution and name of the source image). Each row (analysis record) represents an analysis run executed on the analysis definition with the same name. Double-clicking on the analysis record or clicking   Open opens its source image, whereas double-clicking or clicking   Edit on the analysis definition name opens this definition inside the GA3 definition window. If an analysis record with table data is selected,   Show result table for selected run button is shown in the    toolbar and can be used to view the results data in the Analysis Results panel (  ). ",
     id: 148 }, 
   { title: "Examples",
     xmlid: "id|p2c11s4x",
     content: " Examples have a new home at GitHub. This change allows us to continue improving the examples even after the release of the NIS-Elements software.Please visit   where more examples and use-cases from the field are published.   ",
     id: 149 }, 
   { title: "Hidden Nodes (created in NIS-Elements 5.4x)",
     xmlid: "id|p2c11s5s13x",
     content: " The following chapter contain descriptions of legacy nodes which are no longer available in the GA3 Editor . Recipes created with version 5.4x may utilize such nodes. Therefore they are maintained in the software and also their descriptions are kept here:   ND Processing &amp; Conversions &gt; Bitdepth &gt; Change Bit Depth🔗 Changes the bit-depth of the connected result. Figure&nbsp;877.&nbsp;Output bit depth Sets a new color depth of your image. 8-bit/16-bit integer or a floating-point. Intensity values can be rescaled.See  .Re-scale intensity values If checked the pixel values are mapped to the new range. E.g. when you convert an 8bit image to 16bit and you do not rescale the values, it results in a completely black image. If the check box was selected, the 8bit 255 value would become 65535 in 16bit etc.   Binary processing &gt; Filter objects &gt; Filter Objects🔗 Removes any binary object not meeting the specified condition. Objects having the specified feature outside of the specified range will be removed.   Binary processing &gt; JavaScript &gt; JS Postprocess🔗 Transforms the source into the result with transformation programmed in JavaScript. Up to 6 binary/color/table inputs can be added.See the dedicated documentation:  .   Measurement &gt; Whole field &gt; Object Count🔗 Counts the number of objects in the connected binary result.   Measurement &gt; Whole field &gt; Total Pixel Count🔗 Counts the number of pixels (“TotalPixelCount”) in the connected binary result.   Measurement &gt; Whole field &gt; Total Object Area🔗 Area covered by the connected binary result (“TotalObjectArea”) is calculated.   Measurement &gt; Whole field &gt; Area Fraction🔗 Area of the connected binary result is divided by the total area of the analysed image (“AreaFraction”).   Measurement &gt; Whole field &gt; Measured Area🔗 Displays the total area (“MeasuredArea”) of the currently analysed image.   Measurement &gt; Whole field &gt; Perimeter🔗 Please see   .   Measurement &gt; Whole field &gt; Mean Chord🔗 Please see   .   Measurement &gt; Whole field &gt; SurfVolumeRatio🔗 Please see   .   Measurement &gt; Field intensity &gt; Background Estimate🔗 Calculates the background estimate value.   Measurement &gt; Field intensity &gt; Entropy🔗 Calculates   of all pixel values in the color image. If the connected color image has multiple channels they are averaged into a single value per pixel. If a binary is connected only pixels under it are taken. Figure&nbsp;878.&nbsp;where P(X) is the distribution of intensity values.   Measurement &gt; Field intensity &gt; Focus Criterion🔗 Calculates the value defining which focus criterion is used. For more information please see   .   Measurement &gt; Field intensity &gt; Kurtosis🔗 Calculates sample   of all pixel values in the color image. If the connected color image has multiple channels they are averaged into a single value per pixel. If a binary is connected only pixels under it are taken. Figure&nbsp;879.&nbsp;   Measurement &gt; Field intensity &gt; Maximum🔗 Calculates the maximal pixel intensity (“MaxIntensity”) of the connected color result under the connected binary result. Please see   .   Measurement &gt; Field intensity &gt; Mean🔗 Calculates arithmetic   of all pixel values in the color image. If the connected color image has multiple channels they are averaged into a single value per pixel. If a binary is connected only pixels under it are taken. Figure&nbsp;880.&nbsp;   Measurement &gt; Field intensity &gt; Minimum🔗 Calculates the minimal pixel intensity (“MinIntensity”) of the connected color result under the connected binary result. Please see   .   Measurement &gt; Field intensity &gt; Mode🔗   finds pixel value that appears most often in the color image. If the connected color image has multiple channels they are averaged into a single value per pixel. If a binary is connected only pixels under it are taken.   Measurement &gt; Field intensity &gt; Otsu Threshold🔗 Calculates the value of Otsu's threshold. For more information please see  .   Measurement &gt; Field intensity &gt; Quantile🔗 Calculates the n-th quantile of all pixel values in the color image. If the connected color image has multiple channels they are averaged into a single value per pixel. If a binary is connected only pixels under it are taken.Quantile is a cut point dividing the range of values of input into two parts such that the first part contains 100 * Quantile % of values and the second part contains 100 * (1 - Quantile) % of values. If the quantile is set to 0.5, this function computes the median of values.   Measurement &gt; Field intensity &gt; Sigma Threshold🔗 This action fits a Gaussian curve to the pixel value density graph (histogram) using linear regression. If the connected color image has multiple channels they are averaged into a single value per pixel. It returns the following Gaussian curve parameters: Mean, Sigma, and R^2 (coeffiecient of determination representing the goodness of fit).Equation of Gaussian curve: Figure&nbsp;881.&nbsp;   Measurement &gt; Field intensity &gt; Skewness🔗 Calculates sample   of all pixel values in the color image. If the connected color image has multiple channels they are averaged into a single value per pixel. If a binary is connected only pixels under it are taken. Figure&nbsp;882.&nbsp;   Measurement &gt; Field intensity &gt; Standard Deviation🔗 Calculates sample   of all pixel values in the color image. If the connected color image has multiple channels they are averaged into a single value per pixel. If a binary is connected only pixels under it are taken. Figure&nbsp;883.&nbsp;   Measurement &gt; Field intensity &gt; Sum🔗 Calculates sum of all pixel values in the color image. If the connected color image has multiple channels they are averaged into a single value per pixel. If a binary is connected only pixels under it are taken. Figure&nbsp;884.&nbsp;   Measurement &gt; Field intensity &gt; Uniformity🔗 Calculates uniformity of all pixel values in the color image. If the connected color image has multiple channels they are averaged into a single value per pixel. If a binary is connected only pixels under it are taken. Figure&nbsp;885.&nbsp;where P(X) is the distribution of intensity values.   Measurement &gt; Field intensity &gt; Variance🔗 Calculates sample variance of all pixel values in the color image. If the connected color image has multiple channels they are averaged into a single value per pixel. If a binary is connected only pixels under it are taken. Figure&nbsp;886.&nbsp;   Measurement &gt; Field ratiometry &gt; Ratio🔗 Calculates the mean of ratios between corresponding pixels of the two input channels under the input binary mask for the whole field.   Measurement &gt; Field ratiometry &gt; Pearson Coeff🔗 Calculates Pearson correlation coefficient for the whole field. For more information please see   .   Measurement &gt; Field ratiometry &gt; Mander Coeff🔗 Calculates Manders overlap for the whole field. For more information please see   .   Measurement &gt; Field metadata &gt; Time🔗 Displays the acquisition time (“Time”) of the connected color result.   Measurement &gt; Field metadata &gt; Bit Depth🔗 Displays the bit-depth of the connected color result.   Measurement &gt; Field metadata &gt; Calibration🔗 Displays the calibration of the connected color result.   Measurement &gt; Field metadata &gt; Filename🔗 Displays the filename of the image being processed.   Measurement &gt; Field metadata &gt; Filepath🔗 Displays the path where the image being processed is saved.   Measurement &gt; Field metadata &gt; Channel Name🔗 Displays the channel name of the connected source channel.   Measurement &gt; Field metadata &gt; Channel Metadata🔗 Returns basic metadata of each input channel component (Channel Name, Optical Configuration, Excitation Wavelength, Emission Wavelength, Modality, Magnification, Numerical Aparature, Immersion RI, Pinhole Size).   Measurement &gt; Field metadata &gt; Frame Center🔗 Returns the position of the frame center in micrometers.   Measurement &gt; Field metadata &gt; Frame Center Pixels🔗 Returns the position of the frame center in pixels.   Measurement &gt; Field metadata &gt; Frame Size🔗 Displays the frame size (“FrameSize.X” and “FrameSize.Y”) in [µm] of the image being processed.   Measurement &gt; Field metadata &gt; Frame Size Pixels🔗 Displays the frame size (“FrameSizePx.X” and “FrameSizePx.Y”) in pixels [px] of the image being processed.   Measurement &gt; Field metadata &gt; Max Allowed Value🔗 Displays the maximum allowed pixel value (“MaxAllowedValue”) for the image being processed.   Measurement &gt; Field metadata &gt; Min Allowed Value🔗 Displays the minimum allowed pixel value (“MinAllowedValue”) for the image being processed.   Measurement &gt; Field metadata &gt; Recorded Data🔗 Gives access to the metadata such as Exposure Time, Plate Name, Well Row, Slide Barcode, etc.   Measurement &gt; Field metadata &gt; Stage Position🔗 Displays information about the stage position (“StagePos.X”, “StagePor.Y”, “StagePos.Z”) during acquisition of the image being processed.   Measurement &gt; Field metadata &gt; Z Step🔗 Displays the Z Step size (“ZStep”) of the image being processed.   Measurement &gt; Object size &gt; Eq Diameter🔗 Calculates the Equivalent Diameter (“EqDiameter”) for each object in the connected binary result. It represents a circle with the same area as the measured object. For more information please see   .   Measurement &gt; Object size &gt; Fill Area🔗 Calculates the Fill Area (“FillArea”) of each object in the connected binary result. Fill area calculation ignores any holes in the objects. For more information please see   .   Measurement &gt; Object size &gt; Length🔗 Calculates the length for each object in the connected binary result. Length is a derived feature appropriate for elongated or thin structures. Since it is based on the rod model, it is useful for calculating length of medial axis of thin rods. For more information please see   .   Measurement &gt; Object size &gt; Line Length🔗 Calculates the line length (“LineLength”) of each object in the connected binary result. For more information please see   .   Measurement &gt; Object size &gt; Max Feret🔗 Calculates the maximal value of the set of Feret's diameters for each object in the connected binary result. For more information please see   .   Measurement &gt; Object size &gt; Max Feret 90🔗 Calculates the length projected across the MaxFeret diameter for each object in the connected binary result. For more information please see   .   Measurement &gt; Object size &gt; Min Feret🔗 Calculates the minimal value of the set of Feret's diameters for each object in the connected binary result. For more information please see   .   Measurement &gt; Object size &gt; Min Rect Area🔗 Calculates the minimal rectangular area for each object in the connected binary result.   Measurement &gt; Object size &gt; Object Pixel Count🔗 Counts the number of pixels in each binary object of the connected binary image.   Measurement &gt; Object size &gt; Object Area🔗 Calculates the area of each object (“ObjectArea”) under the connected binary result.   Measurement &gt; Object size &gt; Outer Perimeter🔗 Please see   .   Measurement &gt; Object size &gt; Perimeter🔗 Please see   .   Measurement &gt; Object size &gt; Perimeter Contour🔗 Please see   .   Measurement &gt; Object size &gt; Volume Eq Cylinder🔗 Please see   .   Measurement &gt; Object size &gt; Volume Eq Sphere🔗 Please see   .   Measurement &gt; Object size &gt; Width🔗 Please see   .   Measurement &gt; Object shape &gt; Circularity🔗 Please see   .   Measurement &gt; Object shape &gt; Convexity🔗 Please see   .   Measurement &gt; Object shape &gt; Elongation🔗 Please see   .   Measurement &gt; Object shape &gt; Fill Ratio🔗 Please see   .   Measurement &gt; Object shape &gt; Mean Chord🔗 Please see   .   Measurement &gt; Object shape &gt; Orientation🔗 Please see   .   Measurement &gt; Object shape &gt; Rectangularity🔗 Calculates the rectangularity of each object of the connected binary image.   Measurement &gt; Object shape &gt; Roughness🔗 Please see   .   Measurement &gt; Object shape &gt; Roughness Inf🔗 Please see   .   Measurement &gt; Object shape &gt; Shape Factor🔗 Please see   .   Measurement &gt; Object intensity &gt; Entropy🔗 Calculates entropy of the pixel values for every binary object.Please see   .   Measurement &gt; Object intensity &gt; Kurtosis🔗 Calculates sample kurtosis of the pixel values for every binary object.Please see   .   Measurement &gt; Object intensity &gt; Maximum🔗 Finds the maximum pixel value in every object.   Measurement &gt; Object intensity &gt; Mean🔗 Calculates the arithmetic mean of pixel values for every binary object.Please see   .   Measurement &gt; Object intensity &gt; Minimum🔗 Finds the minimum pixel value in every object.   Measurement &gt; Object intensity &gt; Mode🔗 Finds pixel value that appears most often in every object.Please see   .   Measurement &gt; Object intensity &gt; Quantile🔗 Calculates the n-th quantile of pixel values for every binary object.Please see   .   Measurement &gt; Object intensity &gt; Skewness🔗 Calculates the sample skewness of pixel values for every binary object.Please see   .   Measurement &gt; Object intensity &gt; Standard Deviation🔗 Calculates the sample standard deviation of color pixel values for every binary object.Please see   .   Measurement &gt; Object intensity &gt; Sum🔗 Calculates the sum of pixel values for every binary object.Please see   .   Measurement &gt; Object intensity &gt; Uniformity🔗 Calculates the uniformity of pixel values for every binary object.Please see   .   Measurement &gt; Object intensity &gt; Variance🔗 Calculates the sample variance of pixel values for every binary object.Please see   .   Measurement &gt; Object ratiometry &gt; Ratio🔗 Calculates the mean of ratios between corresponding pixels of the two input channels under the input binary mask.   Measurement &gt; Object ratiometry &gt; Pearson Coeff🔗 Calculates Pearson correlation coefficient for pixels from two inputs (B, C) under input binary mask. The coefficient ranges from -1 to 1. A value of 1 implies positive linear relationship for which C increases as B increases. A value of -1 implies negative linear relationship for which C decreases as B increases. A value of 0 implies that there is no linear correlation between B and C.See also      Measurement &gt; Object ratiometry &gt; Manders Coeff🔗 Calculates Manders overlap (MOC), Manders overlap coefficients (k1, k2) and Manders colocalization coefficients (M1, M2).See also   MOC MOC - ranges from 0 to 1.k1, k2 k1, k2 - range from 0 to oo. MOC^2 = k1 * k2M1, M2 range from 0 to 1, where 1 implies perfect colocalization. They are not sensitive to intensity of overlapping pixels, but are very sensitive to background. A proper background subtraction step is necessary before measuring these coefficients.M1 is a fraction of R in areas containing GM2 is a fraction of G in areas containing R   Measurement &gt; Object parenting &gt; Aggregate Children🔗 Produces a table where each row represents parent object containing ParentEntity, ParentId, parent features (taken as input table), ChildId, aggregated (specified in the GUI) child features (taken as input table). Parent[Entity, Id] and Child[Entity, Id] are replaced with the name of the Binary.Type Child is inside parent couples each child to the parent object which completely covers it (child is completely inside its parent). Figure&nbsp;887.&nbsp;Ignored objects are crossed out in red.Child is intersecting parent couples each child to the parent object which completely covers it or intersects with it. If a child intersects more than one parent, it belongs to the parent with which it has a bigger intersection. Figure&nbsp;888.&nbsp;Ignored objects are crossed out in red. Arrow indicates to which parent the child belongs.Child's nearest parent couples each child to its nearest parent object. If a child lies on the border between two parent areas, it belongs to the parent with which area it has a bigger intersection. Figure&nbsp;889.&nbsp;Virtual borders (red dashed line) dividing the parent-child areas. Arrow indicates to which parent the child belongs. Size of the child intersection with the parent area determines to which parent it belongs.   Measurement &gt; Object parenting &gt; Children🔗 Produces a table where rows contain children (and their features) with their relevant parents (and their features) and a distance. Specify whether the objects are inside or neighboring the parent (Type) and set their Reference (see   ).Type This node uses the same settings for Type already described in   .Reference Parent center to child center calculates distance between the parent center and the child center. Figure&nbsp;890.&nbsp;Parent center to child border calculates distance between the parent center and closest point on the child border. Figure&nbsp;891.&nbsp;Parent border to child center calculates distance between the child center and the closest point on parent border. Figure&nbsp;892.&nbsp;Parent border to child border calculates distance between the parent border and the child border (minimal distance between borders). When the child touches the parent (middle case), the distance is 0. Figure&nbsp;893.&nbsp;   Measurement &gt; Object parenting &gt; Child ID🔗 The child relationship with its parent is detected using the Child ID  action. It adds a “ChildId” feature column (highlighted green) and maps the “ParentId” to “ObjectId” identification column (highlighted red). Figure&nbsp;894.&nbsp; Example&nbsp;13.&nbsp;Simple counting of spots (child) inside cells (parent)The Child ID action gives the result directly in a natural column order. Figure&nbsp;895.&nbsp;Statistics of spot features per cell can be done by adding “SpotFeatures” to “Spots” (from “ParentId”) as they have the same number of rows (all spots) followed by “GroupRecords” and “AggregateRows”. Figure&nbsp;896.&nbsp;ConditionType This node uses the same settings for Type already described in   .   Measurement &gt; Object parenting &gt; Child Distance🔗 Calculates distance between parent and child objects. Every parent object can have zero or multiple childs.Type This node uses the same settings for Type already described in   .Reference This node uses the same settings for Reference already described in   .   Measurement &gt; Object parenting &gt; Nearest Child🔗 Produces a table where rows contain parents (and their features) with their relevant nearest children (and their features). Specify whether the objects are inside or neighboring the parent (Type) and set their Reference (see   ).Type This node uses the same settings for Type already described in   .Reference This node uses the same settings for Reference already described in   .   Measurement &gt; Object parenting &gt; Object ID🔗 Creates a table of object IDs.   Measurement &gt; Object parenting &gt; Parent ID🔗 Relationship of the parent with its child is detected using the Parent ID  action. It adds a “ParentId” feature column (highlighted green) and maps the “ChildId” to “ObjectId” identification column (highlighted red). Figure&nbsp;897.&nbsp;ConditionType This node uses the same settings for Type already described in   .   Measurement &gt; Object parenting &gt; Parent Distance🔗 Calculates distance between child and parent objects. Every child object can have only one or zero parents.Type This node uses the same settings for Type already described in   .Reference This node uses the same settings for Reference already described in   .   Measurement &gt; Object position &gt; Center🔗 Shows the X and Y distance from the top left corner to the center of gravity of each object.   Measurement &gt; Object position &gt; CenterPx🔗 Shows the X and Y distance in pixels from the top left corner to the center of gravity of each object.   Measurement &gt; Object position &gt; CenterAbs🔗 Shows absolute coordinates of the center of gravity of each object in the scope of the stage XY range.   Measurement &gt; Object position &gt; Centroid🔗 Shows the X and Y coordinate of the object's centroid.   Measurement &gt; Object position &gt; CentroidPx🔗 Shows the X and Y coordinate of the object's centroid in pixels.   Measurement &gt; Object position &gt; CentroidAbs🔗 Shows absolute coordinates of the centroid of each object in the scope of the stage XY range.   Measurement &gt; Object position &gt; Geodesic Center🔗 Creates a table with calibrated values of the geodesic centers for all binary objects present in the connected layer. See also   .   Measurement &gt; Object position &gt; Geodesic CenterPx🔗 Creates a table with pixel values of the geodesic centers for all binary objects present in the connected layer. See also   .   Measurement &gt; Object position &gt; Start🔗 Please see   .   Measurement &gt; Object position &gt; StartPx🔗 Shows the start coordinates of each object in pixels. Please see   .   Measurement &gt; Object position &gt; Bounds🔗 Shows the bounds of each object. Please see   .   Measurement &gt; Object position &gt; BoundsPx🔗 Shows the bounds of each object in pixels. Please see   .   Measurement &gt; Object position &gt; BoundsAbs🔗 Shows the absolute bounds of each object. Please see   .   3D Segmentation &gt; Threshold &gt; Threshold🔗 This node thresholds 3D objects. Figure&nbsp;898.&nbsp; Set to current threshold range Zooms to the current range. Set to actual (min, max) range Zooms to the minimum - maximum range. Reset to full range Zooms to the full domain.Smooth, Clean, Fill Holes, Separate These binary processing functions can be activated and their value adjusted using the arrows or directly by typing the value into the edit box. Fill Holes processing can only be turned on/off whereas the values of other processings are applied as a radius in µm or px.For more information about RGB thresholding please see  .   3D Segmentation &gt; Threshold &gt; Simple Threshold🔗 This node quickly thresholds 3D objects simply by setting the low and high threshold limit.For more information about RGB thresholding please see  .   3D Segmentation &gt; Spot detection &gt; Bright Spots🔗 This command detects bright circular objects within a 3D space. Please see    for further description. General analysis 3 spot detection uses the Different Sizes object detection method.   3D Segmentation &gt; Spot detection &gt; Dark Spots🔗 This command detects dark circular objects within a 3D space. Please see    for further description. General analysis 3 spot detection uses the Different Sizes object detection method.   3D Binary processing &gt; Basic &gt; Clean🔗 Removes small objects from 3D binary image. See also   . Figure&nbsp;899.&nbsp;   3D Binary processing &gt; Basic &gt; Invert🔗 Inverts the 3D binary image to its negative.   3D Binary processing &gt; Basic &gt; Smooth🔗 Smooths binary image contours in 3D. See also   .Smooths every binary voxel using sphere of the specified radius.  Figure&nbsp;900.&nbsp;   3D Binary processing &gt; Basic &gt; Separate Objects🔗 Separates binary objects into multiple smaller objects. The higher Count, the fewer objects will separate.  Figure&nbsp;901.&nbsp;   3D Binary processing &gt; Basic &gt; Fill Holes🔗 Fills holes inside 3D binary objects present in the current volume. Holes touching image borders are not filled.   3D Binary processing &gt; Morphology &gt; Open🔗 Performs morphological opening on 3D binary image. See also   . Figure&nbsp;902.&nbsp;   3D Binary processing &gt; Morphology &gt; Close🔗 Performs morphological closing on 3D binary image. See also   . Figure&nbsp;903.&nbsp;   3D Binary processing &gt; Morphology &gt; Erode🔗 Performs morphological erosion on 3D binary image. See also   . Figure&nbsp;904.&nbsp;   3D Binary processing &gt; Morphology &gt; Dilate🔗 Performs morphological dilation on 3D binary image. See also   . Figure&nbsp;905.&nbsp;   3D Binary processing &gt; Morphology &gt; Close Holes🔗 Removes small holes from 3D binary image. See also   . Figure&nbsp;906.&nbsp;   3D Binary processing &gt; Circular Morphology &gt; Circular Open🔗  Figure&nbsp;907.&nbsp;See Also&nbsp;     3D Binary processing &gt; Circular Morphology &gt; Circular Close🔗  Figure&nbsp;908.&nbsp;See Also&nbsp;     3D Binary processing &gt; Circular Morphology &gt; Circular Erode🔗 Radius Figure&nbsp;909.&nbsp;See Also&nbsp;     3D Binary processing &gt; Circular Morphology &gt; Circular Dilate🔗 Radius Figure&nbsp;910.&nbsp;See Also&nbsp;     3D Binary processing &gt; Detect &gt; Centers🔗 Calculates and marks the centroid of the 3D objects found in the connected binary image.   3D Binary processing &gt; Detect &gt; Centroids🔗 This command calculates and marks the centroid on the 3D binary image. If bright/dark (Signal is) areas dominate aside of the centroid position calculated from just the binary layer, the centroid will be shifted in that direction. Figure&nbsp;911.&nbsp;   3D Binary processing &gt; Detect &gt; Distance Function🔗 For each binary voxel, this function computes its euclidean distance from the nearest voxel of background and displays it as intensity. Output is a floating-point image.   3D Binary processing &gt; Detect &gt; Convex Hull🔗 This function expands non-convex 3D binary image objects to their convex boundaries. Figure&nbsp;912.&nbsp;Example   3D Binary processing &gt; Detect &gt; Medial Axis🔗 Creates medial axis from the connected 3D binary objects.   3D Binary processing &gt; Region growing &gt; Watershed🔗 Performs the watershed (flooding) algorithm starting from the connected 3D binary objects. The flooding is based on image pixel intensities.Type Defines the direction of flooding - from high intensities (From Bright Regions ) or from low intensities (From Dark Regions ).   3D Binary processing &gt; Region growing &gt; Grow Objects🔗    3D Binary processing &gt; Region growing &gt; Grow Regions🔗 Grows each binary object up to the specified intensity. Objects will not be merged together.Type Grow Bright Regions will grow from maximum intensity to the specified intensity.Grow Dark Regions will grow from minimum intensity to specified intensity.   3D Binary processing &gt; Skeleton &gt; Detect Branching🔗 Creates 1pixel seeds out of a skeletonized 3D binary image. This function serves for automatic recognition of the intersection points of single-pixel lines.   3D Binary processing &gt; Skeleton &gt; Detect Endings🔗 Creates 1pixel seeds out of a skeletonized 3D binary image. It preserves only ending points of the skeleton and clears all other pixels.   3D Binary processing &gt; Filter Objects &gt; Select Objects🔗 Selects binary objects from the binary image which are present in input table.   3D Binary processing &gt; Filter Objects &gt; Delete Objects🔗 Deletes binary objects from the binary image which are present in input table.   3D Binary processing &gt; Remove Objects &gt; Touching Borders🔗 Removes binary objects touching the volume border.   3D Binary processing &gt; Remove Objects &gt; Touching Frame🔗 Removes binary objects touching the volume frame. Select Units and the limit values of the frame. Then choose a Mode defining which objects are to be removed.   3D Binary processing &gt; Colors &amp; Numbers &gt; Color by Id🔗 Please see   .   3D Binary processing &gt; Colors &amp; Numbers &gt; Color by Value🔗 Assigns any color to any value of the selected source column used from the connected table.See also:   .   3D Binary processing &gt; Colors &amp; Numbers &gt; Renumber Objects🔗 Please see   .   3D Binary processing &gt; Colors &amp; Numbers &gt; Renumber Using Table🔗 Renumbers the binary objects with values taken from the the connected table. Source column has to be specified.Object Id Column This drop-down menu allows the user to select the column that represents the Object ID.   3D Binary operations &gt; Binary x Binary &gt; And🔗 Please see   .   3D Binary operations &gt; Binary x Binary &gt; Or🔗 Please see   .   3D Binary operations &gt; Binary x Binary &gt; Xor🔗 Please see   .   3D Binary operations &gt; Binary x Binary &gt; Subtract🔗 Please see   .   3D Binary operations &gt; Binary x Binary &gt; Equivalence🔗 Please see   .   3D Binary operations &gt; Binary x Binary &gt; Having🔗 Please see   .   3D Binary operations &gt; Binary x Binary &gt; Not Having🔗 Please see   .   3D Binary operations &gt; Single Binary &gt; Invert🔗 Please see   .   Tracking &gt; 3D Object Position &gt; Time &amp; Center🔗 Shows the time and center of the 3D object for tracking.    Tracking &gt; 3D Object Position &gt; Time &amp; CenterAbs🔗 Shows the time and center of the 3D object for tracking in absolute (stage) coordinates.    Tracking &gt; 3D Object Position &gt; Time &amp; Centroid🔗 Shows the time and the center of gravity of the 3D object for tracking.    Tracking &gt; 3D Object Position &gt; Time &amp; CentroidAbs🔗 Shows the time and the center of gravity of the 3D object for tracking in absolute (stage) coordinates.    Tracking &gt; Tracks &gt; Accumulate Tracks🔗 Creates a table with all frames and groups them by the track. Enables filtering based on the number of segments (Min segment count).   Tracking &gt; Tracks &gt; Mean Sq. Displacement🔗 Creates a table with points of MSD ( ) curve. Select the Track ID, Time and Position columns. Before analyzing using this node use Accumulate Tracks (  ) to group by “Track ID”.MSD Curve is generated for each group separately. If you want to have one curve for each track, group the table by Track ID. If you want one curve in total, ungroup the table.   Tracking &gt; Tracks &gt; Sperm Motility🔗 Return CASA motility features. VAP Averaging Specifies the average path velocity parameter.   Tracking &gt; Tracking Features &gt; Tracking Features &gt; Feature Derivative🔗 Calculates derivation of the value in time ((Fi - Fi-1) / (dT)), while requiring Diff Time.Please see  .   Results &gt; Graphs &gt; Barchart🔗 This graph is used for visualizing Y of ordinal X. Settings of each tab are described below.GeneralTitle Sets the title of the graph shown in the top.Inside Sets the inside color of the graph (color behind the visualized data).Outside Sets the outside color of the graph (frame around the visualized data).Default Dark Returns the colors to the default dark scheme.Axes Sets the color of the axes.Text Sets the color of the texts.Default Light Returns the colors to the default light scheme.Series Sets the color scheme for the grouped data.Display Values Shows data values directly in the graph.Hidden Hides the legend.Inside Graph Shows the legend inside the graph. Choose the legend position in the drop-down menu next to this option.Below Graph Shows the legend below the graph.Background Sets the background color of the legend.Text Sets the color of the legend text.DataAll Columns This tab is used to pair the graph axes with the variables available in the table input. Select a data variable and click on the arrow  next to the selected axis to assign it to this axis. Multiple variables can be assigned to the Y axes.   button opens the Data Series dialog setting up the data series graph properties such as the color, line type, stroke type, line width, fill, marker type, and values type. Only the checked properties are visualized in the graph.   button opens the Error Bar dialog setting up the error bar properties for the selected column. Choose an error column from the drop-down menu and optionally set its color, line width, and whisker width.  buttons move the variable in the axis list up/down. button moves the variable from the axis list back to the All Columns list.X Axis/Y Axis/Left Y Axis/Right Y Axis/Color Axis/Size Axis/Category AxisTitle Sets the title of the axis that is currently being set.Reverse Range Reverses the range of the category axis.Visible Shows/hides the labels for the axis currently being set.Format Specifies the axis label format for the axis currently being set.Precision Specifies the axis label numeral precision.Step Specifies the step size of the category axis.Minimum In Auto mode, the minimal value for the range is set automatically. If switched to Fixed, a minimal value from which the data are visualized can be entered into the edit box. Scale specifies the scale type - linear or logarithmic.Maximum In Auto mode, the maximal value for the range is set automatically. If switched to Fixed, a maximal value to which the data are visualized can be entered into the edit box. Reversed Range reverses the values range from “min - max” to “max -min”.Major Step In Auto mode, the major step value for the ticks and gridlines is set automatically. If switched to Fixed, a major step value used for the ticks and gridlines visualization can be entered into the edit box. If Tick is checked, a short tick is added to the axis next to each major step value. Grid color specifies the color of the grid line.Minor Step In Auto mode, the minor step value for the ticks and gridlines is set automatically. If switched to Fixed, a minor step value used for the ticks and gridlines visualization can be entered into the edit box. If Tick is checked, a short tick is added to the axis next to each minor step value. Grid color specifies the color of the grid line.   Results &gt; Graphs &gt; Colormap🔗 Provides a graphical representation of data where the individual values contained in a two-dimensional matrix are represented as colors from selected color gradient map. This graph does not support grouped source table A.All tabs are closely described in the    node.Typical UsecasesHeatmap of a property related to grid type patterns (wellplate) where x, y axes are mapped into columns and rows.Tracking results with fixed number of tracks and time points can color code speed or other track properties at given time.   Results &gt; Graphs &gt; Linechart🔗 This graph is used for visualizing Y as function of X.All tabs are closely described in the    node.   Results &gt; Graphs &gt; Scatterplot🔗 This graph is used for visualizing a cloud of points. If the source table A is grouped the graph shows them in colors.All tabs are closely described in the    node.   Results &gt; Graphs &gt; Scatterplot XY Color🔗 This 3D graph is used for visualizing a cloud of points. The third axis is visualized using a color gradient. This graph does not support grouped source table A.All tabs are closely described in the    node.   Results &gt; Graphs &gt; Scatterplot XY Size🔗 This 3D graph is used for visualizing a cloud of points. The third axis is visualized using disks of different size. This graph does not support grouped source table A.All tabs are closely described in the    node.   Results &gt; Graphs &gt; Statistical Box🔗 Depicts groups of numerical data through their quartiles as a  . It shows minimum, first quartile, mean, third quartile and maximum. It can optionally show outliers. If the source table A is grouped the graph shows one box for each group.All tabs are closely described in the    node.   Results &gt; Report &gt; Report🔗 Designs a template and populates it with images, tables and graphs.   Results &gt; Report &gt; Html Document🔗 Creates an HTML document with a table input. After setting the parameters and creating the content, click Apply to add the document as a tab to the result table. From there it can be printed (Print), exported to PDF (Print to PDF) or saved as HTML (Save As HTML).Code Format Selected coding format.Table Inputs Number of connected table inputs.Code HTML script area.The default state serves as an example of formatting the content in Markdown and referencing input tables or their cells.Code example:{% raw %}# Markdown Overview\n\nBasic text formatting: *emphasized*, **bold text**, ~strikeout text~.\n\nFor details see [Markdown basic syntax](https://www.markdownguide.org/basic-syntax/).\n\nParagraphs are delimited by *empty* line.\nLinebreak are ignored unless there are more spacfes at the end of a line. \nLike **here**   \nwhere the newline is forced.\n\nHTML code can be used as well:\nA&lt;sup&gt;2&lt;/sup&gt; + B&lt;sup&gt;2&lt;/sup&gt; = C&lt;sup&gt;2&lt;/sup&gt;\n\n## GA3 table Inputs\n\nenclose the parameter name into `&amp;` and `;` (`code` is an inline code block)\n\n&amp;T;\n\nor index a specific value (indexing is zero based, first is column second is row):\n&amp;T[0,1];\n\n## JavaScript block\n\nJavaScript block may be embedded inside `${` and `}$` with a return statement. During\nrun whole block is replaced by the returned value. Tables are made available as t, t1, t2, ...\n\n${ return \"T[0, 1]&lt;sup&gt;2&lt;/sup&gt;: \" + Math.pow(t[0][1], 2); }$\n\n## Markdown supports lists\n\n### Ordered lists\n\n1. First item  \n2. Second item  \n3. Third item  \n\n### Unordered lists\n\n- unordered item\n- another one\n- third one\n- ...\n\n### Nested lists\n\n- unordered item\n   - first (starts with three spaces at least)\n   - second\n- another one\n   - first\n   - second\n\n## Quoted blocks\n\n&gt; First line\nfirst line continues (more spaces follow)  \nanother line of quote\n\n## Tables\n\nAfter the header at least three dashes per column are required.\n\n| Value | Name   |\n| ----- | ------ |\n| 1     | one    |\n| 2     | two    |\n| 3     | three  |\n\nFor details see [Markdown extended syntax](https://www.markdownguide.org/extended-syntax/).\n\n## Color chips\n\nRGB colors: `#FF0000``#00FF00``#0000FF`{% endraw %}Style CSS style area.Load Default Style Reverts any style changes.Apply Applies the settings and creates a new tab in the results table. ",
     id: 150 }, 
   { title: "Introduction to HCA",
     xmlid: "id|p2c12s1x",
     content: "  The HCA (High-content analysis) module enables the user to perform full scale HCA experiments by combining the capturing and the analysis phases to a single “HCA job”. There are job templates designed to analyze live or fixed samples with well-plate loader support. Images and results are organized in a database which contains just links to image files placed in a separate folder. Before you start, you need to open an existing or create a new job database (see  ).  Figure&nbsp;913.&nbsp;Jobs Explorer with the HCA moduleJobs Explorer is the heart of the    and    modules. Run the    to display it.See   for more information. Figure&nbsp;914.&nbsp;Jobs Toolbar with HCA templatesJobs Toolbar is used as a container holding single jobs and job templates. It is designed for fast running and editing.See  . ",
     id: 151 }, 
   { title: "HDR Options",
     xmlid: "id|p2c13s1x",
     content: " The following ways to create a HDR image are available:Automatic capture Call the    command and the HDR image will be captured using computer-estimated settingsCapture When the    command is called, you will be asked to adjust the HDR acquisition settings such as number of exposures, exposure time range, camera settings, etc.Create from files If a sequence of images which were captured using different exposure times is available, the    command can load the images and merge them into a HDR image.Create from an ND file Similarly to the situation when the HDR file is created from a sequence of images, it can be as well created from the frames of an ND2 file. Just run the    command. Figure&nbsp;918.&nbsp;A shining bulb captured with HDR ",
     id: 152 }, 
   { title: "HDR module background",
     xmlid: "id|p2c13s2x",
     content: " The HDR module is based on the image fusion technique which considers the edge response, the “well exposed” measure and the “well saturated” measure. The fusion works locally. That means the measures of quality are computed for every single pixel from its neighborhood over the whole picture stack. If we simplify the fusion process, we can say that pixels with the same coordinates are compared throughout the stack of images and the one with the highest quality measure is chosen to be used in the resulting image. ",
     id: 153 }, 
   { title: "User Interface",
     xmlid: "id|p2c14s1x",
     content: "  Figure&nbsp;919.&nbsp; Camera Settings This row shows parameters influencing the timing of the camera which is used for capturing. Use the Assign Current button to assign current camera settings to be used for capturing. In case of multi-camera mode, one camera will be used for IS, defined in a combo box, where the user selects the active camera. The camera I/O signal is the source for the IS timing. In case of dual camera drivers (dual Andor, dual Flash 4.0, dual Neo/Zyla) the timing of the master is shown. List of supported cameras can be found here:  .Image Channels This area aggregates all channels necessary for your experiment. All defined channels will be contained in the resulting ND2 file. Start by clicking on the   button, enter the Name of your channel, enter its wavelength (EM) and choose its Color from the combo box. The real color of the wavelength is offered first, followed by pseudo colors. Click Done and add more channels if necessary or edit the current ones by clicking ... next to their name. EM value defined in IS channel is written in the metadata of the resulting ND2 file inside the Acquisition details pane (see:   ). Channel color defined in the IS will be used as a pseudo color for a given channel in the ND2 document.Merge Frames This option is available only if more than one channel is defined. A number of channels in the target ND2 document will correspond to the number of channels defined in the Image Channels section.If group frames to channel is not selected, for each index of the camera frame with an imaging channel, a document index will be created with the number of planes corresponding to the number of channels.Overall, three options for merging frames are available. Fill empty channel with black image option captures an image only over frames where each channel is placed. Frames where the particular channel is missing are filled with black. Fill empty channel with the last image option uses the last frame of the particular channel and copies it to frames where the channel is missing till a new frame of the channel is updated. Group frames to channel option waits until one frame of each channel is acquired. Then a merged image containing all defined channels is created.Illumination Devices Only NIDAQ based devices which enable TTL Triggering can be used. Each laser line of the used illumination device is represented by a draggable color rectangle. The last “Dummy” device is not a real illumination device but can be used to create a delay after a phase. List of supported illumination devices can be found here:  .DMD Devices To use DMD with IS, you have to configure its logical devices inside the device manager (  ) using Configure.... Switch the HW Interface to USB/PCIe + Ext Trig in the DMD Configuration dialog.If Keep Level stimulation type is used with DMD, Stimulation patterns (Calibration Grid, Full On, Full Off, Custom) can be selected from the Pattern combo box available after double-clicking on the element. Assign Current Value uses the currently selected ROI as the stimulation pattern.If Use Pattern stimulation type is used, Stimulation patterns can be selected from the Pattern combo box at the top of the dialog window displayed after double-clicking on the element. Single Pulsed illumination (Follow Exposure illumination mode) and time and cycle duration (Continuous illumination mode) are supported. Offset, pulsed illumination using a time period or computed period filling selected time cannot be used together with DMD (Custom illumination mode).Check Advanced in the Pattern Properties and click Define... to set an advanced pattern. Figure&nbsp;920.&nbsp;Advanced Pattern Settings dialog window Use patterns fromDevice Uses a selected pattern taken from the Pattern Manager.Rectangular Grid Generates a grid on the live image based on the specified rows/columns. OrderingSequential Stimulates through the pattern/grid in a sequential order.Random Stimulates through the pattern/grid in a random order.Pattern count Number of patterns/grid areas which will be illuminated.Illumination sequence supports only the 1-bit level grayscaling mode (mirror grayscaling button on the DMD Pad needs to be unselected).Other Devices Other NIDAQ devices supporting IS include: Analog Output, Calibrated Analog Output, Filter Changer, Piezo Z, Shutter, Switcher and TTL Output. Only NIDAQ cards which support TTL Output triggering can be used. Analog Outputs of internal cards are always triggerable.Phase Options Drag and drop the Phase rectangle into the sequence area below to add a new phase.  Undo/Redo These buttons implement a standard undo/redo functionality.Phase A phase of any sequence consists of rectangular cells. Each cell has a length of the camera exposure. To create a sequence, fill the cells with particular elements (Channels, Illumination devices, Other Devices) by dragging&amp;dropping their element (rectangular) into the corresponding row of the Phase table.Each row of the Phase table belongs to the corresponding device displayed on the left. By default, two rows are available (Image Channels and Illumination), however more rows are created if Other Devices are dragged&amp;dropped into the table area.Next to the Phase caption name you can define the number of loops which will be performed or a time duration. If Repeat from frame is used, the loop is run from the defined frame on every phase repeat (excluding the first one which starts always from the beginning). Phase length is determined by its first and last element.Phases can be organized inside the sequence by dragging&amp;dropping their caption vertically. Phases can be deleted by dragging&amp;dropping their caption outside the dialog window. They can also be cleared, duplicated and deleted on the context menu. To change the position and order of any Phase, simply drag-drop the elements in its table, use Shift for multi selection and Ctrl for copying the selection into different cells.Image Channels (Camera) row Contains all dragged&amp;dropped Image Channels. On a double-click, or context menu over a channel placed inside the table, there is an option to enter the amount of frames over which the channel will be captured. In case of dual view or dual camera setup, two rows of image channels named Camera #1 and Camera #2 are available.Illumination row The illumination row contains all dragged&amp;dropped illumination devices. The top of each cell shows the illumination wavelength and its power. The bottom half of the element shows the illumination pattern. To edit the illumination pattern, double click the element in the phase table or right-click on it and choose Edit (see:  ). Illumination power can also be changed during runtime by clicking on the illumination element. Figure&nbsp;921.&nbsp;Devices row If a device other than Illumination or Channel is dragged&amp;dropped inside the Phase table, a new row aggregating this type of device is created. To edit this item, simply double-click on it or right-click and choose Edit.Capture Storm Image If the N-STORM device is connected, use this option to capture a STORM image. If you are using a device with a cylindrical lens and want to detect the Z dimension of the molecules, switch to 3D STORM. In other cases use 2D STORM. After defining your illumination sequence, evaluate whether it is Sequential (excitation, then acquisition) or Continuous (excitation and acquisition at the same time). Real Time STORM Analysis determines whether the molecules are detected during the acquisition. If so, the result contains image data and molecules. Perform Drift Correction detects any sample drift made during the image acquisition. If the system finds a drift, turn on the   Drift Correction inside the Molecules Options  ( ) dialog window to compensate for the drift.Load Microscope Setting from OC on Start This combo box loads the microscope settings of Other Devices from your Optical Configuration. The setting will then be used as a starting state before executing the experiment.Save to File Specifies the folder where your new ND2 file will be saved.Filename Enter a name of the ND2 file which will be created.Configurations Click Save As... to save the current IS settings. Enter a name and click OK. A new button is created. If multiple settings are created, you can easily switch between them by clicking their buttons. To delete a setting, simply drag-drop its button outside the IS dialog window or use the context menu over the button to delete or overwrite its configuration.Oscilloscope View... Shows a diagram view of the current IS which will be created. Phase buttons jump to the beginning of the chosen phase. Refresh button updates the oscilloscope view if any changes were made to the sequence.Run Now Starts the currently defined experiment. ",
     id: 154 }, 
   { title: "Creating Piezo Z Stack and Z Offset patterns",
     xmlid: "id|p2c14s3x",
     content: "    ",
     id: 155 }, 
   { title: "Piezo Z Stack",
     xmlid: "id|p2c14s3s1x",
     content: " Dropping the Piezo Z Stack into the sequence table opens the Z Stack definition window. Figure&nbsp;926.&nbsp;Piezo Z StackDefine the Range, Step size or number of steps and the length over which the Z stack is performed (Step Length). Choose the Z direction (Bidirectional acquisition moves the stage in both directions from the sample) and click OK. Check the Oscilloscope View to see how the Z Stack will look like. To create an ND2 file containing separate t and Z dimensions, check Create Z Stack Document in the Illumination Sequence main window. In order to create a valid Z Stack document, the number of frames has to be a multiple of the number of Z steps.For IS examples, please see  . ",
     id: 156 }, 
   { title: "Piezo Z Offset",
     xmlid: "id|p2c14s3s2x",
     content: " Piezo Z Offset defines an additional Z offset. Its exact value can be specified in the first edit box or Random position  can be used to move the stage randomly from the defined center position in a specified range. Figure&nbsp;927.&nbsp;Piezo Z OffsetFor IS examples, please see  . ",
     id: 157 }, 
   { title: "Creating stimulation patterns for Other Devices",
     xmlid: "id|p2c14s4x",
     content: " If devices different than Illumination and Piezo Z are dragged&amp;dropped into the sequence table a dialog window pops-up. The device behavior has to be selected. Two options are offered:Keep Level This type of element determines at which point in time a voltage change is made. E.g. all values for the TTL Output are low (by default) until a frame containing this type of element is executed and changes the value from low to high to all frames hereafter. Double click the element and edit its behavior.For Focus Offset a dialog window is opened automatically. Enter the Focus Offset value and click OK.For (Calibrated) Analog Output set a Value [V] or choose a Function. For the functions Sinus, Cosinus, Saw and Rectangle, set the Polarity of the function, set the Synchronization (Experiment Begin = beginning phase of the signal is at the beginning of the experiment). Then set the Amplitude, Period and Offset (from 0 V). General function is non-periodical and enables the user to enter the switching points manually. Use the edit box to write down each Time [ms] and Value [V] change point in the following notation: {% raw %}time1,voltage1 time2,voltage2 time3,voltage3{% endraw %} etc. To load a predefined set from a *.txt file use Browse.... Points in the *.txt file can be separated either by a space or enter character. Before the beginning of each exposure phase, there is a period of about 20 µs (depending on the camera accuracy) during which it is not possible to change the voltage. This stretch of time is highlighted by a red line showing the user the real progression of the function being set. Figure&nbsp;928.&nbsp;For TTL Output define its State (Low/High) or Assign the Current Value and select when will the Change be made (Exposure Start, Exposure End or Dead Time Start).Shutter, Switcher and Filter wheel use this type of logic by default.Use Pattern This type of element changes the value only over frames where it is present. Fields without this element use the default value taken from the Optical Configuration. Double click the element and edit its behavior.For Focus Offset a dialog window is opened automatically. Enter the Focus Offset value and click OK.For (Calibrated) Analog Output set the Power or Assign the Current Value, choose over how many frames it will be Repeated, select when will the change Start At and Stop At (Exposure Start/End or Dead Time Start/End or Next Exposure Start) and define the Offset if necessary. Pulsed illumination can also be set (see:  ).Settings for TTL Output is the same as for (Calibrated) Analog Output (see above) except choosing the State (Low/High) in the first combo box. Default state of each device is determined by its Optical Configuration (  ).Previewing the illumination sequence Live view does not follow the IS setup, so previewing should be done via Run Now/Finish.Changing the power of laser lines on the fly Even after pressing Run now you can change the power of any laser line by clicking on the corresponding cell and changing its power. The new value will propagate to the NI card waveform and will be applied.For IS examples, please see  . ",
     id: 158 }, 
   { title: "Camera Timing Examples",
     xmlid: "id|p2c14s5x",
     content: "  Figure&nbsp;929.&nbsp;  Figure&nbsp;930.&nbsp;  Figure&nbsp;931.&nbsp;  Figure&nbsp;932.&nbsp;  Figure&nbsp;933.&nbsp;  Figure&nbsp;934.&nbsp;  Figure&nbsp;935.&nbsp;For IS examples, please see  . ",
     id: 159 }, 
   { title: "Supported cameras and illumination devices",
     xmlid: "id|p2c14s7x",
     content: "    ",
     id: 160 }, 
   { title: "Use Cases",
     xmlid: "id|p2c15s10x",
     content: "        ",
     id: 161 }, 
   { title: "Analysis Use Cases",
     xmlid: "id|p2c15s10s1x",
     content: "     ",
     id: 162 }, 
   { title: "Merging Different Acquisition Types",
     xmlid: "id|p2c15s4x",
     content: "     From version 5.0, different acquisition types use a unified storage system which enables the user to nest different acquisition tasks while the result is a single ND2 data set. See the examples below. The following tasks are concerned:             ",
     id: 163 }, 
   { title: "Simple Acquisition Tasks Inside a Parent Loop",
     xmlid: "id|p2c15s4s1x",
     content: "    inside   . Figure&nbsp;977.&nbsp;Result: TZ file   inside   : Figure&nbsp;978.&nbsp;Result: MT file   inside   : Figure&nbsp;979.&nbsp;Result: MT file   inside   : Figure&nbsp;980.&nbsp;Result: MT file   inside   : Figure&nbsp;981.&nbsp;Result: T file ",
     id: 164 }, 
   { title: "Merging Matching Z-Stacks",
     xmlid: "id|p2c15s4s2x",
     content: "    inside    inside   .Result: single TMPZ fileThe same Z-stack is captured for each Lambda channel Figure&nbsp;982.&nbsp;TMPZ Experiment Scheme Figure&nbsp;983.&nbsp; Instead of the    task, you could also use the    task containing a (multichannel)   , however, the arrangement used in the example eliminates unwanted delays which occur when the filter-changer is slower than the Z drive (e.g. if a piezo Z is engaged). ",
     id: 165 }, 
   { title: "Multi-point inside Multi-Point Loop",
     xmlid: "id|p2c15s4s4x",
     content: "    (XY) inside   .All XY positions (points) are merged together.Result: single MP file Figure&nbsp;986.&nbsp; Figure&nbsp;987.&nbsp; ",
     id: 166 }, 
   { title: "GA3 After Acquisition",
     xmlid: "id|p2c15s6x",
     content: " (requires:  ) Can be launched via the context menu in JOBs Explorer on a single jobrun, jobrun selection, single job or job selection.Can be launched from the jobrun results window  ( ). Figure&nbsp;1005.&nbsp;Run General Analysis 3 button in Job Results.. Figure&nbsp;1006.&nbsp;GA3 dialog window.GA3 recipe can be created and edited directly from the dialog window.Captured samples associated to the jobrun can be opened.Preview can be processed, it can be locked to the selected image or applied to the every active image.User can choose to store analysis results to a new jobrun or to add resulting binaries and tables to the original one (only when frame dimensions are preserved).Wizard steps can be integrated into the dialog window.Result tables are stored in the same way as for the    task in JOBS.Example&nbsp;The following example shows segmentation and tracking of bull sperm samples in order to analyze the sperm motility. Figure&nbsp;1007.&nbsp;GA3 definition. Figure&nbsp;1008.&nbsp;GA3 on Jobrun definition. Figure&nbsp;1009.&nbsp;Tracking results. Figure&nbsp;1010.&nbsp;Tracking results. ",
     id: 167 }, 
   { title: "Overview",
     xmlid: "id|p2c15s9s3s1x",
     content: "  Figure&nbsp;1032.&nbsp;Main windowThe main window contains three basic steps (starting from the top):Source files definition (Path, Filter and Prefix).Loops and metadata definition.File/Frame order definition (list of individual files/frames with associated position in the experiment).At the bottom of the window, the status of the definition is displayed. It is not possible to proceed until all the necessary information is provided.Typically, the user starts by defining the source files and then adds the loops and assigns indexes until all files/frames are assigned an unambiguous position in the experiment. ",
     id: 168 }, 
   { title: "Source Files",
     xmlid: "id|p2c15s9s3s2x",
     content: " After the Advanced Import Files into Current Project window is shown and then after every change of the relevant parameters (Path, Filter and Prefix) the source path is searched on the background and matching files are added to the list. TIFF is the only possible image format for import so far. ",
     id: 169 }, 
   { title: "Loops and Indexes",
     xmlid: "id|p2c15s9s3s3x",
     content: "   Loops and their order is the definition of the JOBS experiment. A loop is a specific repetition of actions.For example a simple time-lapse is a repetition of captures with a given count and time delay. Instead of capture, there may be a Z-stack, which is also a loop (involves Z-movement instead of a delay) or the whole experiment (tree of loops).Loops in JOBS are organized in a tree structure:A loop can be either nested inside another loop (parent loop) orfollow a preceding loop. Let’s consider a simple Z-stack of three slices captured every minute four times. In this case, it can be said that the Z-stack (“Z”) is nested inside the time-lapse (“T”) because it is performed in every iteration of the time-lapse loop (parent loop). The total number of frames is 3 * 4 = 12. The order of the experiment is as follows: Figure&nbsp;1033.&nbsp;Nested loops To show an example of succeeding or non-nested loops let’s consider two time-lapses performed one after another. First time-lapse (T0) captures every hour and the second (T1) every 10 minutes. The first 3 times (2 hours) and the second 4 times (40 minutes). The total number of frames is 3 + 4 = 7.It is clear that there is no nesting in this example as none of the loops is captured inside an iteration of the other one. The order of the experiment is as follows: Figure&nbsp;1034.&nbsp;Succeeding loops For non-nested loops it is important that they are exclusive. That is: each image frame “belongs” either to one loop or the other (e.g. frame at index 4 has no relation to the T0). On the other hand for a nested loop an image frame “belongs” to both loops (e.g. frame at index 4 in the previous example was captured in the second iteration of the time-lapse and second iteration of the Z-stack). ",
     id: 170 }, 
   { title: "Defining Loops",
     xmlid: "id|p2c15s9s3s4x",
     content: " Loops are added manually using the toolbar. Nesting is modified using the arrows. The current state can be loaded/saved from/to a bin file. Figure&nbsp;1035.&nbsp;Loops toolbarEach loop must have assigned its type. The type must be one of the following: Wellplate, Well, Time-Lapse (Time) , Position, Z-stack (Z) or Channel. Figure&nbsp;1036.&nbsp;Defining the loop typeAfter each and every loop is added a corresponding column is added to the file/frame table. The order of columns reflects the order of loops (and therefore the order of acquisition). After the loop order is defined the table may look as the following example. Figure&nbsp;1037.&nbsp;Loop columns ",
     id: 171 }, 
   { title: "Assigning Indexes to Frames",
     xmlid: "id|p2c15s9s3s5x",
     content: "   Each row in the file/frame table corresponds to a frame. First columns state the name of the file and some basic information about the image. Columns to the right hold loop indexes.There are two ways to fill each index column: automatically (by parsing the filename) or manually. Figure&nbsp;1038.&nbsp;Defining a pattern Automatic index inferring is done by a Regular Expression (see    ). Each loop may have its own expression which is evaluated on every filename (including the subfolder if not present in the root folder). It must match and produce (capture) either an integer index or special ordering information. Figure&nbsp;1039.&nbsp;Ordering informationMapping Regular expression output to indexes:Index (integer) is the index itselfColumn and Row for defining the index for a given well plate definitionname, barcode, x-, y-, z-position are mapped to an index by natural ordering by defaultFor example, let’s consider time-lapse of two images having three channels: Figure&nbsp;1040.&nbsp;Example of a regular expressionTime can be parsed directly into the index. The channel is printed as text in the filename, so index cannot be inferred directly. The channel name is used instead. Figure&nbsp;1041.&nbsp;Complex regular expression Following editing options apply only to manual index editing.Assign index to the selection&nbsp;As the columns are sorted based on the execution order, the leftmost index is the one that changes less often. Index can be set to a selection of frames. Figure&nbsp;1042.&nbsp;Indexing selected frames manuallyIn this case time-lapse “After” is set to 1 and “Before” is set to 0. Figure&nbsp;1043.&nbsp;Set index to a selection of framesAssign a uniform sequence&nbsp;If the files can be sorted reasonably, sequence Indexing can be used. Figure&nbsp;1044.&nbsp;Setting a uniform sequence to a selection of framesEdit index individually&nbsp;Manual indexes may be edited manually one-by-one. Figure&nbsp;1045.&nbsp;Manual index editing ",
     id: 172 }, 
   { title: "Adding Metadata to Loops",
     xmlid: "id|p2c15s9s3s6x",
     content: " Each loop type has a different set of metadata which must be assigned using Define.... Figure&nbsp;1046.&nbsp;Defining loop metadataThe definition dialog is partially filled by default. The non-parsed columns could be edited. Clicking on Refill inserts the default values. Figure&nbsp;1047.&nbsp;Position Loop definitionIn the Define Position Loop dialog, the position of each point should be filled. Positions are supposed to be set in the “large image coordinate system”. For example when there is no overlap, the step in x-axis direction between the neighboring frames can be calculated as {% raw %}frame width * calibration{% endraw %} (see the image below). Positions can also be imported from MS Excel clipboard using the   Paste from Clipboard button in the toolbar of the definition dialog. Figure&nbsp;1048.&nbsp;Coordinate system for Position Loop Figure&nbsp;1049.&nbsp;Channel Loop definitionIn the definition dialog for a well loop, a well-plate must be selected in order to be able to correctly parse the well position. Figure&nbsp;1050.&nbsp;Well plate definitionIf all the needed information is filled, the status bar at the bottom of the main dialog shows the “Loops are correctly defined.” message and OK is enabled. Otherwise warning messages are shown. Figure&nbsp;1051.&nbsp;Warning message example Figure&nbsp;1052.&nbsp;Correct loop definition ",
     id: 173 }, 
   { title: "Examples",
     xmlid: "id|p2c15s9s3s7x",
     content: "     The source image files are of the following filename format:Time-lapse name\\ Time-lapse name plate name\\well name(fld position index wv channel name).tif Figure&nbsp;1053.&nbsp;Image filenames exampleThe loop structure (nesting) could be set as in the picture bellow: Figure&nbsp;1054.&nbsp;Loop structureWe can use following patterns to extract the corresponding index or name from the filenames:Time-lapse loop(?&lt;name&gt;\\w+)\\\\*Plate loop\\w+\\\\(After|Before) (?&lt;name&gt; Plate \\d+-\\d+)\\\\*Well loop\\w+\\\\(After|Before) Plate \\d+-\\d+\\\\(?&lt;row&gt;\\w) - (?&lt;column&gt;\\d\\d)\\(*Position loop\\w+\\\\(After|Before) Plate \\d+-\\d+\\\\\\w - \\d\\d\\(fld (?&lt;index&gt;\\d\\d) *Channel loop\\w+\\\\(After|Before) Plate \\d+-\\d+\\\\\\w - \\d\\d\\(fld \\d\\d wv (?&lt;name&gt;[^\\)]+)\\)*Another possibility is to use the manual index definition described here  . Matched names or indices should be visible in the corresponding columns: Figure&nbsp;1055.&nbsp;Parsed columnsMatched names or indices (or other types mentioned in section Automatic index finding using pattern) should be defined using the corresponding definition dialogs.If all the required information is filled, it is possible to press OK and import files into the JOB.In this specific case, loops are reordered to follow the created ND2 files, which means that the last three loops are Time-lapse loop, Position loop and Channel loop. Figure&nbsp;1056.&nbsp;Result table of example 1 Suppose the same image filename format as in Example 1. We can avoid using a time-lapse loop and import files as two independent experiments captured in time “Before” and time “After”.Set the loop structure as in the picture below. Figure&nbsp;1057.&nbsp;Loop structureWe can use following patterns to extract the corresponding index or name in “Before” experiment:Plate loopBefore\\\\Before(?&lt;name&gt; Plate \\d+-\\d+)\\\\*Well loopBefore\\\\Before Plate \\d+-\\d+\\\\(?&lt;row&gt;\\w) - (?&lt;column&gt;\\d\\d)\\(*Position loopBefore\\\\Before Plate \\d+-\\d+\\\\\\w - \\d\\d\\(fld (?&lt;index&gt;\\d\\d) *Channel loopBefore\\\\Before Plate \\d+-\\d+\\\\\\w - \\d\\d\\(fld \\d\\d wv (?&lt;name&gt;[^\\)]+)\\)*Substitute Before with After to use the patterns for “After” experiment or use the manual index definition. Figure&nbsp;1058.&nbsp;Parsed columns  Figure&nbsp;1059.&nbsp;Result table of example 2 The source image files are of the following filename format:well name_ttime-lapse index_pposition index_zz-stack index.tif Figure&nbsp;1060.&nbsp;Image filenames exampleSet the loop structure as in the picture bellow. Figure&nbsp;1061.&nbsp;Loop structureWe can use following patterns to extract the corresponding index or name from the filenames:Well loop(?&lt;row&gt;\\w) - (?&lt;column&gt;\\d\\d)_*Time-lapse loop\\w - \\d\\d_t(?&lt;index&gt;\\d\\d)_*Position loop\\w - \\d\\d_t\\d\\d_p(?&lt;index&gt;\\d\\d)_*Z-Stack loop\\w - \\d\\d_t\\d\\d_p\\d\\d_z(?&lt;index&gt;\\d\\d).*Another possibility is to use the manual index definition described here  . Figure&nbsp;1062.&nbsp;Parsed columnsIf all the required information is filled, it is possible to press OK and import files into the JOB. Figure&nbsp;1063.&nbsp;Result table of example 3 ",
     id: 174 }, 
   { title: "Basic Layer Measurement",
     xmlid: "id|p2c16s1x",
     content: " Open image, prepare for measurementOpen your image containing layers to be measured.Calibrate your image (if not already calibrated) using   .Run the    command.Select the proper layer mode using the tabs (Linear, Circular, General, CaloTest). The measuring procedure is specific to each layer mode.Define baseline for the measurementLinear Linear layers are measured perpendicularly to the top/bottom image edge. If the layers in your image are not parallel to the top/bottom image edge, use   Define Baseline to level them. Drag a line, adjust its end points and confirm it by the secondary mouse click. The image is leveled.Circular Define the reference radius using  Define Radius. Click three points on the reference circle which are not too close to each other. Measurement is done from the center of the defined circle. To correct the definition, click   Redefine Radius.General Define the polyline using  Define Polyline. Click as many points you need to define the reference line. Measurement is done perpendicularly to this line. To correct the definition, click   Redefine Polyline.Define Layers to be measuredFor each layer:Click   New Layer.Name the layer in the Name field.Select the Color of the layer.Define a layer using one of the following detection methods: Autodetect Automatically detects a layer. After clicking into a homogeneous area, the system detects a region of similar pixels. Fine-tune the selection using the mouse wheel and confirm it by the secondary mouse click. Draw Runs the binary editor which can be used to define your layer with various drawing tools. Once you are finished with the layer drawing, click   Exit Editor or press Esc.See  . Threshold Opens the Define Threshold dialog window used for thresholding your layer (see   ).In the Gaps drop-down menu choose the behavior which will be use if your image contains gaps and holes (click on    to see the explanation).Check whether this layer will be measured (Measure checkbox).Set optional limits of the measurement (check Limits and fill in the Min and Max value).Resolve overlapping layersSelect the layers which are visible in the image by clicking left from the layer name in the Layers Definition table. Selected layers are highlighted blue with a green point in the Sel. column. Hold down Ctrl to select multiple layers. Change order of the selected layer(s) in the list using   Move Up,   Move Down.Define the Keep Edge feature for each layer. If the binary layers of your sample overlap, it is necessary to define which portions of the overlap are used for the measurement of each layer. This depends on the order in the Layers Definition table and on the Keep Edge setting:None The currently defined layer is cropped by both neighbouring layers defined in the definition table if their binary image is overlapping.Top The currently defined layer is cropped by the layer below and not cropped by the layer above in the definition table.Bottom The currently defined layer is cropped by the layer above and not cropped by the layer below in the definition table.Both The currently defined layer will not be cropped by any other layer.Click   Adjust Edges if your layers overlap and you want to apply the order set in the Keep Edge definition to solve the overlapping areas.Adjust measurement settingsIf you want to limit the measurement on sides of the image, use   Apply Limits and drag the red vertical lines by the white square to limit the measurement only to the inside. For circular measurement click two points to define the circular sector. To turn the limits off, click   Apply Limits again. To change the limits, use  Redefine limits.Optionally define vertical intersections over which the measurement is made. The number of vertical lines (green color in the image) can be specified in the Number of Intersections or their spacing can be set in the Intersection Every option.Handle the resultsAt any time you can delete the defined binary layers together with the measured results after clicking   Clear Data.Results of the measurement are displayed in the results table on the right. You can export the results to a report (  Export to Report), to MS Excel (  Export to Excel), to a .csv file (  Export to File), to windows clipboard (  Export to Clipboard), you can also export raw data to MS Excel (  Raw data to Excel), to a .csv file (  Raw Data to File), or to windows clipboard (  Raw Data to Clipboard). Clicking on   Edit Report... reveals the report panel enabling to define a custom report (see  ).To save the current results into the database, use   Save to History. These results can then be found in the Measurement Explorer panel (see  ).Save Measurement Settings (optional)To save the Layer Thickness Measurement settings, use   Save Definition, name the .bin file and specify its location. To load a previously saved definition, click   Load Definition.   Reset Definition clears the dialog window settings. Figure&nbsp;1220.&nbsp;Image window showing that the linear layer measurement will be performed over the intersection lines (green) inside the limited area (red lines). Figure&nbsp;1221.&nbsp;Image window showing the circular layer measurement on five layers Figure&nbsp;1222.&nbsp;Image window showing the general layer measurement on two layers with 20 intersections ",
     id: 175 }, 
   { title: "Calo Test",
     xmlid: "id|p2c16s2x",
     content: " Open your image containing layers to be measured.Calibrate your image (if not already calibrated) using   .Run the    command.Select the CaloTest mode.Enter the proper Ball Diameter [mm] used for the Calo test.Click   Layer to add a layer.Name the layer in the Name field.Select the Color of the layer.Define a layer using one of the drawing tools. Show Drawing displays a sample Calotest design which can help when defining the layers. 3pts Circle Defines a circular layer by placing three points onto the perimeter of the layer. Circle Defines a circular layer by clicking into the center of the circular layer and then dragging the mouse outwards to define the perimeter while holding the primary mouse button down.Check whether this layer will be measured (Measure checkbox).Repeat steps 6-10 for all layers in your image.Results of the measurement are displayed in the results table on the right. You can change the Units, export the results to a report (  Report), to MS Excel (  Export to Excel) or you can delete the defined binary layers after clicking   Clear Data. Figure&nbsp;1223.&nbsp;Image window showing the Calo test layer measurement ",
     id: 176 }, 
   { title: "Introduction",
     xmlid: "id|p2c17s1x",
     content: " The Measurement Sequencer module provides tools for creating, running and managing measurement sequences. A measurement sequence is a predefined series of subsequent measurement steps suitable for repetitive usage. Results can be evaluated automatically and customized reports can be created. When this module is installed, the following commands appear in the main menu:   For the preparation of measurement sequences.   Used for executing the defined measurement sequences.   A tool for managing the defined measurement sequences.   an example of routine measurement of otolith morphology and morphometry.Unzip the files.Import the “Otolith_sequence_definition.bin” file to the definition panel.Open both PNG images to NIS-ElementsActivate the “Otoliths_sample_image.png” image and run the measurement sequence.Example Videos    ",
     id: 177 }, 
   { title: "Basic Workflow",
     xmlid: "id|p2c17s2x",
     content: " Create a new sequenceRun    to display the definition window.Open a typical image on which the measurement sequence will be specified.Add the first measurement step to the tableSelect a measurement tool from the tool bar and click on it.A new row is added to the table. Define its properties - see  .Test this step on the current imageClick on the  button and draw the object to the image just as the operator will do.If the check box in the Scheme column is selected, the object you draw now will be displayed in the measurement scheme (displayed by the Show Drawing button).Finish creating the sequenceRepeat the above steps to add all the measurement steps intended in the sequence and click the   Test button.Make sure the procedure works correctly. If needed, change the parameters in the table or use the  button to modify the drawing of each step.Click the Save button. The measurement sequence will be saved to an internal database under the name you define.Run the sequenceRun    to display all available measurement sequences.Double-click the name of a sequence to run it.The Measurement Sequencer - Run panel appears. Do all steps required by the sequence definition. See  .Optionally, export results to a report ( ) or do whatever your methodology asks. ",
     id: 178 }, 
   { title: "Features of the Cast Iron Analysis Module",
     xmlid: "id|p2c18s1x",
     content: " Graphite classification according to ISO/ASTM/JIS standards (ISO 945, ASTM A247-06, JIS G5502-2022 JIS, JIS G5502-2022-ISO, JIS G5502-2024-ISO).Ferrite/pearlite ratio measurement.Customization of the predefined measurement procedures.Easy data export. ",
     id: 179 }, 
   { title: "Single Image Measurement",
     xmlid: "id|p2c18s2x",
     content: " Open/acquire the image to analyze.Set the detection procedure parameters within the Detection panel.Set the measurement procedure parameters within the Measurement panel.Run the Measure &gt; Measure command.View the measured data within the Data tab and export it to one of the available outputs via the Export button.See  . The analysis can be performed even on two or more images sequentially. After defining all the parameters, run the Measure &gt; Measure Sequence command. ",
     id: 180 }, 
   { title: "Sequential Measurement",
     xmlid: "id|p2c18s3x",
     content: " Place the images that you want to analyze sequentially to one folder.Select the Measure &gt; Measure Sequence command.A window appears. Set the right Images of Type that will be displayed in the images list.Select the images to be analyzed with a mouse: one by one holding the Ctrl key down, or select a range holding the Shift key down.Run the batch process by clicking the Measure button.Confirm each single measurement the same way as if working with a single image. ",
     id: 181 }, 
   { title: "Reference",
     xmlid: "id|p2c18s4x",
     content: "      ",
     id: 182 }, 
   { title: "Measurement Panel",
     xmlid: "id|p2c18s4s2x",
     content: " Once the Cast Iron application is run, the Measurement tab appears in the right-side docking pane. Figure&nbsp;1232.&nbsp;Select the measurement method in Measurement Presets.Adjust the Test Mask proportions and size.Test Mask OptionsPress the Show Mask button on the left toolbar to display the mask. The test mask dimensions can be modified either by inserting values inside the edit boxes, or by dragging the node points inside the picture by mouse.Fit to screen Check this box to maximize the test mask to the whole image area. Some edit boxes get disabled then.Fit to Centre Press this button to move the whole test mask to the middle of the image area.Default Size Press this button to reset the user settings and resize the test mask according to the standard.Select the Standard according to how the results shall be treated.Set Restrictions parameters to exclude objects of extreme values.Class colors of the Shape Classifier can be modified. The classification rules can be modified using a training procedure. Click the Modify Classifier button, select a class, and pick typical objects from the image. However, the default classifier settings are based on the selected ISO/ASTM/JIS standard, so it is not recommended to modify them in any way.Use the Save button to store changes of the current method, or use the Save As button to add the current settings as a new measurement preset, then press the Save button to store the setting. ",
     id: 183 }, 
   { title: "Left Tool Bar",
     xmlid: "id|p2c18s4s3x",
     content: " Once the detection and measurement options are set and saved, run the Measure &gt; Measure command. The measurement will be performed and the results will be written to the Data table. The detected grains will be indicated inside the image. Since the automated system can make mistakes, it is required to confirm the correctness of each measurement. Use the following tools (placed on the left toolbar) for corrections: Separate Use this tool to separate graphite manually if some of the grains were detected incorrectly. It is recommended to increase transparency of the binary layer so the object borders become more obvious.See  . Connect Creates a connection between two neighboring graphites. Draw the connection by mouse. Delete Deletes continuous areas of the binary layer after they are clicked on. Show Mask Displays / hides the vector test mask. If the mask is moved before the measurement results are accepted, the image is re-measured according to the new placement of the mask. Show Labels Displays labels within the image. Measure Cast Iron Runs the Cast Iron measurement. Options Opens the Cast Iron Options dialog window.General Here you can turn on the option to Show message box for confirm the 'save' action in measurement and detection settings and optionally switch to the JIS G5502-JIS nodularity calculation method.Labels Sets parameters of the labels such as the font type, font size, background color and bounding rectangle color.Export User can check which options are being exported to the report and which are burned into the image. Toolbar Setup Executes   . Finish Measurement Saves the current measurement results to the Data table. The system becomes ready to measure the next field (image). Skip Measurement When measuring a sequence of images, this button discards the results of a single measurement. Cancel Measurement Discards results of the current measurement. ",
     id: 184 }, 
   { title: "Data Panel",
     xmlid: "id|p2c18s4s4x",
     content: " The data panel is placed at the bottom of the application screen by default and is divided into thirds.See  . ",
     id: 185 }, 
   { title: "Introduction",
     xmlid: "id|p2c19s1x",
     content: "       Measurement of single-phase structures (e.g. Austenite, Aluminium).Measurement of two-phase structures (e.g. Ferrite-Austenite, Ferrite-Pearlite).Planimetric, Linear, Circular, Abrams and Comparison Charts measurement methods.Customization of the predefined measurement procedures.Support of  .Easy data export. The Grain Size module is compliant with the following standards: ASTM E112-13 (2013)ASTM E1382-97(2015)ISO 643 (2012)JIS G0551 (2013)GB/T 6394(2017)See also  . There are the following methods implemented in the application: Planimetric The grain size is calculated from the number of grains detected in the measured area.Linear, Circular, Abram's The grain size is calculated from the number and length of intercepts of the detected grains with the measurement mask.Depending on the method and the standard, the Grain Size number (GS) is determined either as a result of an arithmetic expression or by assigning the closest GS value listed in the standard. There are two main modes of measurement in the application:Direct Mode The operator controls every step of the measurement process – image selection, detection settings, method settings. Manual corrections of the detected grains/intercepts are allowed. Multiple individual measurements can be merged together in order to get the GS from a higher number of measured fields unless the method and the standard remain unchanged. Custom settings of the detection or the method can be saved as presets. See  .Wizard Mode It is designed for automated measurement. A complete preset of the detection and the measurement method is created and then applied to any number of images. Once started, changes or corrections are not possible. See   Parts of the ISO 643 have been reproduced in this software with the permission of the Czech Office for Standards, Metrology and Testing (UNMZ) on behalf of the International Organization for Standardization (ISO). This standard can be obtained from Czech Standardization Agency (CAS) or from the ISO Central Secretariat. Copyright remains with ISO. ",
     id: 186 }, 
   { title: "User Interface",
     xmlid: "id|p2c19s2x",
     content: "        ",
     id: 187 }, 
   { title: "Application Workspace",
     xmlid: "id|p2c19s2s1x",
     content: "  Figure&nbsp;1233.&nbsp;Any panel can be displayed from a context menu.The application has its own layout. To enter the layout, run the    command.The following control panels are displayed in the layout by default.      Other important buttons are placed on the left tool bar. Some of them appear or disappear depending on the procedure status. See also  .To quit the application, use the   File &gt; Return to NIS-Elements  command. ",
     id: 188 }, 
   { title: "Measurement Panel",
     xmlid: "id|p2c19s2s3x",
     content: " While the quality of detection depends on the type and look of the sample and may often vary, settings in the measurement panel are fixed and very easy to set up. After installation, there are four basic measurement presets which all meet the requirements of the standards. Figure&nbsp;1235.&nbsp;Measurement panelSelection of the measurement preset.The Save As button for creating new presets.Selection of the standard. The selection of the industrial standard is part of the preset.The Save button for saving changes to the current preset.See  . ",
     id: 189 }, 
   { title: "Display Modes",
     xmlid: "id|p2c19s2s5x",
     content: " When the measurement starts, the detection result and the measurement mask appear on the screen in the form of a binary layer. You can change the display by the following buttons:   The grains are colorized automatically. The counted grains are highlighted, the excluded ones are darker. Figure&nbsp;1237.&nbsp;OFF Figure&nbsp;1238.&nbsp;ON   Contours of the grains are drawn by a thick line. Figure&nbsp;1239.&nbsp; Figure&nbsp;1240.&nbsp;   Object indexes of the counted grains may be displayed – the numbers correspond to the numbers in the data table. Active buttons turn to white base while inactive remain gray. Figure&nbsp;1241.&nbsp; The measurement mask is always applied no matter if the    button is pressed or not. Its visibility has no influence on the measurement. ",
     id: 190 }, 
   { title: "Basic Workflow",
     xmlid: "id|p2c19s4s1x",
     content: " Make sure the Grain Size module is started (  )Click on the    button on the left tool bar.Select the measurement method, industrial standard and the image data source. The procedure then differs depending on the selected Image Data Source: Figure&nbsp;1251.&nbsp; Automatic Capture Requires a motorized stage and a camera. The user specifies size of the sample and the number of fields to be scanned.Manual Capture Requires a camera. The user is asked to capture a new image before each measurement.File Sequence The user selects a folder on the disk, all images inside the folder are measured.Multi Point ND File Each frame of a multi-point ND file is measured. An ND file is a single file containing a sequence of images (see  ). Path to the multi-point file can be specified using the Open... button.Current Document Only the current image is measured. It is useful for testing of settings before an automated measurement is run.Click on the Start Measurement button, select a detection preset, adjust the detection.Every measurement in a Wizard Mode starts with cleaning all previous data. In this mode it not allowed to merge old measurement with a new one. Following message appears. Figure&nbsp;1252.&nbsp;Click Yes to delete the content of the    panel and continue with the measurement.The following window appears: Figure&nbsp;1253.&nbsp;Select a detection preset and, if needed, adjust its settings in the    panel.Click Measure Field to continue.Measure the first image.Decide whether to repeat the procedure for another image or whether to measure the rest of the images automatically. ",
     id: 191 }, 
   { title: "Manual Capture",
     xmlid: "id|p2c19s4s3x",
     content: " This procedure is very similar to Automatic Capture with the exception that the user is prompt to capture a new field after the previous field is measured. Figure&nbsp;1258.&nbsp;If you set the Field Count option, the following window appears when the number of fields is reached. Figure&nbsp;1259.&nbsp; Yes The measurement will go on until the operator terminates it.No The measurement will be terminated.Please see  . ",
     id: 192 }, 
   { title: "Planimetric Method Settings",
     xmlid: "id|p2c19s5s1x",
     content: " The planimetric method counts the detected grains inside the reference area and returns the Grain Size value. The approach to counting of grains inside the reference (measured) area differs according to the selected standard:ASTM E1382-97 The reference area is created by the grains placed only inside the measurement mask. All grains touching the mask boundary are excluded.ASTM E112-13, JIS G0551, ISO 643 The reference area equals the measurement mask. All grains entirely inside the mask have weight 1, grains which touch the mask boundary have weight 0.5 and grains in corners (using a rectangular mask of ISO 643) have weight 0.25. Figure&nbsp;1260.&nbsp;ASTM E1382-97 Figure&nbsp;1261.&nbsp;ASTM E112-13, JIS G0551, ISO 643In contrast to a common thresholding in NIS-Elements, the Grain Size module detection automatically transforms the binary objects (grains) keeping 1 pixel boundary thickness between two neighbours (according to the Norm / Standard instruction). That´s why the binary image changes a little bit after the basic threshold interval setting. After the detection parameters are set, the algorithm transforms all the boundaries between grains to be 1 pixel wide as required by the standard. This is why the binary image changes a little. ",
     id: 193 }, 
   { title: "Overlay Transparency",
     xmlid: "id|p2c19s5s2x",
     content: " The strength of the overlay can be modified by a right click on the overlay button. Figure&nbsp;1262.&nbsp; Figure&nbsp;1263.&nbsp; ",
     id: 194 }, 
   { title: "Introduction",
     xmlid: "id|p2c19s7s1x",
     content: " The comparative method is based on a human eye observation and comparison of the specimen image in the microscope with a set of charts, printed out in a paper form and worldwide spread as the attachment to the industrial standard.The standard charts were designed for a fixed total magnification 100x (sample to chart) . This is mostly realized by using a 10x objective with a 10 x eyepiece. ",
     id: 195 }, 
   { title: "Physical Zoom",
     xmlid: "id|p2c19s7s2x",
     content: " If we want to apply the comparative principle, it is absolutely essential to have a correctly set “physical zoom”. A calibrated acquisition system produces calibrated images (images with known pixel size). To display such images in correct size on the screen, the monitor must be calibrated (sizewise) as well.Show physical zoomClick with  RMB to the image to display a context menu and select the Show physical zoom command. If the image is calibrated, a value of the physical zoom appears in the top left corner. Otherwise N/A would be displayed. Figure&nbsp;1276.&nbsp;Check monitor calibrationClick  RMB on the physical zoom value and select Monitor Calibration. Figure&nbsp;1277.&nbsp;A dialog appears where you can check whether the system detected your monitor correctly. If not, you can enter real dimensions of your screen (the actual display without a frame) manually. Figure&nbsp;1278.&nbsp;Zoom the image as requestedWhen everything is alright, you can zoom the image to a particular physical zoom (sample to screen). Right-click the physical zoom field and select a predefined zoom value. Figure&nbsp;1279.&nbsp;With this function, we can correctly compare images with the standard charts, stored in the software module. As the comparative method operates with the physical zoom automatically, it is not necessary to set it up before the measurement. This option just helps to make a good orientation and to select the best objective for the measurement. ",
     id: 196 }, 
   { title: "How to Create Your Own Grain Size Comparison Charts",
     xmlid: "id|p2c19s7s5x",
     content: " Create or capture a set of TIF images meeting these requirements:Images are calibrated correctly using NIS-ElementsResolution is at least 200 DPIName the files using the pattern: SSS_GNNN_DCCC.tifSSS name of the standardNNN G value = grain size (can be a decimal number)CCC size of the measurement circle in millimetres (can be a decimal number)Send these files to Laboratory Imaging to   with the subject “Request for comparison charts” (use a web storage service if needed).In a matter of days, we will create and send you a binary file (*.bin)Save the binary file to the “c:\\Program Files\\NIS-Elements\\GrainSize\\Masks” subfolder.Restart NIS-Elements, the added industrial standard will appear in the pull-down menu of the Grain Size comparison method window. Do not forget to make a backup copy of the binary file containing your comparison charts. You might need it if you make some changes to the installation in the future, e.g.: change the hard-drive or re-install NIS-Elements or Windows. ",
     id: 197 }, 
   { title: "Export Options",
     xmlid: "id|p2c19s8s2x",
     content: " Apart from exporting the data to a report template, the Export button of the    panel provides other options. Remember, that you select the target location of the export first, but you must click the button again to start the export. Figure&nbsp;1291.&nbsp;Clipboard Data are copied to Windows clipboard ready to be inserted to any other program.Excel A new Excel file called “Grain Size n.xls” where “n” is a number which gets increased with any subsequent export.(MS Excel installation required).Web Browser A temporary HTML page is created and opened in the default web browser.HTML Clipboard An HTML code containing the data is copied to the clipboard.File A window appears which enables the user to save a text file (*.txt) containing the data.Printer Opens the system printing dialog.Report Opens the data in the NIS-Elements report editor formatted as a simple table.Report Template Grain Size module sends the data to a specified report template (*.rtt). See  . ",
     id: 198 }, 
   { title: "Reference",
     xmlid: "id|p2c19s9x",
     content: "          ",
     id: 199 }, 
   { title: "Interface",
     xmlid: "id|p2c1s1x",
     content: " To switch to the GPA application layout, click on GPA in the NIS-Elements main toolbar. Figure&nbsp;489.&nbsp; Figure&nbsp;490.&nbsp;GPA application user interface.Main GPA control panes    ,  , camera and microscope control panels To show or hide the GPA Explorer and Acquisition panel and to select a GPA recipe, use the top toolbar: Figure&nbsp;491.&nbsp;GPA top toolbar. ",
     id: 200 }, 
   { title: "Basic Workflows",
     xmlid: "id|p2c1s2x",
     content: "    ",
     id: 201 }, 
   { title: "Control Windows",
     xmlid: "id|p2c20s2x",
     content: "     ",
     id: 202 }, 
   { title: "Introduction to Triggered Acquisition",
     xmlid: "id|p2c21s1x",
     content: " The “triggered acquisition” in NIS-Elements is the process of acquiring images where some actions during the acquisition are controlled solely by hardware (TTL and analog signals, waveforms) without the software interference. Connection generally requires cables, break-out boxes and/or specialized DAQ cards or cameras. As a result, higher frame-rates can be achieved. There are the following common applications of the triggered acquisition:Multi-excitation A camera triggers a wavelength switcher.Piezo Z A camera triggers a piezo Z stage.Multi-excitation + Piezo Z A combination of the above mentioned applications - a camera triggers a Piezo Z stage and a wavelength switcher.  There are also other applications such as multi camera systems where the master camera triggers slave cameras, but it exceeds the scope of this tutorial (see the Dual Andor Handbook for further info on dual cameras systems). ",
     id: 203 }, 
   { title: "Wiring",
     xmlid: "id|p2c21s3x",
     content: "       Connect the triggering cable to the connector of your camera which transmits the triggering TTL signal. The other end of the cable which carries the Exposure TTL signal must be connected either directly to the triggered device (or its controller) or to a PFI connector of an NIDAQ card - in case such a card is used.Andor iXon, Andor Luca  Figure&nbsp;1359.&nbsp;Andor iXon/Luca triggering cable  Figure&nbsp;1360.&nbsp;Luca Fire connector  Figure&nbsp;1361.&nbsp;iXon Fire connectorAndor Clara  Figure&nbsp;1362.&nbsp;Andor Clara triggering cable  Figure&nbsp;1363.&nbsp;I/O connector, FirePhotometrics CoolSNAP MYO, Photometrics CoolSNAP KINO  Figure&nbsp;1364.&nbsp;CoolSNAP MYO / KINO triggering cable  Obtain the cable from Photometrics part #: CBL-IO-HR10-F-BNCUse the BNC cable labeled “Expose Out”. Figure&nbsp;1365.&nbsp;TRIGGER connectorPhotometrics CoolSNAP HQ2  Figure&nbsp;1366.&nbsp;CoolSNAP HQ2 triggering cable  Obtain the cable from Photometrics part #: 37-107-002  Figure&nbsp;1367.&nbsp;I/O connector, “Exposure” BNCPhotometrics QuantEM:512SC  Figure&nbsp;1368.&nbsp;QuantEM:512SC triggering cable Obtain the cable from Photometrics part #: 37-513-003 Figure&nbsp;1369.&nbsp;I/O connector, Exposure BNCHamamatsu ORCA-Flash2.8 Connect the triggering cable to the Timing I/O connector of the camera. The Exposure signal goes to the branch labeled Signal 1. Figure&nbsp;1370.&nbsp;Exposure signal goes to “Signal 1”  Figure&nbsp;1371.&nbsp;Timing I/O connectorHamamatsu ORCA-Flash4.0 Connect the cable to the Timing 1 connector on the camera body. Figure&nbsp;1372.&nbsp;ORCA Flash4.0 Triggering Cable Figure&nbsp;1373.&nbsp;Timing 1 connectorHamamatsu ImagEM Use standard BNC-to-BNC cable to connect the device to be triggered to the Timing I/O 2 connector. Figure&nbsp;1374.&nbsp;Timing I/O 2 connector Lambda DG-4 Use DG-4 shutter HW box: Plug the In connector to the Exposure signal from the camera, the Out connector to the Strobe connector on the DG-4 body, and the USB connector to the computer. The purpose of the box is to avoid bleaching a sample during camera readout in multi excitation. It is the product of Laboratory Imaging and can be ordered at  , LIM product code LA_TRGSHUT. Figure&nbsp;1375.&nbsp;DG4 Shutter HW box Figure&nbsp;1376.&nbsp;The Strobe connector of DG4 If you do not use the HW box and connect the Exposure signal to the Strobe connector (via a BNC-to-BNC cable), the specimen will be illuminated for the whole time of the experiment (even during the readout phase).CoolLED pE-x The 15-pin D-type connector of the break-out box socket is fitted to the back of the Main Unit. Connect the Exposure signal from the camera to the Sync In connector of the pE-x break-out box. Purchase “break-out box” (product code #244-10000) from  . Figure&nbsp;1377.&nbsp;CoolLED pE-x break-out boxNikon LU4A Connect the Exposure signal from the camera to the thin branch of the Nikon MXA22104 cable. Figure&nbsp;1378.&nbsp;Nikon MXA22104 cable Figure&nbsp;1379.&nbsp;siThe back of LU4AAgilent MLC 400 on NIDAQ board A suitable NI break-out box connected via NIDAQ PCI card will be needed. The box must contain at least one PFI input (capable of receiving the TTL triggering signal) and four analog outputs to control the lasers.Connect the camera Exposure signal to the PFI connector of the break-out box and also select four analogue outputs and connect them to the Analog Input connectors of MLC 400. In the next step, you will assign the connected analog outputs to the Illumination Device within the NIDAQ configuration window. Figure&nbsp;1380.&nbsp;Analog Input connectors of MLC 400 The triggered Piezo Z always requires an NIDAQ board with at least one analog output to control the piezo Z controller and one PFI input which accepts the TTL exposure signal from the camera.Prior NanoScanZ Piezo, Physik Instrumente E-665 Piezo, MCL Nano-Drive Use the 68pin-2BNC Real Time Acquisition Cable (to be purchased at  ), formerly sold under the code LA_RTPZMXAC) to establish the connection between camera, NIDAQ board and the Piezo Z controller. Correct software options must be set within the Piezo Z configuration: input pin - PFI 5, output pin - AO 0. Figure&nbsp;1381.&nbsp;68pin-2BNC Real Time Acquisition Cable  A suitable break-out box can be used instead of the  . The box must contain at least one PFI input (capable of receiving the TTL triggering signal) and one analog output to trigger the Z controller.Connect the 68pin connector to the NI DAQ PCI board (PCI 6711, PCI 6713, PCI 6733). Connect the Camera end of the cable to the Exposure output signal of the camera, and the remaining end of the cable to the piezo Z controller.Use the Input connector of the NanoScanZ/Nano-Drive controllers.Use the Control In connector of the E-665 controller. Figure&nbsp;1382.&nbsp;The Input connector of Prior NanoScanZ controller Figure&nbsp;1383.&nbsp;The Input connector of MCL NanoDrive controller Figure&nbsp;1384.&nbsp;The Control In connector of PI E-665 controller Multi-excitation (wavelength switcher + camera)Sutter Lambda DG-4 + any camera Connect the camera with DG-4 by the triggering cable. It is suggested to buy also the  , but this is optionalCoolLED pE-x + any camera Connect the camera with   by the triggering cable. The   is connected to the CoolLED pE-x controller.Nikon LU4A + Andor iXon Connect the camera with LU4A by Nikon MXA22104 cable.Agilent MLC 400 + any camera Connect the camera with NIDAQ break-out box via PFI (which is connected to a NIDAQ board, e.g. PCI 6711).Z Stack (piezo Z + camera)Any Piezo Z drive + any camera An NIDAQ board and the   is required. The Exposure signal leads to the NIDAQ board. The NIDAQ board triggers the Piezo Z controller.Multi-excitation + Z stack (wavelength switcher + piezo Z + camera)Agilent MLC 400 + any piezo Z controller + any camera This combination requires an NIDAQ board + a break-out box. The beak-out box serves as a hub accepting the input signal from camera and distributing output signals to the wavelength switcher and the piezo Z controller.Any non-Agilent wavelength switcher + any piezo Z controller + any camera This combination requires the  . The camera signal is branched by a BNC “T” piece, one branch leading to the wavelength switcher, the othe leading to NIDAQ board.  Figure&nbsp;1385.&nbsp;Andor Ixon, DG-4, Piezo Z, HW Box, RT Acquisition CableNote Figure&nbsp;1386.&nbsp;Photometrics QuantEM:512SC, Piezo Z, RT Acquisition CableNote Figure&nbsp;1387.&nbsp;Andor Ixon, DG-4, Piezo Z, RT Acquisition CableNote Figure&nbsp;1388.&nbsp;Andor Ixon, Piezo Z, MLC 400 laser combiner Figure&nbsp;1389.&nbsp;Photometrics QuantEM:512SC, DG-4 A suitable break-out box can be used instead of the  . ",
     id: 204 }, 
   { title: "Settings in NIS-Elements",
     xmlid: "id|p2c21s4x",
     content: "        Install NIS-Elements. If an NIDAQ board is needed in your setup (check the   chapter), select one of the following options within the Devices &gt; National Instruments section of the installation wizard:NI DAQ driver for use with all I/O functionsNI PCI legacy drivers for use with Piezo ZSee   for details about settings of the NIDAQ board within NIS-Elements. No special software settings are necessary. The exposure signal is being generated on the I/O connector automatically. NIDAQ board For applications using an NIDAQ board (see  ), run the    command and Add the “NIDAQ” device. The following window appears: Figure&nbsp;1390.&nbsp;All devices which are connected to the NIDAQ card should be added to the list of Installed devices and configured properly (see the description below). Apart from that, it is important to set the Triggering line correctly. There is a list of all NIDAQ cards found in your PC at the bottom of the window. For each card, select the PFI input connector to which the TTL triggering signal is connected.Lambda DG-4, CoolLED pE-x, Nikon LU4A No special settings are necessary for these devices. NIS-Elements uploads the sequence of wavelengths to the device controller via the RS232/USB interface before the triggered acquisition is started.Agilent MLC 400 Laser Combiner Run the    command.Reveal the context menu over the NIDAQ device and click Configure....In the NIDAQ Configuration window, select Illumination Device from the right column and click Configure (add it from the left column if there is no Illumination Device).The following window appears. Select to which lines of the break-out box the camera and MLC 400 are connected.  Figure&nbsp;1391.&nbsp;  It is important that the PFI line and the output lines are attached and assigned to one NIDAQ board.Prior NanoScanZ Piezo Stage System, Physik Instrumente E-665 Piezo controller, MCL Nano-Drive Controller Run the    command.Reveal the context menu over the NIDAQ device and click Configure....In the NIDAQ Configuration window, select Piezo Z from the right column and click Configure (add it from the left column if there is no Piezo Z).The following window appears:  Figure&nbsp;1392.&nbsp;Select the Analog Line to which the device is physically connected and select the Enable Triggering check box. Set mapping between voltage and position and select the type of your Piezo drive to let the system know its orientation: Stage Piezo Z or Objective Piezo Z.Settling Time There is always a delay between the moment the Z drive receives the signal to move and the moment it achieves the target position. By defining the Settle Time [msec], you are telling the software to postpone the capture so images are not taken while the Z drive is moving. The actual settling time is related to travel distance: Figure&nbsp;1393.&nbsp;Optimal settling times for different travel distancesYou can optimize behavior of your piezo Z drive by setting different settling times for different travel distances. Travel distance is given as a percentage of the entire Z range.Shutter Custom-named shutters can be controlled by TTL or Analog signals. For this purpose, you can set-up open and close parameters, and the state of the shutter which will be set upon NIS-Elements startup or shutdown. Figure&nbsp;1394.&nbsp; Triggered Piezo Z In either    or    window, select the Triggered NIDAQ Piezo Z from the Z Device pull-down menu. Define Z stack parameters and press the Run Now button. Figure&nbsp;1395.&nbsp;Multi-Excitation Run   . The following window appears: The window layout and the options-set differs depending on which Multi-Excitation Device is selected.  Figure&nbsp;1396.&nbsp;Example of the Triggered Acquisition WindowCommon OptionsCamera Define which camera is used for acquisition. All currently connected (monochromatic) cameras are listed.Multi-Excitation Device Define which device is used for acquisition. All connected multi-laser illumination listed.Setup Mode Select the Enable Triggering option to enable the Setup Mode. Select the box and radio buttons appear in the # column. Select a channel to test its settings (live image of this channel will run). Add New Channel, Add Line, Add Line or Custom Signal This button adds rows to the list of channels. New channels are created at the end of the table. New Lines are inserted to the currently selected channel. Remove, Remove Line/Custom Signal This button removes the selected row (line or a channel).  Move Up/Down Moves the selected row up or down. Delete All Deletes all lines in the definition table.# (channel number) A check box and an automatic channel number appear in this column. Deselect the check box to omit the channel from acquisition.Channel name Editable channel name. It inherits the wavelength number of the inserted line by default.Line Select one of the available lines to be used with the channel. A line may specify a laser or other output signal (e.g. analog output).Power Defines power to be generated by the line. Percentage of the total power (%) for lasers or voltage of the analog output is used.Exposure Specify exposure time of the corresponding channel.Enable Triggering Select this box to turn the triggering ON. When you run the Live Image - a multichannel image shall appear on screen.Special OptionsFocus Offset If enabled in the   window, it specifies Z-offset for each channel.Imaging If enabled in the   window, imaging and non-imaging channels can be distinguished. A non-imaging channel performs some action without capturing an image.Use different exposures (NIDAQ HW Box required) This option turns on/off the possibility to select different exposures for each channel. It is supported only by Andor and Photometrics Evolve 512 cameras.Trigger signal connected to (DG4/5 only) In most cases, the Exposure option should be turned on. Use the Readout option only when using legacy cables + Sutter DG-4.Use shutter during readout (DG4 HW Box required) This option is available only if HW Box and DG4 are connected. If it is selected, then the shutter is opened only during exposure and is closed during readout. Advanced Settings...&nbsp;The following window appears when the Advanced Settings... button is pressed: Figure&nbsp;1397.&nbsp; Use Focus Offset (Piezo Z) If a piezo Z is available, this option adds a column to the channels setup. The offset value may be specified for each channel separately.Use more lines for one channel Enables the user to add more lines (lasers/output signals) to a single channel.Don't sync power (allow same lines with different power) If one line of an (analog-controlled) illumination device is used in two or more channels, its power is synchronized by default. So if you change the setting in one channel, the corresponding lines in other channels change as well. This behavior can be disabled by selecting this option.Use non-imaging channels Apart from capturing a channel image, the triggering signal can start other actions - for example moving a piezo Z drive. Such an action is called a non-imaging channel because the action is performed but no image is captured.Use channel series (only dual camera/dual view) If you check this option, the number of camera channels and excitation channels has to be the same. The first excitation channel is used with the first camera channel, the second with the second, etc.Use these devices: Output lines and devices which can be used during triggered acquisition are listed in this place. Select the check-boxes of the devices which should be listed in the main Triggered Acquisition window.Multi-excitation + Triggered Piezo Z Perform both of the above procedures and then run the acquisition. There are two ways to create an optical configuration containing a triggered-acquisition recipe.Using the Triggered Acquisition PanelRun the    command to display the panel.Select devices for triggering and define the acquisition channels (see above). Select the Enable Triggering check box.Run    and select to include Camera setting in the configuration. Using the Optical Configurations WindowRun the    command.Create one optical configuration for each channel. Make sure the Camera setting and Microscope setting options are selected to ensure that settings of the camera and the triggered device are included. The Enable Triggering check box in the Triggered Acquisition window must not be selected.  If the configurations contain also settings of devices which cannot be triggered (e.g. certain filter wheel models), the acquisition sequence will be created anyway, but the state of the filter wheel defined in the first optical configuration will be used throughout the whole sequence.Select all optical configurations which are intended to compose the triggered acquisition sequence (hold the Ctrl key).Click the Merge to TA button to create the optical configuration. This special acquisition mode can accelerate all experiments within NIS-Elements using the NIDAQ board.Connect the camera exposure/fire TTL line to the NIDAQ TTL input (or analog input).Go to    and Add the “NIDAQ” device.The NIDAQ Configuration window opens.In the Available devices area, select “Exposure Signal TTL In” and click Add --&gt;. Figure&nbsp;1398.&nbsp;Confirm your Configuration settings. Figure&nbsp;1399.&nbsp;To enable/disable the speed up function, reveal the context menu over the NIDAQ device and click Set Components... and check/uncheck the logical device. Figure&nbsp;1400.&nbsp;Once the speed up is enabled a notification is displayed within the ND Acquisition control panel. Figure&nbsp;1401.&nbsp; NIDAQ Exposure Signal TTL In should not be used with Ti-E or Ti2-E microscopes. It is designed for microscopes which cannot use the Ti recipe. ",
     id: 205 }, 
   { title: "Defining Classes",
     xmlid: "id|p2c22s1x",
     content: " Click the Define button to start “teaching” the classifier. Select a method of evaluating the binary objects (Manual, Nearest Neighbour, Bayes, Neural Network):Manual The classification is based on the measurement feature selected in the nearby pull-down menu, and the limit values of each class shall be put to the Classes table.Nearest Neighbour, Bayes, Neural Network For these non-manual methods, become the additional options are available:Default The default set of features contains only the main object characteristics:  ,  ,  ,  ,  .Features This option modifies the list of features in the Features pull-down menu.Specify the number of classes by pressing  and  buttons.Select a class that you are going to define within the Classes table.Pick reference objects within the image using the picker .Use the histogram to define objects manually.Repeat this procedure for each class.When you click the Test button, number of objects that match each class criteria appears in the table.Reset all samples which were picked previously with the  button.Press the  button to create a separate binary layer out of each class of objects.Press the Define button again to finish the definition.Other Options Classify ND Image When working with a ND2 file, this button enables you to classify all frames. Show Scattergram This button displays a scattergram which visually displays the measured features. ",
     id: 206 }, 
   { title: "Introduction",
     xmlid: "id|p2c23s1x",
     content: " The Smart Experiment (SE) module was specifically designed for the Nikon Ji microscope with the aim of streamlining the process of well plate imaging using predefined assays. The module utilizes artificial intelligence to automatically detect well plates, a task that previously required a manual input. Additionally, it incorporates the power of Autosignal.ai, an intelligent algorithm that determines the optimal brightness values for well plates and AI-based automatic objective focus.The module comes with 6 default assays that have been established as scientifically accurate. All assays are designed to save user time and effort by automatically identifying the well plate type, cellular staining material used with each assay, by measuring only the relevant parameters and by generating a comprehensive report. This level of automation not only improves the precision and accuracy of the data, but also highly reduces the potential for user input errors. For those who require more assays, the module offers additional assays that can be purchased.Default assaysCell Counting - endpointCell Counting - proliferationCytotoxicityIntensity MeasurementSize &amp; Morphological analysisTransfection EfficiencyAdditional assaysApoptosisAutophagyCell Cycle - Fucci (CA)Cell Cycle - Fucci(SA)DNA Damage - gammaH2AXEndocytosisMicronucleus TestMitochondrial ToxicityNeurite OutgrowthNuclear TranslocationPhagocytosisWound HealingRunning NIS-Elements SE opens the application home page with the four main buttons: Figure&nbsp;1403.&nbsp;SE main buttons.Run Assay Opens the Assay layout guiding the user through the process of selecting the appropriate assay, configuring its settings, selecting the relevant wells and labeling or dosing the sample.Assay Results Shows detailed results for both the well plate and individual wells. The results view is specifically designed for each assay, ensuring that the user receives precise and relevant information for their particular experiment.Assay Settings Opens the Assay Definition window where an assay can be enabled and its parameters adjusted for a later run in the Run Assay  window. A copy of an existing assay can be created here as well.Users &amp; Groups Opens the Assay User Management window used for creating the user groups and assigning user privileges, configuring storage settings, setting up e-mail notifications and cluster computing. ",
     id: 207 }, 
   { title: "Selecting and Running an Assay",
     xmlid: "id|p2c23s2x",
     content: " To successfully run the assay, perform the following steps:Click on the Run Assay button from the home page. The Assay tab opens.Select an enabled assay. Information about the assay and used dyes is shown.Adjust the Assay Settings  in the right. Click Next.Insert the well plate into the microscope stage and fill in information about it (Name and Plate are obligatory).Click Next. The system automatically detects the well plate, locates the first well containing cells, performs a basic auto focus on it, and then scans the entire well plate using brightfield optics. An overview of all wells is captured, and the system automatically selects the wells that contain cells.Click AutoSignal Selection and select wells or confirm the current selection on which wells the Autosignal.ai will run. Confirm the selection by clicking OK. Do this step only when Fluorescence check User-defined wells was selected.Click Acquisition Selection and select the wells for capturing (Scanning phase). Confirm the selection by clicking OK.Click Labeling &amp; Dosing  click into the well plate to add a label selected on the left. More labels can be added by clicking . Color/name can be changed by clicking on the label color/name itself. Delete the label by clicking  next to it.Switch to Dosing and insert dosage numbers directly into the selected well. The dosing selection can be expanded by dragging the yellow line to automatically fill in the neighboring wells.To save the label/dosing definition into the database, click  in the top right corner and use the saving manager.Confirm the labels and dosing by clicking OK.Click Next. Autosignal.ai is executed and all channels required for the assay are captured.If Run analysis was checked in the Assay settings, the analysis is performed on the captured data and the results are shown both for the whole well plate and for the single (selected) well. Results can be inspected and exported using the automatic report. Assay has to be enabled by the administrator before it can be used in the Run Assay window.Click on the Assay Settings button and the Assay Definition window opens.Select an assay and optionally adjust its parameters.Click Save. Assay is now enabled for the user.Assay SettingsSend Progress e-mail If turned on, an e-mail notification is sent to each user in the group during and after the Scanning phase.Run analysis If turned on, analysis is executed immediately after the Scanning phase.Fluorescence check User-defined wells - Autosignal.ai finds the best brightness value for the well plate only on wells selected by the user.All wells selected for acquisition  - performs Autosignal.ai on all wells to obtain the most optimal brightness value.Last settings  - the last option selected.Continue If Auto. continue after preview step  is turned on, the application executes the Scanning phase directly after the Preview phase.Selection Scan Area Selection defines which wells are selected for the scanning phase.Anomaly detection If Use Anomaly detection is set to YES, Morphology and/or Condensation options may appear depending on the assay type. Morphology uses the Nikon AI (artificial intelligence) module to evaluate whether the cell shape is normal or anomalous and labels the wells “Morphology Anomaly”. Condensation uses Nikon AI module to detect condensation on the well bottom of each well in the well plate and labels the wells “Condensation Anomaly”. Both detections are run only on the wells selected in the Acquisition Selection using the Brightfield image.Well Plate - Options (requires:  )Specifies the well plate material.Timelapse - Options The Run Timelapse switch turns the timelapse acquisition on/off (switch accessibility depends on the assay type). Once this switch is turned on, Duration of the whole time-lapse experiment and Interval between each time point can be set. To start the timelapse immediately, turn on Immediate start or switch it off and set the exact start (Start at) by clicking on the  button for selecting the date and then enter the time in the edit boxes. ",
     id: 208 }, 
   { title: "Introduction to Tracking",
     xmlid: "id|p2c24s1x",
     content: " NIS-Elements offers different levels of object tracking functionality. Depending on what additional modules are installed (activated), there are the following tracking levels available:Tracking (requires:  )Tracking module enables object tracking in NIS-Elements. With this module object tracking is available on Regions of Interest (ROIs). ROIs are well suited for mixture of automatic and interactive tracking in cases where automation is not required and aided tracking is possible.See  .Advanced Tracking (requires:  )Advanced Tracking adds binary object tracking capability, track processing phase to the tracking process and a statistics view to the Tracking functionality. Binary object tracking is suited for tracking large number of objects (thousands of objects) where interactive editing is not practically possible.See  .3D Tracking (requires:  ) and (requires:  )3D tracking adds the ability to track 3D binary objects and show the 3D related features in the Tracking output tables as well as in Volume View and 3D Volume Measurement. ",
     id: 209 }, 
   { title: "Overview",
     xmlid: "id|p2c24s16s1x",
     content: " A complete description of the method can be found  .Intra-nuclear Single Particle Tracking (SPT) is a method of quantifying fluorescent molecules dynamics inside the nucleus or cytoplasm of living cells.Single particle tracking (SPT) enables the user to recover the movement of the molecules or diffusion characteristics from the tracks of the individual molecules. Figure&nbsp;1428.&nbsp;Alternative experiments to SPT are Florescence recovery after photo-bleaching (FRAP) and Fluorescent correlation spectroscopy (FCS). As opposed to FRAP and FCS, SPT offers:Direct observation of the movement: Reaching single molecule accuracy may reveal the heterogeneity of the fluorescent labeled molecule populationSingle molecule resolution: By locating single molecule individually, one breaks the fundamental resolution limit of microscopy imposed by diffractionUsually SPT is performed on membranes because of higher contrast (obtained by TIRF illumination) and slower diffusion in membrane (typically up to ~1µm² / s) compared to nucleus (typically up to ~10µm² / s). For tracking bright and slow moving objects, we recommend rather the binary tracking module in NIS-Elements.I-SPT uses some specific detection and analysis tools to enable tracking inside the nucleus. I-SPT unravels the mechanisms by which a functional nuclear protein finds its substrate. ",
     id: 210 }, 
   { title: "Statistics",
     xmlid: "id|p2c24s16s10x",
     content: "    This following graphics are reproduced from the following free access article: : Izeddin, Récamier, Darzacq et al., E-life, June 2014.To a user point of view, this could be extracted from the name.csv file produced by I-SPT with further integration in NIS-Elements if demand is significant. Below are the MSDs obtained by I-SPT of two molecules inside the nucleus: Dendra 2, which moves freely and H2b, a nucleosomal protein with reduced mobility: Figure&nbsp;1466.&nbsp;Here, the diffusion of Dendra2 is free with diffusion coefficient ~10µm² / s. H2b motion is sub-diffusive. The step-translocation histograms are the bin distribution of distances for a given time-lag. As explained on Tracking session, the I-SPT translocation histograms may be corrected from tracking error (white bar). Figure&nbsp;1467.&nbsp;They also can be used to determine the number of different Brownian diffusing populations in the sample: 1 (do line), 2 (dash line or free (plain line). Figure&nbsp;1468.&nbsp; An angle is defined between two successive displacements for one or several time steps. Figure&nbsp;1469.&nbsp;Plotting the distribution of the angles can give information on the geometry in the diffusion. Below are the step angle histograms of Dendra 2 with the isotropy characteristic of free diffusion: Figure&nbsp;1470.&nbsp; ",
     id: 211 }, 
   { title: "NIS-Elements configuration",
     xmlid: "id|p2c24s16s2x",
     content: "     Acquisition with PA-fluorescent moleculesAdvanced Research package (NIS-ELEMENTS AR- MQS31000)6D acquisition (NIS-A 6D- MQS42560)NIS-A RT Acquisition (MQS42780 )NIS-D Wavelength Switcher (MQS41930)Illumination sequence (LW_IS) Image analysisNIS-A Advanced 2D Tracking (MQS42900) or LW_3DTRACK 2D+3D tracking+3D Meas (LW_3DTRACK)SPT processing (LW_SPTP) I-SPT uses the localization microscopy methodology to isolate the signal of single molecules.Localization microscopy as opposed to bulk imaging uses the photo-physics of the fluorophore to sequentially illuminate single molecules and detect them.Bulk imaging: Fluorescent molecules (green) are numerous and cannot be detected individually: Figure&nbsp;1429.&nbsp;Localization microscopy: only a few molecules are emitting in the imaging channel (red) and its position can be individually accurately estimated: Figure&nbsp;1430.&nbsp;Different methods (specific buffer, photo activating UV laser, ...) can be used so that only a small sub-sample of fluorescent molecules are “on”. In the nucleus, the signal can be significantly enhanced by  : Figure&nbsp;1431.&nbsp;Any localization microscopy hardware is eligible to I-SPT module. The following protocol is dedicated to acquisition with  .Photoactivatable proteins can be switched to a new fluorescent state upon activation with UV laser. Single molecule regime with low activation power: Figure&nbsp;1432.&nbsp;Here, we show how the illumination sequence of NIS elements can be used to accurately illuminate a sample tagged with photoactivatable proteins and reach the single molecule regime required by I-SPT. We use two lasers:Activation laser needs to illuminate the sample with very low power between frames so that a fluorescent does not “pop out” during acquisition.Illumination laser needs to illuminate the sample with very high power during frame acquisition for high signal to noise ratio.Exposure time needs to be set prior to illumination sequence settings for the camera. Typical exposure time for target moving inside the nucleus is 10 ms.  Figure&nbsp;1433.&nbsp;Launch   .Assign current camera settings (ROI, frame rate).Create an image channel and drag and drop it into the phase sequence.Drag and drop excitation laser.Drag and drop activation laser. Right click Edit &gt;Custom. Activation shall start at “exposure end” and stop at “next exposure start”.Set a low level of activation at first (5%). This level will be adjusted if necessary during acquisition by double clicking on the activation cell: Figure&nbsp;1434.&nbsp; Figure&nbsp;1435.&nbsp;Set a number of loops.Check the sequence with the oscilloscope view: Figure&nbsp;1436.&nbsp;Run the sequence. ",
     id: 212 }, 
   { title: "Image analysis",
     xmlid: "id|p2c24s16s3x",
     content: " I-SPT module detects and tracks single molecules. It uniquely provides quality control tools for very short (2-3 frames) tracks.The I-SPT analysis module is integrated in the analysis explorer directory of analysis. Launch Analysis Controls &gt; Analysis explorer &gt; Create new &gt; Single Particle Tracking. Figure&nbsp;1437.&nbsp; Figure&nbsp;1438.&nbsp; ",
     id: 213 }, 
   { title: "Recipes",
     xmlid: "id|p2c24s16s4x",
     content: " Create a recipe for each experiment type. Experiment type is defined by the following characteristics:Environment for tracking (cytoplasm, nucleus)DyeActivation and illumination laser powerExposure time Figure&nbsp;1439.&nbsp;To create a new recipe select Create New.Recipes can be stored by using the save menu: Figure&nbsp;1440.&nbsp;Save Update an existing recipe.Save as Creates a new recipe.Export Export the recipe as a .spt file.Import SPT file generated on another configuration.Recipes are stored in the analysis explorer.Recipes can be modified ( Open)Recipes can be run on a single file or can be run by batch. Recipes are set in a preview preview mode. Only a subset of the movie frames is analyzed. To run the recipe on the whole movie, press Run now. ",
     id: 214 }, 
   { title: "Detection",
     xmlid: "id|p2c24s16s5x",
     content: " Detections is performed by selecting the other-the-noise pixels and setting the center of mass as their position. A specificity of I-SPT is that the parameter of detection are computed along the whole movie and not on a single frame. Figure&nbsp;1441.&nbsp;To change the default parameter for detection, ticks Advanced Settings (check the advanced I-SPT section of this document). Figure&nbsp;1442.&nbsp;I-SPT provides the relative density of detections. Relative density of detections shall always be lower than 1 / µm² / s. ",
     id: 215 }, 
   { title: "Tracking",
     xmlid: "id|p2c24s16s6x",
     content: " I-SPT sets a maximal particle displacement or cut-of radius R for the linkage of two successive detections. I-SPT algorithm is a very restrictive tracking algorithm. Figure&nbsp;1443.&nbsp;I-SPT provides a new tool to estimate the maximum particle displacement R and reduce tracking error. For further details on tracking error, check the advanced I-SPT section of this document Finding the maximum tracking displacement.Select tab Displacement, then select Calculate Histogram. I-SPT suggests the maximum displacement which maximizes the number of correct detections : number of detections minus the error (green curve): Figure&nbsp;1444.&nbsp;Put the value In the corresponding box Figure&nbsp;1445.&nbsp;or use the Set Displacement button.Click Calculate Histogram to compute another histogram. Figure&nbsp;1446.&nbsp; The number of cut traces shall never exceed 10%. If it exceeds 10% tick Advanced Settings and change the parameters for detection (see  ). The maximum particle displacement is not automatically computed. For batch analysis of a group of files, it is recommended to fix a maximum tracking displacement on a test file and to run recipes by batch. ",
     id: 216 }, 
   { title: "I-SPT Outputs",
     xmlid: "id|p2c24s16s7x",
     content: "     Quit the preview mode and run the recipe on the complete movie by clicking Run Now.Tracking on a single file creates an additional layer to the nd2 file. Layers can be be managed using Analysis Controls &gt; Layer Management Tracking binary layer. Figure&nbsp;1447.&nbsp;The quality of the tracking can be checked in the regular tracing window of NIS-Elements. Figure&nbsp;1448.&nbsp;The tracking window enables to:Select different traces and go to their corresponding frames.Display statistics such a polar graph: . Compared to other object tracking method, I-SPT tracks are expected to be short (2-6 frames). This is due to fast movement out of the 1µm focal depth of the system: Figure&nbsp;1449.&nbsp;A two frames track is displayed as a “bar” in the tracking window. Figure&nbsp;1450.&nbsp; Tracking windows does not only applies to single particle tracking but also to regular binary tracking of NIS-Elements. The button   Track Binaries will track objects using regular tracking recipe and not SPT tracking plug In. Batch processing produces I-SPT specific statistics. They include the mean square displacement (MSD).Recipes can be run on a set of movies acquired for the same experiment.In the Analysis Explorer, select  Batch.Batch processing produces two types of data file (.csv : coma separted values):One csv file per movie with the particle coordinates.One csv file per batch with the movie characteristics and statistics.Movie specific file: “Name of the movie”.csv. It reports for each movie the X and Y particle coordinate frame number and particle ID. Figure&nbsp;1451.&nbsp;Batch specific file: “Name of the movie”.csv.Stat.csv summarizes global results per file.Files shall have a low level of density (&lt;1/µm / s) and low level of cut traces (&lt;10%).It includes:File name.Total number of detections.Density.Number of traces cut due to impossible matching.Time and ensemble mean square displacement (MSD). Figure&nbsp;1452.&nbsp; MSD is the standard statistic to investigate diffusion. For each traces it computes the displacements as a function of the time lag dt: Figure&nbsp;1453.&nbsp;By plotting the average square displacement as a function of the time lag, we have an indication of the type of diffusion: Figure&nbsp;1454.&nbsp;In the case of free diffusion, the slope of the MSD is equal to the diffusion coefficient D. I-SPT tracks are too short to display their MSD. I-SPT computes a time and ensemble (all the traces) mean square displacement per file. ",
     id: 217 }, 
   { title: "Advanced tracking",
     xmlid: "id|p2c24s16s9x",
     content: "     Miss-assignments occur when:Two detections that does not belong to the same molecule are linked together.Miss-detection is inaccurately incorporated inside a trace.I-SPT estimate miss-assignment and use this estimation to rescale diffusion statistics. Figure&nbsp;1460.&nbsp; With an increasing cut-off tracking radius R, the number of possible connections (or step translocations) increases. This is in favor of a large cut-off radius for accurate statistics: Figure&nbsp;1461.&nbsp;Unfortunately the proportion of miss-assignments which quantifies the validity of the statistics also increases with the cut-off radius: Figure&nbsp;1462.&nbsp;Since miss detections depends on the molecule speed it cannot be a-piori computed. However it has a strong effect on diffusion statistics such as the step translocation histogram which quantifies the number of displacements as a function of their size. I-SPT advantageously uses the photo-physics of the fluorophore to estimate miss-assignments under the following assumptions:Molecules are permanently bleached after a given bleaching time Tb.The density of detections does not vary rapidly along the experiment.Under those assumptions, connections performed with inter frame larger than Tb is a miss-assignment: Figure&nbsp;1463.&nbsp;I-SPT therefore estimates tracking miss-assignment by computing the local density around a each detection a time labs larger than Tb: Figure&nbsp;1464.&nbsp;The I-SPT translocation histograms are the detected translocations, corrected by the tracking error. Figure&nbsp;1465.&nbsp; ",
     id: 218 }, 
   { title: "Measurement of Tracking Features",
     xmlid: "id|p2c24s6x",
     content: " A data set of object features is gathered during the tracking procedure. Common static and dynamic features of objects can be measured (e.g.: object area, object position, acceleration, velocity, ...).Description of each supported tracking feature can be found here:  .Which features are measured is determined in the Object Tracking Settings dialog window. To display it, click on the Data tab in the Tracking panel, right-click on one feature in the table and choose Settings.... Figure&nbsp;1408.&nbsp;The check boxes indicate which features are going to be measured and shown in the Data table. Figure&nbsp;1409.&nbsp;Define Reference PointThere are three movement-descriptive features which can be measured in relation to a reference point (if it has been defined). These are the Reference Length, Reference Velocity, Reference Acceleration. If an object was moving in circles around the reference point, the features will be constant / zero.Press the  Reference Point button to display reference point in the image.Position of the reference point can be changed by drag and drop.Export&nbsp;The measured data or the graph can be exported to an external file using the Export button.Please refer to the   chapter for further details. ",
     id: 219 }, 
   { title: "Graph Properties",
     xmlid: "id|p2c24s7x",
     content: " The appearance and behavior of the graph can be modified in the General Graph Properties window. Right-click inside the graph area and select the Options command from the context menu. Figure&nbsp;1410.&nbsp;Vertical axis always visible, Horizontal axis always visible If checked, the axes do not leave the graph area while zooming in the graph.Show grid The grid in the background may be or may be not displayed by selecting this option.AntiAlias Turning this option on will make the graph line edges look smooth.Display graph points (if possible) Small dots indicating the actual data values position can be displayed on the graph line. The points appear only if the distance between them is big enough for them to be recognized (they usually appear when you zoom in the graph).Interpolation method The profile line shape may be smoothed out by using an interpolation method. Select one of the three available: Quick (basic), Linear (smoother), Bicubic (really smooth).First Line Properties, Second Line Properties These settings (line width and style) are applied to the velocity and acceleration graph lines. The Color Highlighting option ensures that the two lines will differ in color. Basic graph properties such as object selecting, graph smoothing and data exporting can be performed from the context menu over the graph. ",
     id: 220 }, 
   { title: "Algorithm Overview",
     xmlid: "id|p2c24s9x",
     content: "    NIS-Elements tracking algorithm is inspired by the paper by Jaqaman, K. et al.: “Robust single-particle tracking in live-cell time-lapse sequences” published in Nature Methods 2008 Vol. 5 No. 8.The tracking algorithm has two major phases:Frame-to-frame object linking (needs Tracking module: applicable to ROIs and Binaries)Track processing (needs Advanced Tracking module: applicable to Binaries on The algorithm works with a set of active tracks. Initially - in the first frame each object is set to be an active track. Then the algorithm goes through all frames in time and tries to extend the track. An object not fitting to any track may initiate a new active track (if ignore new tracks is unchecked) at any time as well. At a given frame, each track can be linked to at most one object. If there is no suitable link a gap is introduced. In case that Gap Closing is disabled, the track is terminated and no longer active for further linking. If gaps are allowed the track remains active. It can link until the gap length reaches the threshold which terminates it.The goal of linking is to find the best correspondence between active tracks so far and objects in the current frame. A probability for each possible link (active track with object) together with a probability of track ending is calculated. The result is the solution with globally (over all possibilities: linking, gap closing, track ending and new track starting) highest probability.The probability of a link is the likelihood that the track will extend through the given object. As each track is built a Kalman filter is applied and its prediction and error estimate are used to calculate a multidimensional Gaussian probability distribution function for a given frame. From this function it is possible calculate the probability that any object links with the given track.The probability of non-linking (i.e. track termination) is given by user as a multiplication factor of Standard deviation. The lower the factor the smaller the linking area becomes and the probability of non-linking increases. The Standard Deviation is calculated for each track and time-point. With increasing object speeds the Standard deviation increases as well. This criterion is relative.Additionally, user can set an absolute maximum speed, which eliminates impossible links right away.Splitting (when enabled) is done first. Objects that split in two are determined based on selected mass conservation model: either the number of pixels or the sum intensity should be constant during the division. Remaining objects are linked normally. Some “distinguishing” object features may be selected to be included into probability calculation. Features that are selected should have the ability to distinguish between objects that form distinct tracks. Maximum change per second may be set for every feature as a restriction. Infinity means that the feature is not restricted. This phase is optional and it works on tracks from previous Frame-to-frame linking phase. In general every pair of two tracks can link (Gap Closing), branch (Splitting), merge to each other or remain independent. In this phase every of these possibilities for all possible track pairs is given a probability and a globally best combination is found.Depending on the number of tracks found in previous phase Track Processing can be a lengthy process. The complexity is greatly reduced by filtering tracks and not using all options at the same time. Enabling track processing on thousands of tracks may cause several minutes of calculations.For Gap Closing the probability of linking two tracks depends on similarity of both track characteristics, such as their speed, direction and variability of both. For Track Splitting a track start can link to any midpoint of another track and the probability calculated from mass conservation model and characteristics of involved tracks. Track Merging uses similar calculation for probability of linking a track end to a midpoint of another track.This phase can find and close additional gaps or splits that were not found in the Frame-to-frame linking phase. It may be also aided by specifying restrictions in Object features. It should be used as refinement of an already good tracking result of Frame-to-frame linking. On the other hand, It will hardly improve if Frame-to-frame phase failed. ",
     id: 221 }, 
   { title: "Integration in General Analysis 3 (GA3)",
     xmlid: "id|p2c2s4x",
     content: " NIS.ai functions can be combined with the    application. It is very easy to use already trained classification in GA3 definition. You can use many post-processing functions such as removing objects touching borders, filtering objects according to their EqDiameter, Size, Circularity, etc.Using the    method it is possible to quickly find frames where AI produced insufficient results (low Pearson coefficient) and provide more such images to retrain the AI. There are the following GA3 nodes:         Figure&nbsp;536.&nbsp;Example of Segment.ai in General Analysis 3 with post-processing.If you want to use multiple image channels with a NIS.ai node, connect it to the    or    node first.   ",
     id: 222 }, 
   { title: "Examples (version 5.42)",
     xmlid: "id|p2c2s5x",
     content: "   ",
     id: 223 }, 
   { title: "Introduction",
     xmlid: "id|p2c5s1x",
     content: " Concrete application is a system dedicated to concrete air content measurement which implements the EN 480-11 Euronorm. The concrete samples should be treated in the right way in order for the automatic detection to perform well: the air voids should be filled with white paste and each sample should be polished properly before scanning. The concrete itself should be colored so it would contrast with the paste. ",
     id: 224 }, 
   { title: "Step by step concrete measurement",
     xmlid: "id|p2c5s2x",
     content: " To run the Concrete application, run the    command. Three main buttons appear in the left panel.      Calibrate the camera if needed.See  .Adjust the camera settings (exposure time, white balance, etc.).See  .Set the size of your concrete sample in the    window.Set a suitable focusing method in the Measurement section.Initialize the motorized stage.Define (teach) the classifier by clicking the    button.Place the concrete sample onto the stage. It will be scanned several times in the X axis direction (from left to right). If the sample size is correct (150x100 mm), place the shorter edge along the X axis.Start the measurement by clicking   . If there are multiple samples to be scanned, an album is optionally created at the end of each sample (which may take a few seconds). Then you will be prompt to insert the next sample.When the whole measurement finishes, the following window appears. You can open any of the report files from here using Open. Figure&nbsp;591.&nbsp;Report dialog window. ",
     id: 225 }, 
   { title: "New Database",
     xmlid: "id|p2c6s1x",
     content: " Before you can connect to a new database the structure of tables and fields, or at least the protection level shall be set:Run   .Specify the file to be created.Select one of the database templates. Each template consists of several predefined tables. If you choose Blank, no tables will be created (later you will have to use the    command). Click Next.Select the default protection of the database. The user accounts can be based on MS Windows accounts, or arbitrary number of database user accounts protected by password can be created. Or, the database can be fully accessible for all users - if the Not Protected option is selected. Click Finish.The database connection will be automatically created and connected. You can browse the database with the  . ",
     id: 226 }, 
   { title: "New Connection",
     xmlid: "id|p2c6s2x",
     content: " If there is an existing database created by NIS-Elements, you can connect to it via the    command:Run   . A window appears.Locate the database (*.MDB) file.Confirm the action with OK.Connections to databases can be managed via the    command. ",
     id: 227 }, 
   { title: "Database Tables",
     xmlid: "id|p2c6s3x",
     content: " A new table can be added to an existing database. Every database must contain at least one table. If you have created a blank database or you would like to add a table to any of the connected databases, do as follows.Run   . A window appears.Select one of the connected databases to which the table will be added.Select one of the table templates. Table templates can be imported from the other connected databases. If you choose Blank, you will have to create the table structure from scratch (in the next step). Click Next.Define the field properties in the window that appears.Finish the table creation. ",
     id: 228 }, 
   { title: "User Permissions",
     xmlid: "id|p2c6s4x",
     content: " Database access rights can be set for individual users. As we mentioned above, there are two types of protection:Password protection Any number of database user accounts can be created. Then you can connect to a database by the user name and password.Windows account protection User permissions to databases can be set for different MS Windows users. The access to a database is then granted if only the user is currently logged in Windows. Both protection types can be combined in one database. To manage the user accounts and permissions of a connected database: You have to be connected to the database under an account with sufficient user rights.Run   . A window appears.Select the database of your interest. All user accounts of the database will appear in the list below.Create new or select an existing user account.Use the buttons on the right to manage the account properties, duplicate it (along with the permissions settings), change the password, modify permissions (Change properties), or even delete it. ",
     id: 229 }, 
   { title: "Database Backup",
     xmlid: "id|p2c6s5x",
     content: " The capability to backup the database is essential for serious work. The Database Backup Scheduler backs up the MS Access database automatically once in a precisely specified time interval, so you do not have to be afraid to loose any data. The Database Scheduler uses the standard Windows Scheduled Tasks tool (Start &gt; Control Panel &gt; Scheduled Tasks). Once you schedule a backup, there is only one condition to perform it successfully: The computer must be turned ON at the scheduled time. Configure the backup via the    command. ",
     id: 230 }, 
   { title: "Creating Reports from Database",
     xmlid: "id|p2c6s7x",
     content: " Pictures of a database together with the associated table data, or common images with the associated Image Info can be exported straight to a report.Switch NIS-Elements to Organizer by the    command.Select one or more images. These images will be inserted to the report.Click the Report button on the main toolbar. The following window appears: Figure&nbsp;595.&nbsp;In the Columns section, you can select fields, which will be included in the report. If you are exporting images from a database, the database table fields will be listed. If you are exporting images from a directory, image description items will appear.The Template portion of the dialog specifies the layout details. If you select the Standard template, images will be organized in rows and columns. It is possible to set the number of columns and rows. If you select the Custom report template, a user report template (*.rtt) can be opened and used for the report creation. If such report does not exist yet, you can create it using the Create New button.If You Pressed the Create New ButtonA wizard appears. Select number of columns and rows of the new report template. Click Next.Select the paper size, page orientation, and define margins. Click OK.An untitled report template opens containing a grid of images spaced according to the columns / rows settings.Edit the top-left cell of the image grid. You can change the text/image boxes position, size, and mapping. During report creation, all the other cells of the grid will be filled automatically according to the settings of the first one.A custom header or automatic page numbering can be added to the report. ",
     id: 231 }, 
   { title: "Proper Sampling",
     xmlid: "id|p2c7s5x",
     content: "   Before acquiring images for deconvolution, ideal sampling density has to be found. The XY Calibration and Z Step calculations are different for confocal and widefield microscopy. Ideal value for a widefield microscope is calculated as follows: Figure&nbsp;601.&nbsp;XY Calibration Figure&nbsp;602.&nbsp;Z Stepwhere λem  is the sample emission wavelength, n is the refractive index of the objective lens, and α is the objective half-aperture angle which is calculated as follows: Figure&nbsp;603.&nbsp;where NA is the numerical aperture of the objective calculated as follows: Figure&nbsp;604.&nbsp; Calculated values for XY calibration and Z step are ideal. Use this value or the closest possible value. A range from zero to 3x the ideal value (for XY calibration) or 2x the ideal value (for Z step) can be used. A warning message is shown for values higher than these limits, notifying that the deconvolution effect may be weak or ineffective. Ideal value for a confocal microscope is calculated as follows: Figure&nbsp;605.&nbsp;XY Calibration Figure&nbsp;606.&nbsp;Z Stepwhere λex  is the excitation wavelength of the light source, n is the refractive index of the objective lens, and α is the objective half-aperture angle. Calculated values for XY calibration and Z step are ideal. Use this value or the closest possible value. A range from zero to 3x the ideal value (for XY calibration) or 2x the ideal value (for Z step) can be used. A warning message is shown for values higher than these limits, notifying that the deconvolution effect may be weak or ineffective. ",
     id: 232 }, 
   { title: "Dialog Settings",
     xmlid: "id|p2c7s6x",
     content: "      ",
     id: 233 }, 
   { title: "Running Deconvolution",
     xmlid: "id|p2c7s7x",
     content: " Open the image file to be processed.Select one of the deconvolution commands and select the deconvolution method. See  .Adjust the PSF parameters, eventually use the Import PSF button and select the file containing the PSF image. See  .Adjust the parameters which are not part of PSF such as number of iterations, Z-stack support or level of noise.Select image channels which will be processed.Run deconvolution.Bit-depth conversion&nbsp;Deconvolution processing can change the range of intensity values to compensate for the higher dynamics of intensities in the resulting image. Bit-depth conversion is applied in such cases. After resetting LUTs the image can look darker than expected because the distribution of intensity values is having a different ratio.Images having bit-depth smaller than 16-bit are converted to 16-bit, while images already having 16-bits are converted to floating point image. This applies to 2D Automatic deconvolution. Other 2D deconvolution methods keep their original bit-depth. 3D deconvolution methods use bit-depth conversion (&lt;16-bit to 16-bit and 16-bit to float) if Keep Current Document  is selected. The deconvolved image is always converted to floating point image if Create New Document is selected. ",
     id: 234 }, 
   { title: "Introduction to Filters",
     xmlid: "id|p2c9s1x",
     content: "    Scanning The filter is scanned and the image is saved as a filter image database file (.flt). This file contains all information about the scanned filter. See  .Image Analysis Scanned filters can be measured, i.e. analyzed, immediately or later. This is usually done on the same computer. Optionally, to speed up the process, you can scan the images on one computer and use a different computer for measuring. See  ,  Results The results are displayed directly in the application, a report can be created easily, or the data can be exported to MS Excel. See   and  . Run the application by clicking  Filter Particle Analysis in the top toolbar or via   . Figure&nbsp;616.&nbsp;The top horizontal toolbar contains commands for controlling the camera, objective calibration, binary layer visibility and the detection preset.The right pane contains multiple docked tabs. The filter preview tab displays a preview of the filter being analyzed and enables you to open single image frames with their surroundings. The Detection Control tab defines the thresholding parameters for the filter measurement. See  . Camera settings tab and microscope control tab can be reached from this pane as well.The left toolbar contains tools in a logical order to guide you through the whole analysis procedure: adjusting illumination intensity, filter definition, scanning, measurement and report creating.  Options reveal further settings.The Filter Analysis Results panel placed at the bottom displays tabular data of the measured features. See  . ",
     id: 235 }, 
   { title: "Basic Workflows",
     xmlid: "id|p2c9s2x",
     content: "        ",
     id: 236 }, 
   { title: "Operational Precautions",
     xmlid: "id|p2c9s2s1x",
     content: " The following precautions concern using a motorized filter-scanning stage:Do not put your hands or anything else in the way of the moving parts when the device is in operation.The device contains fine electro-mechanical parts. Use the device in a clean and dry environment only. ",
     id: 237 }, 
   { title: "Advanced Functions",
     xmlid: "id|p2c9s3x",
     content: "          ",
     id: 238 }, 
   { title: "Creating a Report Template",
     xmlid: "id|p2c9s3s8x",
     content: " There is one default report template which complies the ISO 16232 standard. However you can create a custom report template.Run the    command to create an empty template.This opens the Report Generator with a new Data Source available. This Filter Analysis data source contains all the dynamic data available in the results table.Please see:   for more information about creating reports.Once the new report template is ready and saved to an *.rtt file, close the Report Generator, click   Report Input and insert the path to the new report template in the Report Template field (use ...). ",
     id: 239 }, 
   { title: "Planimetric Method",
     xmlid: "id|planimetric.methodx",
     content: "      ",
     id: 240 }, 
   { title: "Real Time Tracking Using XY Stage",
     xmlid: "id|real.time.trackingx",
     content: " (requires:  )A motorized XY stage can be used to keep the current scene - such as a living specimen - in view. This is the recommended procedure:Run live camera image and get the object of interest to the view.Click the  placed on the image tool bar and specify settings for the tracking algorithm:Track Object Given by Rectangle Lets the user to draw a rectangle around an object of interest. This part of the live image will be used to track the changes.Keep Current Pattern in Center Selects a small portion of the scene in the middle.Create ND Timelapse If selected, a timelapse image sequence is created as a background process. The sequence is opened once the tracking function ends.Channel For Tracking Select the image channel most suitable for the selected tracking method (e.g. a channel having the best contrast).Tracking Method Correlation Searches for similarities/changes in the image data. It is suitable for any scene.Threshold Searches movement by analysing auto-thresholded binary objects. It is suitable for scenes with enough contrast.Spot Detection Searches movement by analysing auto-spot-detected binary objects. It is suitable for scenes containing circular objects.Confirm the settings by the OK button and run the function by the Track XY - Keep Current Pattern In Center  button in the tool bar. If the user-defined rectangle shall be defined, the icon changes to indicate this . If the tracking algorithm fails in this mode, press Esc to exit.The function is aborted if either the user stops the tracking by the toolbar button or the scene changes rapidly so that the tracking algorithm fails. ",
     id: 241 }, 
   { title: "Assay Settings",
     xmlid: "id|se.assay.settingsx",
     content: " Opens the Assay Definition window listing all available assays. Administrator can change the enabled parameters in the right-most column for each assay. Assays are not editable by default. However a copy of an assay which can be adjusted can be created from the context menu over the assay name (Make Copy). Assay has to be enabled by the administrator (saved in this dialog) before it can be used in the Run Assay window. ",
     id: 242 }, 
   { title: "Browsing Results",
     xmlid: "id|se.resultsx",
     content: " When the Scanning phase is finished, the results are automatically opened or can be opened manually. The results layout is designed specifically for each assay and usually consists of five main parts (from left to right):Assay Explorer - result list for the current group which can be arranged by Date, by Experiment (Name field in the Insert Plate phase), by Assay or by User.Well plate viewWell plate results (graph and table)Single well previewSingle well results (graph and table)Main Toolbar Hide Hides the Assay Explorer. Home Returns to the NIS-Elements SE home page. Run Analysis This button is active only if the analysis on the current assay was not executed yet. Use it to run the analysis afterwards. Run Analysis on Cluster Processes the analysis in the dedicated cloud cluster of computers. This button is shown only when Run analysis is switched off in the assay settings and the Cluster computing is set (in the Assay User Management) and successfully connected. Adjust Analysis Parameters Some assays may have adjustable parameters which can be adjusted here. Edit Labeling &amp; Dosing Opens the Labeling and Dosing dialog used for adjusting the labeling and dosing. Repeat Acquisition with same settings Runs acquisition with the same settings all over again.Well plate view Image Each well in the well plate view is filled with the acquired image. Labeling Displays color labels for each well if the labels were defined. Dosing Displays dosing concentrations if they were defined. Heatmap Data of the selected feature is represented by a color square. Bars Data of the selected features are shown as bars. Barstack Data of the selected features are displayed as bars stacked one over the other. Linechart Data of the selected timelapse features are displayed as line charts. Boxplot Data of the selected feature are shown as a box and whisker plot. Violin Data of the selected feature are shown as violin charts. Linechart Data of the selected feature are shown as line charts (y as function of x). Color Axis Feature Selects the feature which will be visualized using a color gradient. Size Axis Feature Selects the feature which will be visualized using the size.Data Feature used for the well plate view visualization. Show numbers in wells Feature value is shown directly inside each well. Scale visible Shows a scale bar in the well plate view.Well plate resultsShows results from all measured features for the whole well plate. Dose-response Shows the   curve describing the magnitude of the response of an organism as a function of exposure to a chemical after a certain exposure time. Notes Can be used to insert additional notes related to the current well plate. Report Generates a detailed predefined report already customised for each specific assay.Fit equation Selects the features for which the dose response curve is calculated. Error Bars Visible Displays the   (graphical representations of the variability of data) in the chart. Legend Visible Displays the graph legend. Summary Shows the summary information about the measurement. Data table Shows a table with the well plate measurement data. Auto size all columns Automatically sizes the table column width. Statistics Shows the min, max and mean statistics below the table. Export Exports current objects or all objects to the selected file format. Pan Pan (move) tool for positioning the graph. Select Frame By clicking in the graph the dimension frame navigation changes accordingly to the vertical line. It also shows tooltip information about the current frame. Lasso Select Click and drag inside the graph (draw any shape) to select the data. Box Select Click and drag a rectangle in the graph to select the data. Horizontal Pan Pans (moves) the graph horizontally. Wheel Zoom Switches the mouse wheel to be used for zooming in the graph. Tap Activate this function and click inside the graph to select a data point. Hover Displays information about the data point(s) under the cursor. Reset Resets the graph. Export Exports the graph into a .png image. Copy to Clipboard Copies the graph image into clipboard.Single well previewPreview image is shown for the well selected in the top-left well plate view. Auto LUTs Adjusts the look-up-tables with the purpose to enhance the image reasonably. Context menu over this button reveals the Auto Scale Settings window where the Low and High fields determine how many of all pixels of the picture are left outside the LUTs sliders when Auto LUTs is applied (0-10%). It also adjusts the opacity of the brightfield channel (Brightfield Opacity).   Split Components Turns on a special view, where color channels of the image are displayed separately (tiled). Show Scale Displays a line scale which can be moved around the image using the mouse. Context menu over the scale enables the user to Burn Scale into the image permanently, move it to the Default Scale Position, Lock Scale Position or adjust Scale Properties (  ). Use  /  to show or hide the scale. Show Graticule Displays a graticule layer over the image for quick and approximate measurements. Graticules behave like adjustable floating rulers. User can simply align a graticule with the measured object and read the distance value. Click on the line and drag it to move the graticule. Context menu over the graticule enables the user to move the graticule to the center (Move to Center) and adjust the Graticule Properties. Use the * key shortcut to show or hide the graticule layer. Show Binary Displays or hides the binary layer in the image. Use the  Binary Layers button below the single well preview to open the binary layer opacity panel where you can adjust the color and intensity for each binary layer present in the image. Use Ctrl + Up/Down to decrease/increase the binary layer transparency. Fit to Screen Adjusts the zoom factor to view exactly the whole image as big as possible. Zoom 1:1 Adjusts the zoom factor so that one pixel of the image matches one pixel of your monitor.Zoom Select a zoom factor from the drop-down menu. Select All Frames Selects all frames of the dimension. First Frame Jumps to the first frame of the selected dimension. Previous Position (Left) Displays the previous frame of the dimension. Play Sequence Plays all images of the dimension at a defined speed. If a selection is applied, only the selected images are included in the playback. Stop Playing Stops playing the sequence at the last displayed frame. Next Position (Right) Displays the next frame of the dimension. Last Frame Jumps to the last frame of the selected dimension. Advanced Options Reveals/hides the speed controls described below. Decrease Playing Speed Changes the playback speed by one step down.Playing Speed Click in the bar to define the speed using the mouse. Increase Playing Speed Changes the playback speed by one step up. Real Time Playing Speed Sets the playing speed to real-time (as the image was captured). Maximum Playing Speed for Every Frame Sets the playing speed to maximum while the display of every frame is guaranteed (when the speed is set to maximum by the + button, some frames are usually omitted when playing the sequence depending on your graphics card).Single well resultsData are shown for the well selected in the top-left well plate view. Scatterplot Well data are shown in a scatter plot. Histogram Well data are shown in a histogram. Barchart Well data are shown in a bar chart. Linechart Well data are shown as a line chart. Fitplot Well data are shown as a fitted curve.X axis feature, Y axis feature, Z axis feature Choose a feature for each available axis (depends on the selected graph). Fit normal distribution Adds the normal trend line to the graph. Scale to fit the data Fits the data into the graph area. Auto size Automatically sizes the table column width. Statistics Shows the min, max and mean statistics for the single well right below the table. ",
     id: 243 }, 
   { title: "User Management",
     xmlid: "id|se.user.managementx",
     content: " Assay User Management works with MS Windows users and is used to create user groups and assign them user rights for the Smart Experiment. Typically, users are responsible for running assays and reviewing results, while administrators manage storage settings, e-mail notifications and can create copies of assays.The dialog window shows three main tabs: Groups, All Users and Cluster computing. The first one enables the administrator to create a group ( Add New Group ) with a given name (Description), set the Storage Path  for all the images acquired by the users in the group (or the Storage Path for users not in any group at the bottom of the window), add Group Emails  ( Add New Email ), Add users... and set their Privilege . Choose whether the analyses will be computed in the compute cluster for the selected group of users (Run analyses on cluster).Settings for the e-mail communication can be adjusted and tested in the  SMTP server settings . The Run analyses on cluster sets the analyses to be processed in the compute cluster for users not assigned to a group or when there are no groups defined.All Users tab is an overview list of users where a user can be added to the list ( Add New User) or removed from the list ( Remove User).Cluster computing tab sets the computing cluster connection parameters used for processing the analyses in the compute cluster. Figure&nbsp;1404.&nbsp;Fill in the Server name and port on which the NIS-Elements Compute Cluster is running. The default port number is 4444. Adjust the Update interval (refresh time for the NIS-Elements cluster status and analysis progress) and then click Connect and the Connection status is indicated. Please see the   for more information. ",
     id: 244 }, 
   { title: "Measurement Sequencer - Definition",
     xmlid: "id|sequencer.definitionx",
     content: " This panel is used for the preparation of measurement sequences. A set of measurement tools is arranged in five groups which enables anyone to easily define complex measurement sequences which can then be performed repeatedly by an operator. Run the    to display it. Figure&nbsp;1224.&nbsp;Measurement Sequencer - DefinitionProperties of a single step/rowTool Measurement tool icon.Parameter The name of the measured feature (e.g. Line_length_A).References Some features can be based on other - e.g. line parallel to an existing line. Select the related feature names from the pull-down menu(s).Style Color and thickness of the feature line in the image. A transparent color can be selected so that the drawn measurement object is invisible in the image.Scheme Show the feature in the drawing?Label Display a label containing the measured value in the image?Help Text Enter a short Help Text for the operator.Edit Click this  button to draw the object to the image.Feature Some tools can measure more than one feature (e.g.: distance to a circle can be measured to the center or to the edge). Specify the right option in the Feature column.Dimensions Select the box if you want to show dimension lines of the measured value in the image. Click ... to select color and thickness.Report Select Incl. to include the feature in the report ( ).Result Check Show to view the measured value and the validation result directly in the result table. Click ... to set the measurement Limits, JavaScript Condition, what happens to the validation when a measurement is skipped (Skip Measurement Behavior - see below), validation texts (Passed/Failed), Visual Warning (yellow-orange Validation field indicating the exceeded min/max limits), Conditional Action with a JavaScript Condition, message (Show Message) or a selected action (Perform Action).Skip Measurement Behavior automatically fills in the measurement value based on the settings made here when the user skips the measurement. If Pass, Fail or N/A is selected, this value is automatically assigned to the skipped measurement. The User Defined option enables the user to assign the Passed, Failed or N/A value to the measurement right in the moment of skipping the measurement or section simply by clicking on one of the three buttons.Value Displays values measured during the test run.Unit Displays the measured units.Validation Displays the validation text defined by the ... button in the Result column. It indicates whether the measured result falls within the specified range of values.Basic Tools Line Draws a simple line. Point Draws a simple point. 3 Points Arc Draws an arc by clicking 3 points on it. 3 Points Circle Draws a circle by clicking 3 points on it. Circle Draws a circle by clicking and dragging. Autodetect Circle Automatically detects a circle with its center in the center of gravity of the clicked object (confirm its detection by the secondary mouse click). The circle has the same area as the detected object. Autodetect Contour Automatically detects the contour of the clicked object. The contour can be adjusted using the mouse wheel and has to be confirmed by the secondary mouse click. Parameters of the contour can be later used in the   Custom Expression. Set of Points Draws multiple points.Using ReferenceReference object can be drawn to any existing line, point or circle. Parallel Line Draws a line parallel to an existing line. Specify the line in the References column. Perpendicular Line Draws a line perpendicular to an existing line. Specify the line in the References column. Line with Reference Point Draws a line starting in an existing point or center of a circle. Specify the point or the circle in the References column. Free Normal Line Draws a line perpendicular to an existing point or center of a circle. Specify the point or the circle in the References column. Tangential Line Draws a tangent to an existing circle. Specify the circle in the References column. Point on Line Draws a point lying on an existing line. Specify the line in the References column. Point on Circle Draws a point on an existing circle. Specify the circle in the References column. Circle from Point Draws a circle from an existing point or circle center. Specify the point or the circle in the References column. Concentric Circle Draws a concentric circle from an existing circle. Specify the circle in the References column.GeneratedGenerated object is drawn automatically based on the selected existing objects. Point in XY Generates a point from the XY coordinates. Specify the point position in the References column. Line Between Points Generates a line between two selected points or circle centers. Specify the points or the circles in the References column. Line From Point Under Angle Generates a line from an existing point/circle center under a specified angle. Specify the point and angle in the References column. Line Between Parallel Lines Generates a line between two existing parallel lines in the specified distance from the first line to towards the second one. Specify the lines and distance in the References column. Tangential Line Generates a tangent to an existing circle using a point or circle center specifying the direction of the intersection. Specify the point or the circle center in the References column. Point on Line in a Distance from Point Generates a point on the line in the distance from another point on the line. Specify the line, point/circle center and the distance in the References column. Circle Based on Center Point and Radius Generates a circle in a distance (defined by a Measurement tool) from an existing point/circle center. Specify the point/circle center and the distance in the References column.IntersectionDraws an intersection of the selected existing objects. Point by Lines Intersection Creates an intersection of two lines. Specify the lines in the References column. Point by Line and Circle or Contour Intersection Creates an intersection of a line with a circle. Specify the line, the circle and the point position in the References column. Figure&nbsp;1225.&nbsp;Point position optionsMeasurementPerforms a measurement between the selected objects. Point to Point Distance Measures the distance between two points or circle centers. Specify the points or the circles in the References column. Distance of Centers or Contours Measures the distance between the centers of two circles. Specify the circles in the References column and their center/edge relation in the Feature column. Point to Line Distance Measures the perpendicular distance between the line and a point/circle center. Specify the line and point/circle center in the References column. Parallel Lines Distance Measures the distance between two parallel lines. Specify the lines in the References column. Circle Radius Measures the circle radius or diameter. Specify the circle in the References column and the measured feature in the Feature column. Angle Measures the angle between the two lines. Specify the lines in the References column and specify the angle range in the Feature column. Arc Measures the circle arc. Specify the circle and two points/circle centers which define the arc in the References column. Also specify the angle range in the Feature column. Custom Click Define... to set any custom calculation combining features of previously defined objects, measurement results, global variables or cells from Excel. Predefined Functions and a custom JavaScript can be used as well. Figure&nbsp;1226.&nbsp;Custom ExpressionInsert Feature inserts the selected feature into the expression below. Insert Units Multiplier inserts a “_unitsMult” multiplier which is used to convert your expression to units selected in the Measurement Sequencer - Definition Units drop-down menu. By default, micrometer unit is selected (“_unitsMult” =1). User Input Enables the user to enter a number or a text string. Specify the input type in the Feature column. The input value can be used in the   Custom Expression. Execute Click the   Test button to test the defined measurement sequence on the currently opened image. To reset the measurement use the   Reset button. Explore Opens the Measurement Explorer ( ) organizing the previously defined measurement sequences (see  ). Configure Opens the Configuration dialog window with the following options:Draw Variable Parallel Lines Length dynamically changes the line length when defining lines dependent on different lines.Arrow Dimensions shows arrows for all finished Measurements.Labels Sync Colors displays the text label in the color set in the Dimension Properties on a black background. The selected color Style affects the parameter name, color of the object in the original image, in the Measurement Sequencer - Definition and Measurement Sequencer - Run and in the report.Black/White displays black label text on a white background.Draw red border if result failed in validation displays a red border around each result which is found invalid.After the definition is made, labels can be moved around to a different position so that they do not overlap. This label shift can be remembered if Remember last position if moved manually is checked.Annotations Draw/Measure new definition objects immediately after insertion replaces clicking the  button for each defined feature. If it is turned on, the user draws objects immediately after defining the feature. Force set dimension line endpoints to pixel center places the dimension lines right in the middle of the image pixels thus not resulting in sub-pixel measurements below the camera resolution.Text To Speech Speech Speed defines how fast the speech assistant reads the Help Text. Properties settings are saved together with the definition. Figure&nbsp;1227.&nbsp;Measurement sequence executed on a weld sample. These buttons move the current row up/down in the sequence definition table. Remove Deletes the selected record(s).Global ResultNone Global result is turned off.All Pass If each measurement result passes its defined limits, the global result becomes valid.JavaScript Use the ... button to add a JavaScript condition to the global result validation. It can include any combination of the defined object features and measurement results.True Defines the text shown for the measurements which pass.False Defines the text shown for the measurements which do not pass.Name Name of the measurement definition.More Options Sound Turns the object definition sound on/off.Access By default, all definitions created by a user are Shared and any other user can access them. If this option is switched to Private, only the user who created the definition can see it. The access state is saved together with the definition.If the Modify shared Sequencer definitions option is unchecked (Edit &gt; Options &gt; User rights &gt; Privileges &gt; Modify shared Sequencer definitions) shared sequences can be used by any user but cannot be edited.If the Remove shared Sequencer results option is unchecked (Edit &gt; Options &gt; User rights &gt; Privileges &gt; Remove shared Sequencer results) any results acquired on a shared definition cannot be deleted by other users but the definition creator.Units Sets the measurement units.Precision Sets the measurement precision. Show Drawing Displays a diagram showing the defined measurement. Load Current Document loads the diagram using the current image. Edit Report Reveals the report panel enabling to define a custom report ( ). Edit Name/Description Enables the user to edit the name and description of the current measurement definition. New Creates a new blank definition. Save Overwrites the current definition in the database. Save As Saves the current definition into the database with a defined name and description notes. Import Opens the saved definition from a .msd or .bin file. Export Saves the current definition into a .msd (Measurement Sequencer Definition) file. Preview Report Opens a preview of the report. Adjust the report appearance using   Edit Report.Context Menu over Step(s)Move Selection Up Moves the selected step(s) one row up.Move Selection Down Moves the selected step(s) one row down.Create Section Over Selection Creates a section from the selected steps. Once a measurement hits a section, the section can be skipped as a whole, i.e. all steps inside the section are skipped at once. Use   Skip Section in the    panel.Discard Selected Section Removes the selected section.Create Loop Over Selection Creates a loop from the selected steps. Once a measurement hits the loop, it repeats and generates new results for the looped measurement again and again until the user exits the loop (by a secondary-mouse click). This is useful e.g. when the user measures an unspecified number of objects not known in advance.Discard Selected Loop Removes the selected loop.Duplicate Selected Duplicates the selected step(s).Remove Selected Removes the selected step(s). ",
     id: 245 }, 
   { title: "Excel Template",
     xmlid: "id|sequencer.report.excelx",
     content: " Reports can be exported to a prepared MS Excel template. This template can contain some pre-filled texts, user formatting, company logo, etc., and can be saved in different file formats (.xls, .xlsx, .xlsm, .xlsb, .xlt, .xltx, .xltm, .ods).After adding a new   Excel tab using the  button, the user can load the pre-defined template by clicking on the ... button and selecting the template file.  button can be used to clear the path to the template which is shown on the left when the template is loaded.The main table in the Excel tab displays all measurement features and validation values available from the current measurement definition (Feature column) and the target cells (Cell column) which will be filled with the appropriate value in the Excel template. It can list cells in the Excel format, e.g.: “A1, B3, G7” or the range of cells “A5:C8”. Alternatively the user can set the combination of the two mentioned “A5:C8, B9:B12”. Images are automatically sized into the target cell. Timestamps are inserted into the cell respecting the time format defined by the user.When exporting the Excel template (either from the   or  ), a copy of the .xlsx file is opened from the disk, the worksheet is named according to the report name and filled in. When more reports are exported, more worksheets are appended. The system recognizes the opened Excel document and automatically appends new worksheets to it. Saving any exported Excel document is up to the user as the saving is not managed by NIS-Elements. ",
     id: 246 }, 
   { title: "Report Editing",
     xmlid: "id|sequencer.reportx",
     content: "  In the report definition area, it is possible to create a new or customize an existing report so that all measurement results are exported with the same appearance. Use the   Edit Report button in the    panel to reveal the report definition area. Figure&nbsp;1230.&nbsp;Report definition areaClick or drag and drop the elements from the left side of the report area into the definition area and freely move or resize the items on the page. Open the   Edit Mode window to position the elements precisely with the help of the blue rectangles which snap together. To set the grid size to which the elements anchor, open the context menu over the white area and select the alignment size. Context menu over a placed element reveals further settings.After a measurement sequence is run, Export to Report button fills the resulting data into the defined report.Report Editing Options The add button in the top-right corner adds another   PDF report template (described below) or   Excel template (see  ). You can switch between the templates by clicking on the tabs. A template can be duplicated or removed in the context menu over the tab. Click   Preview Report to view the template preview.Page Switches between multiple pages of one report.Paper Aspect Selects the aspect of the report page layout. When the report is generated, a similar drop-down menu appears in the top toolbar enabling to choose the output page size. The report elements are automatically arranged to fit the selected output size. Portrait Orientation Switches the report layout orientation to portrait. Landscape Orientation Switches the report layout orientation to landscape. Save to File Saves the current report settings to a .html file. Load from File Loads the report settings from a previously saved .html file. Edit Mode Opens the report definition in a separate window with real size fonts. Load Default Report Loads a default report while overwriting any previous report changes. Clear Scheme Removes all items from all report pages. Input Image Inserts the current picture. Input Table Inserts a table which will be filled with measurement data. Right-click on the table and select whether measurements or results are shown. Adjust the text, border and fill in Properties and set the Header Properties. Input Graph This input feature is used in the    application where the report node has to be connected to a graph output first. The connected graph is then visualized in the report. Supported node groups are: Graphs, Wellplate, Legacy Graphs, Legacy Ji Graphs. Input HTML This input feature is used only with the HTML Report node from    utilizing the Nikon Ji Assay. Static Text Inserts a static text. Adjust the text, border and fill in the context menu (Properties). Edit Box Inserts a text box which can be filled with any text after the report is generated. Use Properties in the context menu for any visual adjustments. Picture from File Inserts a loading image. Use the secondary mouse click on the image, select Load... and choose an image which will be loaded. Date Inserts a date or time. The time source, display format and text formatting is adjusted in the context menu over the element. Current User Reference Inserts the name of the current user. Adjust the text, border and fill in the context menu (Properties). Single Value Inserts a value selected from the context menu. Properties in the context menu adjust the text look. Sample ID This option is present in the GPA application only. It inserts the sample identification (Sample ID) entered at the start of the GPA run. Grid Layout Inserts a grid layout displayed as a table. The number of columns and rows can be changed in the context menu in Properties. Drag and drop a tool from the left toolbar onto a table cell to fill in its value. Reveal the context menu over the filled cell to adjust its parameters. ",
     id: 247 }, 
   { title: "Measurement Sequencer - Run",
     xmlid: "id|sequencer.runx",
     content: " This panel is used for executing the predefined measurement sequences. Run the    command to open it. Figure&nbsp;1228.&nbsp;Measurement Sequencer - Run Group Filter Filters the measurement based on the selected measurement group. Select Definition Select a measurement sequence definition from the list. Show Drawing Displays a diagram showing the defined measurement. Explorer Opens the Measurement Explorer ( ) organizing the previously defined measurement sequences (see  ).Hint Displays the name of the currently measured parameter together with its help text.Incl. Only the selected measurement items will be included in the measurement. To check all items, reveal the context menu and select Include All.Show Measurements Only If checked, only the measurement items are shown in the table.Units Sets the measurement units. Measure Executes the currently loaded measurement sequence. Continue Resumes the measurement sequence. Skip Skips the current measurement step. Undo Jumps one step back in the current measurement. Reset Resets the current measurement. Finish Finishes the current measurement and saves its result. Hide already measured objects Hides objects in the image which were measured so far. Sound Turns the object definition sound on/off.Result Displays the result of the current measurement. Export to Report, Export to Excel, Export to File, Export to Clipboard Exports the measurement data into a Report, Excel, File or Clipboard. Click on the black arrow next to the button to select the exporting method. Then click on the button itself to execute the selected export.When exporting using the Export to Report method, a new window opens showing the report which can be printed or saved as .pdf or .html. Select Report Template selects a template for the report being exported ( ).To export measurements into a .csv file, select Choose File..., navigate to a place where the file will be saved, name it (File name), click Save, select Export to File from the drop-down menu and click on the Export to File button to execute the export.If Export automatically is checked, the currently selected export is performed once the measurement is finished. ",
     id: 248 }, 
   { title: "Acquisition",
     xmlid: "id|task.acquisition_sectionx",
     content: "                           Acquisition &gt;  Capture Definition🔗 Defines the single-channel or multi-channel acquisition settings.Use this task to define parameters of the image captured by the    task. Figure&nbsp;1162.&nbsp;Select an optical configuration to be used to capture each channel.Please see  .Specific optionsFocus Offset Sometimes when using multiple channels for acquisition, not all channels have the same Z positions with the best focus. Using one channel as reference, user can specify focus offsets for individual channels. Therefore all channels are captured in focus automatically.Close Act. Shutter In addition user can control active shutter individually for each channel. Check Close Act. Shutter check box to minimize bleaching or uncheck it to leave the shutter opened and thus speed up the acquisition.Live Opens the live image dialog window.&gt;&gt;, Focus Offset Setup This button reveals the Focus Offset settings. If two or more Optical configurations are used (e.g. when OC's use different objectives) an offset between them can be defined. Click on the first OC, click Live to see the live view of the selected OC, focus (you can use AF for automatic focus) and set it as Reference. Click on the second OC, click Set OC to activate it (switch to its live view), use the up/down arrows to focus and click Offset to assign the offset distance from the first OC.  Acquisition &gt;  Capture🔗 Performs acquisition according to the selected   .Use this task to acquire large images and run an analysis upon capture.Requires:Task:&nbsp;   Figure&nbsp;1163.&nbsp;Name Name of your capture task.Using Select a    task set previously.Use Shading Correction Check this option to turn on the shading correction using a captured shading image. Please see    for more information.Large Image Large image is acquired as a series of overlapping images taken one by one in regular grid of given dimensions. After acquisition, images are stitched together into one big image.Enter the grid dimensions and check if you want to Use Image Registration. This function tries to find the perfect match of the overlapping parts of images, however it takes some time to compute.Automatic Shading Correction To perform the shading correction automatically without capturing the shading image, check this option and click ... to choose the type of correction which best represents the background of your sample image.Save Image to Database In this part of the Capture dialog, you can define the image saving conditions. Two options determining how to save the images to the database are available: Always or Never or Later.&gt;&gt;, Advanced Z-stack Settings When combining multiple captured images together not all frames have to be captured in the image file. For example: Capture Z-stack with Capture1 and then capture only home position with Capture2. Images from Capture2 will be missing in other then home position of the Z-stack.Use, for Missing Images in Z-stack Select Black for missing images to be black, similarly select White. Select Duplicate to duplicate captured image to all missing images in the Z-stack.  Acquisition &gt;  Capture Current OC🔗 Performs acquisition according to the currently selected Optical Configuration.    is not needed.Use this task for quick image acquisition with the currently used Optical Configuration. Figure&nbsp;1164.&nbsp;Name Name of your task.Use Shading Correction Check this option to turn on the shading correction using a captured shading image. Please see    for more information.Save Image to Database Define the image saving conditions. Two options determining how to save the images to the database are available: Always or Never or Later.  Acquisition &gt;  Capture HDR🔗 Captures a high dynamic range image according to the selected   .Use this task for a high dynamic range image capture removing reflections from an over-illuminated sample.Requires:Task:&nbsp;   Figure&nbsp;1165.&nbsp;Name Name of your task.Using Select a    task set previously.Use Shading Correction Check this option to turn on the shading correction using a captured shading image. Please see    for more information.Save Image to Database Define the image saving conditions. Two options determining how to save the images to the database are available: Always or Never or Later.Number of steps Defines the up and down steps of the HDR.  Acquisition &gt;  Capture Current OC HDR🔗 Captures a high dynamic range image according to the currently selected Optical Configuration.Use this task for quick high dynamic range image acquisition with the currently used Optical Configuration. Figure&nbsp;1166.&nbsp;Name Name of your task.Use Shading Correction Check this option to turn on the shading correction using a captured shading image. Please see    for more information.Save Image to Database Define the image saving conditions. Two options determining how to save the images to the database are available: Always or Never or Later.Number of steps Defines the up and down steps of the HDR.  Acquisition &gt;  Capture Current OC Continuous EDF🔗 Captures an extended depth of focus image according to the currently selected Optical Configuration by utilizing the continuous Z device movement.Use this task to capture an extended depth of focus fully covering a thick sample.Requires:Module:&nbsp;  Name Name of your task.Range Specifies the range in [µm] around the current Z position.Absolute Absolute Z position range can be set here.Save Image to Database Define the image saving conditions. Two options determining how to save the images to the database are available: Always or Never or Later.Image Operation Choose whether you want to capture an EDF or Max IP.Move Z to Focus Position at the End If checked, Z is moved to the focus position once the task is completed.  Acquisition &gt;  Capture Scan Area🔗 Captures the whole field of view of the Confocal Ax microscope using the specified pixel size.Instead of creating a large image, the whole FOV is captured without any XY stage movements. This approach also minimizes photo-bleaching of the sample.Requires:Module:&nbsp;  Name Name the task.Use OC Click on the ... button to select an optical configuration (e.g.   ) used for the capture. Galvano scanner must be used.Option Specifies the method for determining the pixel size.Pixel Size Set the pixel size manually using the slider or edit box below.Nyquist An optimal value for calculated from the optical resolution. The pixel size is 2.3 times smaller than the optical resolution.Optical resolution The calculated optical resolution of the microscope.  Acquisition &gt;  Capture VC🔗 Captures a Z-Stack of a bright field which is used for the volume contrast image (image with increased contrast between the sample tissues). Volume contrast image is not created automatically but can be created later using   .Use this task to capture a Z-stack for the volume contrast reconstruction. Figure&nbsp;1167.&nbsp;Name Name of your task.Use OC Click on the ... button to select an optical configuration (e.g.   ) used for the capture.Use Shading Correction Check this option to turn on the shading correction using a captured shading image. Please see    for more information.Save Image to Database Define the image saving conditions. Two options determining how to save the images to the database are available: Always or Never or Later.Z-Stack Shows the Z-Stack size to be captured. For example “3 x 0.625 µm” means that three Z planes with a step size of 0.625 µm will be captured.Wavelength Specified the wavelength for the capture.  Acquisition &gt;  Capture and Reconstruct VC🔗 Creates a single volume contrast image after capturing a Z-Stack of a bright field. The number of the Z frames and the Z step size are determined automatically depending on the condition of the objective lens and camera.Use this task to capture and reconstruct a volume contrast image. Name Name of your task.Use OC Click on the ... button to select an optical configuration (e.g.   ) used for the capture.Use Shading Correction Check this option to turn on the shading correction using a captured shading image. Please see    for more information.Save Image to Database Define the image saving conditions. Two options determining how to save the images to the database are available: Always or Never or Later.Z-Stack Shows the Z-Stack size to be captured. For example “3 x 0.625 µm” means that three Z planes with a step size of 0.625 µm will be captured.Wavelength Specified the wavelength for the capture.Background Level Defines the algorithm parameter influencing the background removal. The lower the number the more of the background is removed. Click on the  Recommended button to set the recommended value.  Acquisition &gt;  ND Acquisition🔗 Acquires an ND2 image according to the definition specified within the task.Use this task to acquire multi-dimensional images.For more information about ND2 Acquisition please see   .  Acquisition &gt;  Fast Timelapse🔗 Uses the maximum capture speed of the camera set in   to acquire a desired number of frames.Use this task whenever a high-frequency image acquisition is required. Figure&nbsp;1168.&nbsp;Select the optical configuration used for timelapse acquisition (Acquire Timelapse on). Use the   Max button to get the maximum number of frames or enter the number manually.The acquisition can be triggered by a TTL Signal or it can start Automatically. The frames are then stored into an ND2 file. Sample Storing should be faster when the Memory Only (RAM memory) option is selected.  Acquisition &gt;  Triggered Experiment🔗 (requires:  )This task aggregates all necessary features used for triggered acquisition.Use this task in the job for the fastest image acquisition possible. The camera acquisition actions are controlled solely by the hardware.Requires:Device:&nbsp;Wavelength Switcher, Piezo XY (optional), Piezo Z (optional) Figure&nbsp;1169.&nbsp; Experiment Name of your Triggered Experiment.Z Series (Large Image), Large Image (Z Series) Specify which procedure is done first and which is done later. First option creates a full Z-Stack and Large Image is created later on each Z plane. Second option creates a Large Image on the first plane, then moves to the second plane, etc.Time Check this option if you want to specify the duration of your experiment. Number of loops and time per one loop is indicated next to the combo box. Use Refresh to update the loop/duration estimate.Piezo Z stack Check this option if you want to capture a Z stack. Specify the Step size and/or a Range, use the  Suggested Step Size or enter a number of steps. Z direction can also be specified. Bidirectional acq. moves the stage in both directions from the sample, Towards sample scans in the direction to the sample and Away from sample scans the Z stack from the sample.Piezo XY / Large Image This option can be used for acquiring XY points or Large Images. For Large Images enter the number of fields or use the Full piezo range. Specify the image overlap in percent. You can set whether you want to Create Large Image or XY Multipoint. Register tries to find the perfect match of the overlapping parts of images, however it takes some time to compute. Point list can be used for capturing multi points. For more information about XY points, please see  .Optical Conf. Click on ... to select which optical configuration is used for the triggered experiment.Acquisition details Displays additional information about the adjusted experiment (e.g. used Objective, Camera FOV and Large Image FOV).  Acquisition &gt;  Illumination Sequence🔗 This task sets the camera-to-device triggering patterns.Please see   in the electronic help.Requires:Device:&nbsp;Wavelength SwitcherModule:&nbsp;    Acquisition &gt;  Live Window🔗 Displays, freezes or hides the live-signal window.Use this tool whenever a preview of what is happening under the microscope is needed. Figure&nbsp;1170.&nbsp;Action Choose one of three live actions: Show Live Window (opens a new preview window), Freeze (freezes the preview) or Close Live Window (closes the preview).Window size Depending on your preference, choose one of the three window size options: General options (exerts the last used window size), Tiled view (tiles the live window alongside with other windows) or Maximized (Maximizes the live window).Window position If using multiple monitors, this function can put your Live window onto your Second screen, keep it on the Main screen or embed it directly into the Progress dialog.  Acquisition &gt;  Merge and Store Captured Images🔗 Specifies the file format used to store images created by the connected    task. The recommended (and default) setting is ND2 (the system tries to save images from different loops to a single file if possible). If the captured data are to be processed by other software, set the format to TIFF (Single or Multi-page).Use this task to change the output file format of the data acquired by your job. Figure&nbsp;1171.&nbsp;Name Name your storage setting.Storage type Selects the file format storing images created by a capture task.Compression Compression defines the amount of compression for the selected file format.Save Color Channel Data Choose a color channel storage method - Pixel or Plane. The default is per plane. This options may be useful for advanced users who plan to further process the images in other applications (such as MatLab). You can ignore it if you use just NIS-Elements.Save OME Metadata OME metadata can be saved with the TIFF file to increase the TIFF file compatibility (e.g. proper channel designation).See also  .Generate Multi-Resolution (requires:  )Specifies whether to use multi-resolution creation per storage basis.  Acquisition &gt;  Detect Background🔗 This task can detect background from captured images.This task creates two parameters (IsBackground and IsSpecimen) which can be used with a    to analyze features only on the specimen or its background.Requires:Task:&nbsp;  ,    Figure&nbsp;1172.&nbsp;  This task requires Background/Specimen detection to be set up first. Go to:   .Name Name your detect background task.Detect Background on Choose the data for the background detection.using Channel Select on which channel the detection will be performed.  Acquisition &gt;  Save Captured Image🔗 If Save Image to Database in the    task is set to Never or Later, this task can be placed into the job to decide when you want to save the images.Use this task to save images if a condition is met or when an analysis brings satisfying data.Requires:Task:&nbsp;   Figure&nbsp;1173.&nbsp;Name Name of the task.Save Image From Select the    task from which the acquired images will be taken.  Acquisition &gt;  Capture N-SIM🔗 Captures an N-SIM image using the selected method. The captured image is composed of several image tiles.Use this task to capture structured illumination super-resolution images.Requires:Module:&nbsp;  A single N-SIM image can be created within JOBS Explorer:Open JOBS ExplorerExpand the JOB in the list to display job runs.Select and right-click the job run used to capture the N-SIM image.Select Reconstruct N-SIM Image from the contextual menu. This task is only available with the    module installed (it is a separate setup file). Figure&nbsp;1174.&nbsp;NSim mode Select the mode for capturing the N-SIM image.Optical Configuration Select an optical configuration which supports N-SIM acquisition.Focus Offset Sets focus offset - see    for description.  Acquisition &gt;  STORM Acquisition🔗 This task is used for acquisition using the STORM (Stochastic Optical Reconstruction Microscopy) method. More information can be found in a separate help file:  .Use this task to capture fast biological processes by the STORM method.Requires:Device:&nbsp;Multi-laserModule:&nbsp;    Acquisition &gt;  SPTplus Acquisition🔗 Controls the parameters necessary for proper Single Particle Tracking acquisition. For more information please see  .Acquire a timelapse image of functional molecules in the nucleus to analyze their trajectory statistics and motion.Requires:Device:&nbsp;Multilaser for STORM acquisition.  Acquisition &gt;  Assign Capture Definition🔗 Assigns a capture definition to a selected label.Capture images using different capture definitions in each well of a well plate.Requires:Task:&nbsp;   Name Name of the task.Assign Selects a capture definition to be assigned.to Selected label to which the capture definition will be assigned.  Figure&nbsp;1175.&nbsp;Example of Usage  Acquisition &gt;  Load Capture Definition🔗 Loads a capture definition from a selected label.Defines different settings for wells using the    task.Requires:Task:&nbsp;  ,   Name Name of the task.Load Capture Definition from Selects a point/well from a loop from which a capture definition is loaded. Figure&nbsp;1176.&nbsp;Example of Usage  Acquisition &gt;  Adjust Camera ROI🔗 Adjusts the camera ROI according to the light path (microscope tube aperture) or the well size. ROI according to the light path is the default ROI for all camera pads. The ROI can be fitted (Inner/Outer) in the well or optionally you can Shrink to a specified percentage value.  Acquisition &gt;  Loop over Components🔗 Loops through image channels defined by the    task.Use this task for finding Z offsets for channels or for swapping the order of the capture.over Select a capture definition (  )Select OC Activates the optical configuration assigned to the current channel.Split Storage If checked, each component is stored in a separate ND2 file. ",
     id: 249 }, 
   { title: "Autofocus + Focus Surface",
     xmlid: "id|task.autofocus.group_sectionx",
     content: "                    Autofocus + Focus Surface &gt;  Autofocus Settings🔗 Defines the Z device settings to be used by the    task.Different autofocus (AF) settings may be defined for different acquisition methods. Settings of this task have many in common with the    command.Requires:Device:&nbsp;Stage ZSelect a method you will use:Step by Step This method moves the Z drive in steps within the defined range, captures images and evaluates their focus criterion. The Z position with the best focus criterion is used as focused. Figure&nbsp;1150.&nbsp;Continuous (requires Nikon Ni or Ti2-E microscope) This method is similar to Step by Step  except that in this case, the step size depends on the camera frame rate. The camera takes pictures as fast as possible while the Z drive is moving within the defined range. Figure&nbsp;1151.&nbsp;Cells.ai (requires:  )  This option offers a fast continuous auto focus with the selected Z Device over the full Z range (Full Range or a Short Range) around the well bottom. It uses an AI-based criterion fine tuned on the typical cell lines.Name Name your autofocus setting.Z Device Choose the Z drive to be used for focusing. If you choose &lt;fastest&gt; , the Piezo Z will be used in as many cases as possible. If its range is not big enough, then the main Z device movement will be added to reach the required absolute Z.Criterion Defines which focus criterion is used:Brightfield - standard contrast based criterion - for brightfield imagesFluorescence - suitable for fluorescence microscopyConfocal - criterion based only on the intensity values. This is useful in confocal microscopy.Yeast, Bacteria (Ph) - a criterion optimized for yeasts under phase contrast (Ph) microscope.Use OC Select the optical configuration to be used for focusing.Offset Z after AF This option may be useful in fluorescence applications for the cases where the auto focus algorithm produces offset (the system states it is focused but you would like to focus on some other objects within the image, which are blurry). After performing auto focus, the Z drive will move by the distance [µm] defined within this field.Exclude Border This function can be used to exclude the border of the image and use only the central part for focus. Width of the excluded border can be set in the edit box.Move to Original Z on Failure If autofocus fails, the Z drive moves to the original Z position.Close Active Shutter during Stage Movement  The active shutter will close in times between acquisitions phases. Typically, this option is used to reduce the photo-bleaching effect on live cells.Skip Focusing on Background (requires Background/Specimen detection to be set up) Speeds up the focusing by omitting the background area. Please see:   .AF mode Auto Automatic focus mode, only the range shall be defined. The system calculates optimal step and whether 1 pass or 2 passes will be performed.Manual Manual focusing mode enabling further settings. The focusing range and step in the first and second pass can be defined manually.Range Define the total distance which the Z drive will use for auto focus.Single pass Single pass performs auto focus in one step. This is a standard auto focus method which can be used.Two passes This method is suitable when you are far out of focus. It is performed in two steps: Coarse and Fine. If the Continuous in range two-pass auto focus fails the reason may be:Camera exposure time is not compatible (too short or too long) with the speed of your Z drive.The automatically calculated coarse step is too large and the specimen is thin so it does not come into focus at all.Try reducing the auto focus speed to Slow, adjust camera the exposure time, or use the Single pass mode.  Autofocus + Focus Surface &gt;  Fast Autofocus Settings🔗 Defines the Z device settings to be used by the    task for the fastest possible focusingUse this task especially if the hardware requirements are met. It will speed up the focusing significantly.Requires:Device:&nbsp;Stage Z, Nikon Eclipse Ji or Nikon Eclipse Ti2 + a triggered camera.Range, Absolute Select a way the focusing range is specified and enter the required values.Criterion Select the focus criterion according to the type of experiment.Automatic A criterion suitable for the current light path will be selected automatically.Brightfield Uses DIA illumination.Confocal Uses a confocal microscope.Fluorescence Uses EPI illumination.Closed Aperture Cells.ai, Opened Aperture Cells.ai .ai-based brightfield criterions intended for a thin layer of cells in a PBS. Unlike the Brightfield criterion, they does not mis-detect scratches for cells. Specific hardware settings are required:Closed Aperture DIA illumination, aperture: 0.02.Opened Aperture DIA illumination, aperture: open (0.15 for 10x objective, 0.24 for 20x objective).Brightfield Criterion Select a criterion to be used if the Automatic option determines the current modality is brightfield.See  .  Autofocus + Focus Surface &gt;  Autofocus🔗 Performs autofocus using the selected Autofocus settings.Run the previously set autofocus settings on a series of slide samples which are changed manually.Requires:Task:&nbsp;   or   .Device:&nbsp;Stage Z Figure&nbsp;1152.&nbsp;Focus Name your autofocus.Using Select autofocus setting defined by the    task or the   .within Specifies within which portion of the image the autofocus is performed (e.g. within the predefined points). If Autofocus task fails to focus, captured images will be labeled with “Focus Failed” flag and will be marked by a red circle in the Labeling view. Focus Fail flag will not be assigned if Z coordinate is modified by 10 µm or more after unsuccessful autofocus.  Autofocus + Focus Surface &gt;  Autofocus Short🔗 Executes short-range autofocus with the same functionality as the button in the   . Automatic criterion for focus detection is dynamically changed based on the selected light path (modality).Focus your sample using the short-range autofocus.Requires:Device:&nbsp;Stage Z  Autofocus + Focus Surface &gt;  Autofocus Long🔗 Executes long-range autofocus with the same functionality as the button in the   . Automatic criterion for focus detection is dynamically changed based on the selected light path (modality).Focus your thick sample using the long-range autofocus.Requires:Device:&nbsp;Stage Z  Autofocus + Focus Surface &gt;  Create Focus Surface🔗 Calculates global focus surface from an existing set of Z positions. The Z positions must be defined beforehand e.g. by the    task.If your sample holder has an uneven surface, create a focus surface from a few points to ensure that all samples on the sample holder are captured in focus.Requires:Task:&nbsp;   or   Device:&nbsp;Stage ZModule:&nbsp;   Figure&nbsp;1153.&nbsp;Name Name your focus surface.On Select the source Z point set from which the surface will be calculated.Compute Z-values from Smooth interpolation surface Any position on the resulting surface is a result of interpolation.Nearest control point Z-value This is not really a surface. In any XY position, Z value of the nearest existing XY point is used.  Autofocus + Focus Surface &gt;  Move to Focus Surface🔗 Moves the Z drive to the position defined by the    task (or   ).Use this task on each XY position within a point loop to get the image into focus (or pre-focus).Requires:Device:&nbsp;Stage ZModule:&nbsp;  Name Name of your task.Preferred Z Device If multiple Z-devices are present on your microscope system, this option sets the preferred device used for the Z motion.  Autofocus + Focus Surface &gt;  Offset Focus Surface🔗 Modifies an existing Focus Surface by adding / subtracting an offset distance.Use this task on each point for pre-focus and check every N-th point whether the focus has not shifted. If it has, shift (offset) the whole focus surface to match the current focus.Requires:Device:&nbsp;Stage ZModule:&nbsp;   Figure&nbsp;1154.&nbsp; Offset by Offset your focus surface by a defined numeric value.  Autofocus + Focus Surface &gt;  Assign Current Z to Point/Well/Point Set🔗 Assigns the current Z position to the selected XY position of a sample.Use this task inside a loop after    to pre-define Z positions for fast scanning.Requires:Task:&nbsp;   or    or   Device:&nbsp;Stage Z Figure&nbsp;1155.&nbsp; Redefine Z on Select a sample holder on which you want to set the current Z position.  Autofocus + Focus Surface &gt;  Escape Z🔗 Escapes the Z drive to the lowest position.Use this task to avoid objective collision with the sample or stage.Requires:Device:&nbsp;Stage Z  Autofocus + Focus Surface &gt;  Refocus Z🔗 Uses the Z drive to perform refocus of the image.Use this task whenever the image is blurry and needs to be in focus again.Requires:Device:&nbsp;Stage Z  Autofocus + Focus Surface &gt;  Get Current Z Position🔗 Retrieves the current position of the Z device.Move to the position saved by this task later on in the job with the aid of the    task.Requires:Device:&nbsp;Stage ZName Name of the task.  Autofocus + Focus Surface &gt;  Set Focus Position🔗 Saves the current focus position.Focus the image and then save the focus position using this task. Later on in the job move to this position using the    task.Requires:Device:&nbsp;Stage ZName Name of the task.  Autofocus + Focus Surface &gt;  Move to Z Position🔗 Moves the stage to the specified Z position.Move to a previously saved Z position.Requires:Task:&nbsp;  Device:&nbsp;Stage ZName Name of the task.Z Position Position retrieved by the    task.Preferred Z Device If multiple Z-devices are present on your microscope system, this option sets the preferred device used for the Z motion.  Autofocus + Focus Surface &gt;  Move ACC🔗 Moves the objective correction collar to the position set in the ACC Preset.If image degradation occurs (due to optical aberration), this task can be used to move the correction collar to a position compensating for this problem.Requires:Device:&nbsp;Stage Z and an objective with a correction collar Figure&nbsp;1156.&nbsp;Name Name of your task.ACC Preset Preset carrying a correction collar position (see  ).  Autofocus + Focus Surface &gt;  Move to Predefined Z Position🔗 Moves the stage to the specified predefined position.Requires:Device:&nbsp;Stage ZName Name of the task.Position Predefined position.  Autofocus + Focus Surface &gt;  Autofocus using AI🔗 Performs a super-fast autofocus using a pre-trained AI network. It does not analyze a range of images so it focuses in few Z drive movements. It is trained to focus on cells with brightfield illumination.Use this task for super-fast focusing instead of the standard auto focus if the required conditions are met: opened aperture 0.15 (10x objective) or 0.24 (20x objective), the expected focusing range is less than +-250 µm.Requires:Device:&nbsp;Stage Z, Objective 10x or 20x ",
     id: 250 }, 
   { title: "Conditions",
     xmlid: "id|task.conditions_sectionx",
     content: "         Conditions &gt;  Condition (If)🔗 If the specified conditional expression is evaluated TRUE, the contained tasks will be run. Otherwise the contained tasks are skipped.Use this task during the job to decide what happens next when a defined condition is met. Figure&nbsp;1184.&nbsp;Example of an expression.After defining your If condition it is possible to add more conditions defined by different parameters. This can be done by right-clicking the task's frame and choosing Add 'Else' or Add 'Else If'. Else represents all options except those defined by the condition. If Else requires defining another condition in order to run its contained tasks. Figure&nbsp;1185.&nbsp;See also  .  Conditions &gt;  Condition (If-Else)🔗 If the specified expression is evaluated TRUE, the contained tasks will run. Otherwise the script proceeds to “Else”.Use this task during the job to decide what happens next when a defined condition is met and not met.If The same as in the    task.Else Tasks contained in the Else section will be run only if the specified conditional expression is evaluated to FALSE.See also  .  Conditions &gt;  Every nth🔗 When using repetitive tasks (e.g.: repeats, well loops, ... ), you can create a new task sequence that is triggered by every first / last / nth loop using the Every nth function. This task changes its options with respect to the type of loop it is placed in.Capture and analyse every 5th well in a well plate to speed-up the analysis as long as every 5th well is sufficient for the experiment.Requires:Task:&nbsp;  ,   ,   ,    Figure&nbsp;1186.&nbsp;Every Nth task inside a Loop over Plates Figure&nbsp;1187.&nbsp;Every Nth task inside a Loop over Points Figure&nbsp;1188.&nbsp;Every Nth task inside a Time LapseLoop Choose the loop you would like to modify by this task.Every Enter the desired number of the repeated task.First Executes after the first repetitive task selected.Last Executes after the last repetitive task selected.Include First Specifies whether the inner tasks will be executed also on the first Plate/Point/Time Loop or not. Figure&nbsp;1189.&nbsp;Every Nth task inside a Loop over WellsEvery n-th Row/Column Select this option to execute the inner tasks on selection of wells defined by a specific combination of rows and columns. Click the icons to select first, last, first and last, all wells or every n-th well in a row and a column. An Offset can be set for the “first well” option (offset 0 selects the first well, offset = 1 selects the second well and so on).Every n-th Well Select this option to execute the inner tasks only on selected wells. These are defined using the icons below, described in the task caption and shown in the preview area as red circles. Some options enable the user to enter a number further specifying the position.One Specified Well Define one well by setting the row and column on which the inner tasks will be performed. Figure&nbsp;1190.&nbsp;Every Nth task inside a Z-Stack LoopFull Z-Stack Executes the inner tasks on each plane of the Z-Stack defined by the    task.Z-Stack part Specifies the part of the Z-Stack (top, home and bottom) and how many slides are taken into account when performing the inner tasks.  Conditions &gt;  Break🔗 Exits execution of the selected loop and continues to subsequent tasks or exits the whole job.Use this task with a    to terminate a well loop when a sufficient amount of objects is captured.Break from loop Select one of the parent loops to exit. The job will continue with the task following the selected loop. If Exit Job is selected, the whole job is stopped.  Conditions &gt;  Continue🔗 While being inside a loop, this task breaks the current iteration and continues with the next one.  Conditions &gt;  Logical Expression🔗 Defines a logical expression for variables used in JOBs. For more information about working with expressions please see   in the electronic help.If a variable equals 5, then return true.You can also use this task as a parameter inside other tasks to show them or not. ",
     id: 251 }, 
   { title: "Device Control",
     xmlid: "id|task.devicecontrol_sectionx",
     content: "       Device Control &gt;  Temperature and Gas Control🔗 Sets the target temperature and gas concentration conditions.Set the temperature to be maintained inside an incubator.Requires:Device:&nbsp;IncubatorModule:&nbsp;   Figure&nbsp;1217.&nbsp; Name Name your Temperature and Gas Control task.Run in parallel When selected, the subsequent tasks will be run immediately - the system will not wait till the time-out is reached or the temperature/concentration achieved.Wait for stabilization This option overrides the Timeout settings which are ignored. The task waits until the defined temperature/concentration is reached.Set heating Check this box if you want to adjust the temperature.Target Temperature Set the target temperature of your selected device.Tolerance above, below Select the tolerance interval above and below your Target Temperature. If any temperature within this interval is reached, the system considers the task to be finished successfully.Temperature increase/decrease If this option is available, you can set the speed of heating. Three options are available: Fastest, Time to reach the Temperature and Rate of change (choose a change rate in degrees per minute). Only some devices support this feature.Time out Time out specifies the time given to reach the target temperature. If the device does not reach it in time, the job will continue nevertheless. If the temperature is reached before the time-out, the job will continue immediately.Turn off heating Once the task is used to reach a temperature, the device holds the temperature within the given tolerance. Select this option if you want to turn the heating off in another part of the job.CO2, O2 Set Concentration Set the target concentration of the selected gas.Time out Time given to the task to reach the concentration (see above).  Device Control &gt;  Analog Out🔗 This task controls the analog output devices.Control hardware accessories by sending analog signals through a NIDAQ card.Requires:Device:&nbsp;NIDAQModule:&nbsp;   Figure&nbsp;1218.&nbsp;Device Type Type of the output device.Name Analog output name.Output Output voltage set by clicking ....  Device Control &gt;  Replenish Water🔗 Replenishes water using the Water Immersion Dispenser.Use this task to replenish water by the dispenser to the tubes and onto the water immersion objective each time the water evaporates.Requires:Device:&nbsp;Water Immersion Dispenser (Nikon Ti2-E)  Device Control &gt;  Wait for I/O Event🔗 Pauses the job until a device input/output condition is met.Capture images only when the incubator temperature rises above 40°C.Requires:Device:&nbsp;NIDAQModule:&nbsp;  Name Name of the task.Device Type Type of the device sending the input/output signal.Name Name of the device in the system.Condition Defines the input/output device condition which resumes the job if it is met. ",
     id: 252 }, 
   { title: "Optical Configurations",
     xmlid: "id|task.opticalconfigurations_sectionx",
     content: "                 Optical Configurations &gt;  Select Optical Configuration🔗 Switches the system to the selected optical configuration.Use this task to make arbitrary changes on automated devices such as changing the camera settings, switching objectives, etc.Name Name of this task.Optical Configuration Choose one of your optical configurations to be activated.When the light path is switched, the current settings before the switch are stored and are set again when the user returns to the original light path. The Last Used option for Multichannel experiments means that it will use these saved light path settings. So, if you use this option for the EPI light path, it will use the current settings if the system is now on the EPI light path. If you use this option for the EPI light path but the system is, for example, on the DIA light path, then it will use the saved EPI light path settings from before the switch.  Optical Configurations &gt;  Auto Brightness🔗 Automatically adjusts the brightness settings (exposure time, gain, ... ) based on the parameters of the captured image.Benefit from the captured image and use it to automatically adjust the brightness settings for the rest of the job. Figure&nbsp;1177.&nbsp;Do Auto Brightness on Select the Optical Configuration on which auto brightness will be performed.Target Maximum Intensity Determines intensity of the lightest pixel in the resulting image. It is computed as a percentage of the camera bit-depth. For 8-bit cameras, the Target Maximum Intensity of 75% would be 255 x 0.75 ~ 191.Overillumination Tolerance Specifies number of pixels which will be ignored when computing the Target Maximum Intensity. For example, if your camera has one or more defective pixels which gives constantly the maximum value (white), these pixels shall be excluded from the calculation. Define the tolerance either as percentage of all pixels (Relative) or Absolute by specifying their number.Estimate from single Capture Estimates the values needed for Target Maximum Intensity from a single capture.  Optical Configurations &gt;  Autosignal.ai🔗 Uses AI (artificial intelligence) to find an ideal camera exposure time and illumination power on the captured image and stores the brightness settings into a Well Selection or Point Set.Place it inside the    task and than use the    to set acquisition parameters for the whole well plate.Place it inside the    to set acquisition parameters for each channel of a    task. Set OCs from Select the capture definition to which the exposure time and illumination power will be set. &lt;current settings&gt; just runs the task and does not save the result anywhere.Run on Select whether where to assign the result (e.g.: to the current well within the    task). &lt;current position&gt; just runs the task and does not save the result anywhere.Mode Live Prefers fast exposure times.Fixed Prefers image quality (longer exposure times).Max signal [%] Percentage of the available intensity range allowed in the image. Higher intensities will be considered as over-illuminated. For example, if the value is set to 50% for 8-bit images, any intensity higher than 128 is over-illuminated.Over-illuminated pixels limit [%] The acceptable percentage of over-illuminated pixels in the image.  Optical Configurations &gt;  Aggregate Autosignal.ai Results🔗 Aggregates results of the    used inside a loop and finds the best exposure and light power which is sent to the optical configuration.Is used in combination with    on    to aggregate the signal results.Requires:Task:&nbsp;  Run on The well selection for which to calculate the exposure and illumination power.Allow N over-illuminated wells Number of wells where over-illumination is acceptable. A well is over-illuminated if the Over-illuminated pixels limit [%] setting of the    is exceeded.  Optical Configurations &gt;  Set Exposure to Optical Configuration🔗 This task assigns the desired camera exposure to an optical configuration selected from the drop-down menu.Use this task in a job to quickly switch between different camera exposures on the same optical configuration.  Figure&nbsp;1178.&nbsp;Name Name of this task.Opt. Conf. Select an optical configuration to which the exposure time value will be applied.Absolute Absolute exposure value.Multiply by Relative multiplier of the existing value in an optical configuration.Variable Specify the exposure value by selecting a global value.  Optical Configurations &gt;  Adjust Camera Settings Manually in OC🔗 This task is useful for changing camera settings during a job run.Use this task in a job sequence to pop-up a maximized live window on your second monitor to comfortably adjust the camera settings. Figure&nbsp;1179.&nbsp; Name Name of this task.Optical Configuration Select optical configuration which will be used during the job run.Live Window, Position Choose where the Live window will be placed. Three options are available: Main Screen (main monitor), Second Screen (second monitor) and Progress Window (embeds into the Progress window).Live Window, Size Select the size of the Live window. General Options is the default size set in NIS-Elements options. Tiled View rearranges your opened windows together with the Live window to form a tiled layout. Maximized fully maximizes the Live window.Open Live Window automatically Check this option if you want to launch the Live window automatically. If you want to turn this window on manually, leave this check box empty and click the  Live button inside the Job Execution Progress window after running the job. Figure&nbsp;1180.&nbsp;Bottom part of the Job Execution Progress windowWhen you execute the Job, more options appear at the bottom of the Job Execution Progress window: Live Turns on the Live window as defined in the    task inside Job Definition. Freeze Freezes the Live window. Camera settings This control opens the Camera Settings dialog window. Auto Brightness Applies automatic brightness (exposure, gain, ...) on the selected optical configuration. Assign &amp; Continue Clicking this button assigns the current camera settings to the selected optical configuration and continues in the job progress.  Optical Configurations &gt;  Assign Camera Settings to Point/Well🔗 Takes camera settings from a selected optical configuration and applies it to a selected row, column, well or point.Let an analysis mark which wells are interesting to be inspected in detail and assign a prepared camera settings to them.Requires:Task:&nbsp;  ,     Figure&nbsp;1181.&nbsp;Name Name of this task.Assign Camera Settings from Select optical configuration from which the camera setting is taken for the assignment.to Select to which you want to assign the camera settings and specify the selection in the last combo box.  Optical Configurations &gt;  Interpolate Exposure between Wells🔗 This task interpolates/copies exposure values from wells with known values (e.g. assigned by   ) to wells with unknown values.When capturing fluorescence images on a well plate using dilute solutions or suspensions, fluorescence intensity may decrease linearly. Exposure time and other parameters can be adjusted only on the first and last well of the dilution range - manually using    or automatically using   . The middle section can then be interpolated using the s   task.Requires:Task:&nbsp;   Figure&nbsp;1183.&nbsp;Name Name of this task.Well Selection Select the desired    task.Optical Configuration Choose the desired Optical Configuration used.Method Choose one of the methods - Interpolate calculates interpolation between two wells with known exposure and applies the new values between these two, whereas Copy copies known values to all ongoing unknown values.Direction Specifies the direction of Interpolating/Copying. Left to Right goes from left to right on each well plate row, Top to Bottom goes from top to bottom on each well plate column.  Optical Configurations &gt;  Close Shutter🔗 Closes the shutter specified within the task. Select one of the shutters available in the system.Use this task in the job whenever a selected shutter needs to be closed.Requires:Device:&nbsp;Shutter  Optical Configurations &gt;  Open Shutter🔗 Opens the shutter specified within the task. Select one of the shutters available in the system.Use this task in the job whenever a selected shutter needs to be opened.Requires:Device:&nbsp;Shutter  Optical Configurations &gt;  Set Brightfield Acquisition🔗 Sets acquisition settings to the hardware according to the settings defined in the    window.Use this task in the job before the    task.Opened Aperture, Closed Aperture There are different values defined for each use case in the    window. Select which one will be used.Camera On multi-camera systems, one of the cameras is selected to be used for overview scanning at the top of the    window.Active Use the camera selected at the time the job is run.Overview Use the camera selected for overview scanning.Use current objective Load acquisition settings for the objective which will be active at the time the job is run.Objective Load acquisition settings for the selected 4x objective.  Optical Configurations &gt;  Set Brightfield Acquisition 4x🔗 Sets acquisition settings to the hardware according to the settings defined in the    window. This is a simplified version of the    task with the 4x objective pre-selected.Use this task in the job before the    task. Camera On multi-camera systems, one of the cameras is selected to be used for overview scanning at the top of the    window.Active Use the camera selected at the time the job is run.Overview Use the camera selected for overview scanning.Objective Load acquisition settings for the selected objective.  Optical Configurations &gt;  Select Objective🔗 Sets the nosepiece to the position of the selected objective.Place this task before the    task so that the acquisition settings and the active objective match.Objective A list of objectives currently assigned to nosepiece positions.  Optical Configurations &gt;  Select Capture Definition OC🔗 Selects an optical configuration assigned to a single channel within the    loop. If not in side the loop, it selects the first optical configuration of the linked    task.Place it inside the    loop.Capture Definition Select “CurrentLambda” of the ancestral    loop to select the OC for capturing the current channel. ",
     id: 253 }, 
   { title: "PFS",
     xmlid: "id|task.pfs.group_sectionx",
     content: "              PFS &gt;  PFS On and Focus🔗 Turns the Perfect Focus System (PFS) on and tries to find the PFS focal plane. Make sure the Z drive focal plane is not too far from the PFS focal plane otherwise the task may fail.Use this task in long-term live-cell experiments (timelapses) to solve the axial focus fluctuations in real time.Requires:Device:&nbsp;Perfect Focus System Figure&nbsp;1157.&nbsp;If you check On fail, do the following, more actions can be done when PFS fails. Auto focusing can be done using a selected    task or Z stage can be moved up/down by a specified value.  PFS &gt;  PFS Off🔗 This task switches the PFS Off.Use this task in the job to turn the PFS off whenever it is not needed.Requires:Device:&nbsp;Perfect Focus System.  PFS &gt;  PFS DM In🔗 Puts the Dichromatic Mirror (DM) in the optical path of the Perfect Focus System (PFS).Use this task for conducting experiments that require the exclusion and then returning the dichromatic mirror back in the optical path of the PFS.Requires:Device:&nbsp;Perfect Focus System.  PFS &gt;  PFS DM Out🔗 Puts the Dichromatic Mirror (DM) out of the optical path of the PFS.Use this task for conducting experiments that require the exclusion of the dichromatic mirror.Requires:Device:&nbsp;Perfect Focus System.  PFS &gt;  PFS Offset from the current z position🔗 Finds a PFS offset for the current Z position and sets it as the current offset.Use this task to ensure that exactly the same Z level will be kept when PFS is switched on.Requires:Device:&nbsp;Perfect Focus System  PFS &gt;  Auto PFS Focus Setup🔗 Defines settings to be used by the    task. Select the optical configuration to be used. Similarly to auto-focus, range can be defined in the automatic mode, range and step for single-pass or two-passes can be defined for the Manual mode.Use this task to set up the Auto PFS offset.Requires:Device:&nbsp;Perfect Focus SystemStage Z Figure&nbsp;1158.&nbsp;Select the focus criterion based on the nature of your specimen.  PFS &gt;  Auto PFS Focus🔗 The PFS system searches for the most in-focus plane and sets the PFS offset accordingly. This task uses settings defined by the    task.Use this task for creating a PFS surface.Requires:Task:&nbsp;  Device:&nbsp;Perfect Focus System, Stage ZSee   for a sample use case.  PFS &gt;  Assign PFS to Point/Well🔗 Uses the current Z position to specify PFS offset for the selected XY position (e.g. a well).Use this task inside a loop over XY points after   .Requires:Task:&nbsp;   or   Device:&nbsp;Perfect Focus System  Figure&nbsp;1159.&nbsp;Redefine PFS on Select one of your sample holder positions.See   for a sample use case.  PFS &gt;  Create PFS Surface🔗 Calculates the global PFS surface from an existing set of PFS positions. To XY positions without the PFS position defined, the PFS position of the closest XY point (which includes PFS position) is copied.Use this task for creating a global PFS surface.Requires:Task:&nbsp;   or   Device:&nbsp;Perfect Focus System, Motorized StageSee   for a sample use case. Figure&nbsp;1160.&nbsp; Compute PFS values from Smooth Interpolation Surface Any position on the resulting surface is a result of interpolation.Nearest Control Point PFS Value Value of the nearest control point position is taken.  PFS &gt;  Move to PFS Surface🔗 Changes the current PFS offset value to match the calculated PFS surface.Use this task in the job whenever it is needed to move to the PFS surface.Requires:Device:&nbsp;Perfect Focus System, Motorized Stage  PFS &gt;  Offset PFS Surface🔗 Modifies an existing PFS surface by adding / subtracting an offset distance.Use this task to modify the current PFS surface by a specified value.Requires:Device:&nbsp;Perfect Focus System, Motorized Stage Figure&nbsp;1161.&nbsp; ",
     id: 254 }, 
   { title: "ROIs",
     xmlid: "id|task.rois_sectionx",
     content: "      ROIs &gt;  Define ROI🔗 This task defines ROIs on the live image using the specified optical configuration.Draw ROIs inside the live image and stimulate them using the connected stimulation device. Figure&nbsp;1148.&nbsp;Task Name Name of the Define Stimulation ROI task.Create New ROI Definition Creates a new Stimulation ROI Definition.Redefine ROI from Redefines the existing Stimulation ROI Definition from the selected point/well loop/ROI from analysis.Use OC Specify the optical configuration which will be used for defining the ROIs.Draw ROI(s) This button runs the Live view and opens the Simple ROI editor (see:   ).Define ROI(s) in Runtime If checked, this feature enables the user to define ROIs during runtime.Use Last Frame from Preview If checked, the last frame from the preview is used for the ROI definition.Clear ROI(s) before Definition Any previously drawn stimulation ROIs (drawn by clicking Draw ROI(s) or during Runtime) are cleared if this option is checked.Define Laser Powers If checked, Laser Power Setting dialog window appears when drawing stimulation ROIs. This window defines the power of each laser line on each stimulation group. Each point from a MultiPoint experiment can carry a different definition of ROIs or laser lines and laser powers. To assign a stimulation ROI using DMD / Galvo XY to a preset, please register the preset using the DMD / Galvo XY Pad in advance.  ROIs &gt;  Use ROI from Analysis🔗 This task takes binaries created in analyses and applies them as stimulation ROIs during stimulation experiments.Measure features of the cells using the cell count analysis and stimulate them later on in the job.Requires:Task:&nbsp;  ,   ,   ,   ,   ,   Task Name Name of this task.Load from Analysis Select the analysis task from which the binaries will be taken and used as stimulation ROIs.Auto Focus Defines auto focus on the selected ROI.Stimulation Lists the stimulation ROIs to which a stimulation configuration can be assigned using the ... button.  ROIs &gt;  Assign ROI to Point/Well🔗 This task assigns the stimulation ROI defined in the    to a point of a point loop or a single well/row/column of a well loop.Draw ROIS on a single well and assign them to different wells on a well plate which will be stimulated later on in the job.Requires:Task:&nbsp;  ,    or    Figure&nbsp;1149.&nbsp;Assign ROI definition Select the desired    or    definition.to Choose to which element you want to assign the ROI. ",
     id: 255 }, 
   { title: "Sample Holder",
     xmlid: "id|task.sampleholder_sectionx",
     content: "            Sample Holder &gt;  Define Slide🔗 Specifies the dimensions of a slide including its working area.Used whenever slide is used as a sample holder.Requires:Module:&nbsp;  If you use a standardized slide, click Select from DB... and search for your slide in the slide database.For non-standard slides, click Custom Slide... and define its properties (width, height). If you use slides with alignment crosses, it is recommended to check Alignment Crosses and set their X and Y Offset for later precise alignment. Figure&nbsp;1100.&nbsp;Once you have selected/defined your slide, you have to specify its orientation on the stage. The orientation can be defined by Label Area Position, Smear Direction or Crosses Position. Select the real Orientation of the slide from the combo box. Figure&nbsp;1101.&nbsp;At last it is possible to limit working area of the slide. This area can be a Rectangle or a Circle defined by Dimensions or using the Interactive feature which requires to Add XY points lying on the border of the selected shape (3 corner points for the rectangle or 3 edge points for the circle).After defining the slide it is ready for alignment. Please see:   .  Sample Holder &gt;  Define Stage Area🔗 In this task you can specify the area of interest on your stage holder.Used to define a non-standard sample holder.Requires:Module:&nbsp;   Figure&nbsp;1102.&nbsp;Name Type the name of your stage area task.Shape Choose a stage area shape for its definition. Then click the arrows on the right side one after another while moving the stage to define the edge points. A small preview image is assigned to each defined point. Define without Capturing sets the scanned area shape without capturing images of the definition points, therefore no preview images are shown. Reset can be used to remove all defined points and start all over again.Some shapes do not use edge buttons but a definition table - in such case use Add to enter the definition points.Circle by radius shape requires to enter a Radius value and to Set Center Position.  Sample Holder &gt;  Align Slide Holder🔗 Determines exact position of the selected slide on the XY stage.Used each time the slide is changed on the stage.Requires:Task:&nbsp;  Module:&nbsp;  Select a slide from the combo box, move your stage to one corner of your slide and click Add. Move the stage to the Opposite Corner and click Add. Alignment is now finished. Changes to the alignment can be done by removing the XY coordinates clicking Remove or Remove All. Figure&nbsp;1103.&nbsp; Name Name of the Align Slide task.Slide Select slide for alignment.Add Uses the stage coordinates as corner points used for alignment of the slide.Remove Removes the selected coordinate.Remove All Removes all coordinates used for slide alignment. Slides can also be aligned globally using   . This alignment is saved into the database and applied on new slides automatically.Slides with covered corners&nbsp;If your slide is mounted to the stage in a manner that its corner(s) are not visible in the Live View, it is impossible to align it using the predefined slides from the database. In this scenario there are two options:Create a custom slide (using   ) with Alignment Crosses and use these crosses to target corners of the coverslip, visible corners of the slide or any other reference points.Use    with a rectangular shape and find corners of the coverslip, visible corners of the slide or any other visible reference points.  Sample Holder &gt;  Slide Loader🔗 Communicates with the slide loader connected to your system.Put this task to wizard to initialize the slide loader and scan hotels for slides just before the experiment begins.Requires:Device:&nbsp;Slide loaderModule:&nbsp;  Click   Initialize and follow the instructions to initialize your slide loader.Scan for the loader hotels by clicking   Scan Hotels.To change the slide model used in the slide loader, click   Select slide....Select or deselect the found slides and hotels to be used in your job.Optionally add the name and barcode to each of your slides and add Notes for the hotel. Figure&nbsp;1104.&nbsp; Dialog Window OptionsSlide loader name Name of the task. Initialize (Re)initializes the connected slide loader. Scan Hotels Scans the hotels of the connected slide loader for all available slides. Refresh Refreshes the slide loader. Select slide Opens a dialog window defining the slide model used in the slide loader.  Hotel tabs Each hotel tab represents a hotel of the connected slide loader. Slides contained in the selected hotel are listed below. The slides and hotels with a selected check box are used throughout the job. Select All Selects all slides in the hotel. Clear Selection Deselects any checked slides in the hotel.Slide # Position of the slide in the hotel.Name Name of the slide.Barcode Barcode of the slide.Notes for More notes for the selected slide.  Sample Holder &gt;  Manual Slide List🔗 Specifies the list of slides to be used in a loop over slides. These slides have to be changed manually.Assign names and barcodes to slides in an ordered list. This information will be included in the experiment results.Requires:Task:&nbsp;  Module:&nbsp;   Figure&nbsp;1105.&nbsp; Manual Slide List Name Name of your manual slide list.Slide Definition Select the slide definition (  ).Multi Slide Holder If using a multi slide holder, you can adjust the slide XY offset from the first slide in the table.First Slide is Already on Stage Check this option if your first slide is already on stage. Add New Slide Adds a new slide into the table. Remove Removes the selected slides. Remove All Removes all slides from the table.Slide # Automatic sequence number of your slides.Name Name of your single slides.Barcode Here you can enter your own barcoding for each slide.X Offset from Slide #1 X axis offset from slide 1.Y Offset from Slide #1 Y axis offset from slide 1.Notes for In the blank window below the slide ordered list you can fill out any other information about each of your currently selected slides.  Sample Holder &gt;  Loop over Slides🔗 Runs the contained tasks on each slide of the specified slide list.Analyze all slides in the list and perform high-magnification capture on the ones meeting the defined condition.Requires:Task:&nbsp;   or   Module:&nbsp;   Figure&nbsp;1106.&nbsp;Loop Name your slide loop.over Choose a slide list from your current job.  Sample Holder &gt;  Advanced Loop over Slides🔗 This task can be used to fully control the procedure when working with a slide loader. Objective clearance (escape    and refocus   ) has to be respected, otherwise the loop fails. The slides are loaded (  ) and unloaded (  ) manually using the tasks.Typically the job runs through all slides in the loader and assigns some information to them. For example, an analysis (  ) can determine which slides are worth a high resolution capture - all slides are captured with low resolution and only those marked by the analysis are captured in high resolution.Requires:Task:&nbsp;  Module:&nbsp;   Figure&nbsp;1107.&nbsp;Manual slide loop procedure respecting the objective clearance, similar to what the    task does automatically.Loop Name of the task.over Defines the slide loader containing slides over which the inner tasks will be executed.  Sample Holder &gt;  Load Slide to Stage🔗 This task loads the current slide from a slide loop onto the stage.See    example of usage.Requires:Task:&nbsp;  ,   Device:&nbsp;Slide LoaderModule:&nbsp;  Name Name of the task.Slide Selects a slide loop.  Sample Holder &gt;  Unload Slide from Stage🔗 This task unloads the current slide from a slide loop from the stage.See    example of usage.Requires:Task:&nbsp;  ,   Device:&nbsp;Slide LoaderModule:&nbsp;  Name Name of the task. ",
     id: 256 }, 
   { title: "Stage XY Points",
     xmlid: "id|task.stagexy_sectionx",
     content: "                        Stage XY Points &gt;  Generate Points🔗 Creates a pattern of XY points on the selected sample.Used with quantitative/statistical methods to detect objects in a large number of “randomly” scanned frames.Requires:Task:&nbsp;   or    or   Module:&nbsp;   Figure&nbsp;1111.&nbsp;Main settings of this task are described below, however some settings allow for advanced adjustments (e.g. size, distance, count, ...). These functions are self-explanatory and are clearly depicted in the preview area.Point Set Name Type the name of your point set.On Select on which sample holder to generate the points.Working area Define the working area of the sample holder.Field Size Actual at Run-time  uses the field of the currently used objective. Points are generated using the objective set right before the job run. Custom can be used to select a specific objective from the combo box which is then always used for the point generation. If a    task is defined, its field size can be selected here.Customize Preview Field Size If checked, an objective can be selected from the drop-down menu defining the used field size.Objective Objective to be used for generating points.Camera If multiple cameras are connected, select the one to be used with this task.Area Restriction Specify the restriction inside the working area.Point Placement Choose one of the point placement methods listed. Manual method enables the user to add points by clicking into the preview area. Created points can be moved using the primary mouse button. Right-clicking deletes the point below the cursor.Scan Direction If Point Placement is set to Random, Random + Center or Manual, Random scan direction scans in the randomized order whereas Optimal Path scans points in the most effective order.If Covering or Regular Pattern point placement is selected, Meander scans in a winding manner and Left to Right scans row by row from left to right.If Point Placement is set to Spiral, options in the Scan Direction combo box specify the direction of rotation - clockwise (CW) / counterclockwise (CCW) and heading from/to the center (Outwards / Inwards).Frames on Border Specify whether the frames on the border of the restriction area are touching the border, completely inside or having a center inside.Overlap In Covering mode you can select the frame Overlap amount in percent.For well plate loops, check the Auto option to set the optimal overlap to cover the rectangle inside the well automatically. To maximize the coverage to the edge of the well, combine the Auto option with the Frame inside option.Count Set the number of points you want to generate. The maximum number is indicated in the bracket.Distribution Uniform in Area places points evenly and Uniform in Radius places most of the points into the center of the restriction area.Always Create New Points There is always a new point set generated whenever the task is performed.Randomize Regenerates your random points.  Stage XY Points &gt;  Predefined Points🔗 Specifies a list of XY points on the selected sample manually in a tabular way.Used for fixed samples. Find interesting scenes with a joystick and add them to the table one-by-one.Requires:Task:&nbsp;   or    or   Module:&nbsp;  Please see   for details about points predefining. There are some additional options available within the JOBs module:Include Z-Stack Displays the Z-Stack definition column with the  Z-stack definition button enabling to define a Z-Stack for the currently selected point.Include Camera Settings Displays the Camera column with the  Redefine Camera Setting button enabling to adjust the camera settings (please see:   ).Include ROIs Definition Displays the ROIs column with the  button. Click this button and draw ROI(s) for the selected multi-point.Include Labels Labels defined in    task can be assigned to each multi-point. Select a label from the pull-down menu in the Label column.Define Stimulation Laser Powers If checked a dialog window appears when drawing stimulation ROIs. This window defines the power of each laser line on each stimulation group. Each point from a MultiPoint experiment can carry a different definition of ROIs or laser lines and laser powers.Clear ROI(s) before Definition Any previously drawn stimulation ROIs (drawn by clicking Draw ROI(s) or during Runtime) are cleared if this option is checked. The maximum number of generated points is limited to 30 000. For random points this limit is set to 1500. If you need to change this limit, run a job with the    task, then open the Windows Registry Editor, search (Ctrl + F) one of the following functions: {% raw %}GeneratePoints_MaxPointCount{% endraw %}{% raw %}GeneratePoints_MaxRandomPointCount{% endraw %}and modify the “esrData” decimal value to a new value. Then click OK.  Stage XY Points &gt;  New Point Set🔗 Creates an empty point set. Points shall be added to it later either by    or   .While looping an existing generated point set, only interesting scenes are added to the empty point set for later use.Requires:Module:&nbsp;   Figure&nbsp;1112.&nbsp; Point Set Name your point set.  Stage XY Points &gt;  Loop over Points🔗 Runs the contained tasks on each XY point of the selected point set.Used for capturing XY(Z) nd2 files.Requires:Task:&nbsp;   or    or   Module:&nbsp;   Figure&nbsp;1113.&nbsp; Loop Name your point loop.over Select one of your active point tasks to be looped (  ,   ,   ).Split storage per point This option influences how the system stores the captured data on the disk. If checked, separate ND2 file will be created for each point.Move to XY If checked, the stage is moved to the XY points.Move to Z If checked, the Z-drive moves to the Z position assigned to the current well (e.g. Z value found by    during the first   ).Advanced Each point can be assigned to a class (by an expression). If you select this option, the Loop over wells in class field appears. Type the class number to which the current loop will be applied.Use Point PFS Offset These options tells the system to set Z drive to a position previously assigned to the current point.Use Points with If you select a label defined by the    task, the tasks inside Loop over Points will be executed only on points marked with this label.  Stage XY Points &gt;  Move to Point🔗 This task can be used inside a point loop to move to a XY/Z position of a point.If a condition is met, move the motorized stage to a specific point.Requires:Task:&nbsp;  Module:&nbsp;   Figure&nbsp;1114.&nbsp;Move to Defines in which point loop the movement will be performed.Move to XY Moves to the XY coordinates of the point.Move to Z Moves to the Z coordinate of the point.Use Well PFS Offset PFS Offset of the well is incorporated into the movement.  Stage XY Points &gt;  Move to Previous/Next Point🔗 Moves the XY stage to the previous/next well.When placed in runtime, the user can move to the previous/next point to check the procedure.Requires:Task:&nbsp;  Module:&nbsp;   Figure&nbsp;1115.&nbsp; Task name Name of your task.Move from Select the point from which you want to move.to Specify the moving distance to the previous/next point.Previous, Next Choose the point direction you want to move.  Stage XY Points &gt;  Offset Point Set🔗 This task can correct the XY offset between objectives.Used to offset all XYZ coordinates of the current stage position or of the selected point set.Requires:Module:&nbsp;   Figure&nbsp;1116.&nbsp; X, Y, Z shift Name of the task.of Is used to select a point set (  ,   ,   ) or the Actual Stage Position  to be shifted.Correct XY Offset between Objectives If checked, XY correction between objectives is performed using the two objectives specified below. Please see    for more information.Define Offset Manually Specify distances of the manual shift in all three axes (X, Y, Z).  Stage XY Points &gt;  Add Point to Point Set🔗 Adds a named XY point to the selected point set. Current XY position is used.Pause the job to manually find an interesting XY position using the joystick and include this point in the analysis.Requires:Task:&nbsp;  Module:&nbsp;   Figure&nbsp;1117.&nbsp;Name Name your new point.Add current stage point to Select a point set to which you want to add a new stage point.  Stage XY Points &gt;  Remove Point from Point Set🔗 Removes the current XY point from the selected point set which is used in a point loop.Use this task in an XY point loop to remove points which are not useful for further analysis (e.g. do not produce any image signal).Requires:Task:&nbsp;  Module:&nbsp;   Figure&nbsp;1118.&nbsp;Remove Point from Point Set Select an active point loop from which your point will be removed.  Stage XY Points &gt;  Add/Edit Points Manually🔗 Waits for the user to add or edit points to the selected point set manually during the runtime.May be used to check focus and offset the point set if needed.Requires:Task:&nbsp;  Module:&nbsp;   Figure&nbsp;1119.&nbsp;Name Enter the name of your manually defined points.Embed to progress window Check this function if you want to embed the Edit Points window into the bottom of the Job Execution Progress window.Point set Select a point set which you want to edit manually.  Stage XY Points &gt;  Append Point Set to Point Set🔗 Appends points from a selected point list into another selected point list.Append interesting points to a list on which a high magnification capture will be made.Requires:Task:&nbsp;   or   Module:&nbsp;   Figure&nbsp;1120.&nbsp;Name Name of your task.Append all points from All points from this list will be appended.to Defines to which point list will the points selected from the first combo box be appended.  Stage XY Points &gt;  Clear Point Set🔗 Removes all points from the selected point list.If the user decides to clear the point list e.g. because of an incorrect sample.Requires:Task:&nbsp;   or   Module:&nbsp;   Figure&nbsp;1121.&nbsp;Name Name of your task.Remove all points from Select the plate list which will be cleared.  Stage XY Points &gt;  Export Point Set to ND🔗 Exports the selected point set definition to the ND Acquisition window.Find points of interest using a complex job and use them later in ND Acquisition.Requires:Task:&nbsp;   or   Module:&nbsp;   Figure&nbsp;1122.&nbsp;Point set to export Select the point set you want to export to the XY tab of the ND Acquisition window.Append points Adds new generated points to the the XY tab of the ND Acquisition window right behind already existing points.Replace points Replaces all existing points in the XY tab of the ND Acquisition window by newly generated points.  Stage XY Points &gt;  Reorder Point Set🔗 Reorders the input point set to minimize the distance between the points.Use this task to optimize the point trajectory.Requires:Task:&nbsp;   or    or   Module:&nbsp;    Stage XY Points &gt;  Import Point Set from ND🔗 Imports the multi point definition from the ND Acquisition window to the selected point set. The existing point set settings will be overwritten.Use the multi points created in ND Acquisition to run a complex job.Requires:Task:&nbsp;   or   Module:&nbsp;   Figure&nbsp;1123.&nbsp; Import to point set Select a point set to which data from the ND Acquisition window will be imported.Append points Adds newly generated points to your selected point set behind already existing points.Replace points Replaces all existing points in your point set by newly generated points.  Stage XY Points &gt;  Get Current XY Position🔗 Retrieves the current XY position of the stage device.Use the retrieved XY value to move the stage to this position later on in the job.Requires:Module:&nbsp;    Stage XY Points &gt;  Move to XY Position🔗 Moves the stage to the previously remembered position.Use the retrieved XY value to move the stage to this position later on in the job.Requires:Task:&nbsp;  Module:&nbsp;    Stage XY Points &gt;  Set XY Speed/Accuracy🔗 Sets the stage speed so that different speeds can be used throughout the job.Use a slower stage speed to prevent the sample shift and get stability for the PFS during acquisition. Then use faster speed when changing samples to reduce the total acquisition time.Requires:Module:&nbsp;    Figure&nbsp;1124.&nbsp; Name Name of your task.XY Speed Select the check box and enter a new speed value of the stage. Otherwise the default value will be used.Accuracy Select the check box and select a different value to be used. Better accuracy (lower value) usually results in longer movement times. If you select the Open value, the stage position will not be corrected at all. It is the fastest but least precise option.Restore original values when the job finishes If selected, default speed and accuracy values are set after the job is finished.  Stage XY Points &gt;  Import Area From Sample Navigation🔗 Imports “Area” data set in the    panel.  Stage XY Points &gt;  Move to Loading Position🔗 (requires:  )Moves the XY stage to the loading position.  Stage XY Points &gt;  Move to Acquisition Position🔗 (requires:  )Moves the XY stage to the acquisition position. ",
     id: 257 }, 
   { title: "Stimulation",
     xmlid: "id|task.stimulation_sectionx",
     content: "     Stimulation &gt;  Sequential Stimulation🔗 Chains acquisition time-phases and stimulation / bleaching / waiting phases. For more information about Sequential Stimulation please see   .Use this task to define a sequential experiment combining stimulating, bleaching and waiting phases.Requires:Device:&nbsp;Stimulation device  Stimulation &gt;  Simultaneous Stimulation🔗 Runs a time-lapse acquisition and stimulation simultaneously.  For more information about Simultaneous Stimulation please see   .  Use this task to stimulate the sample and capture a time-lapse at the same time.Requires:Device:&nbsp;Stimulation device ",
     id: 258 }, 
   { title: "System",
     xmlid: "id|task.system_sectionx",
     content: "                            System &gt;  Question🔗 Displays a pop-up window requiring user interaction. After “answering” by clicking one of the buttons, the job continues.Use this task to stop the job sequence and let the user decide what to do next. Please see   in the electronic help for detailed examples.  Figure&nbsp;1191.&nbsp; Name Name your question.Show Preview... Displays the preview window showing how the defined question will look like.Icon Choose an icon which will be displayed together with your question. Four graphics are available: Information, Question, Exclamation and Error.Caption Type a text to be used in the dialog window heading.Wait for user action Stops the job sequence until the user answers the question. If the Keep visible during current loop execution option is selected, the question stays visible during the loop execution even after user input.Timeout (for ENTER key) If there is no interaction from the user in the specified time, the question is confirmed and the job automatically continues.Continue execution and stay visible If checking this option, the question shows up and the job continues even without user interaction.Input Values Check if you want to input any user data (e.g. defined in   ). Click the  Add New button and select the variable. Name of the selected variable can be set in the Text field. To change the variable, click ....Buttons Combination Presets Here you can choose some predefined button combinations.Label Define the button names to be displayed when the question is to be answered.Escape Key, Enter Key Select which buttons will correspond to Esc and Enter key-strokes. Add New Adds a new button. Remove the button Removes the selected button. Remove All Removes all buttons.For more information about using Questions please see:  .  System &gt;  Macro🔗 This task enables the user to write and run a macro. The whole set of available macro commands can be used.Write sophisticated C-like scripts for tasks which cannot be done by a Job or reuse your existing macro. Please see   in the electronic help.Using Job Parameters within the Macro Task&nbsp; Figure&nbsp;1192.&nbsp;Macro job example 1This example job generates a point set (pattern of points on a well plate) without moving the stage. The embedded macro fills the point set with x and y coordinates of the center of each well from the well selection. Obtained points can be used later in a different job for a fast acquisition. Figure&nbsp;1193.&nbsp;Macro job example 2This example job has similar functionality as the    task following the object of interest and keeping it in the center of the camera FOV. In this specific job, three manually selected objects are followed for an amount of time set in the   . The first macro defines the variables and declares objective calibration. Then the three points are manually marked using the    task.    is performed once and thresholding defined in the    is used for identifying the objects of interest. The second macro sets the objects initial position into the predefined variables. The third macro finds the current x and y position of the center of the object and shifts the stage to these coordinates. Then it updates the objects position in the point set with the current position.Another example of using a macro job can be found here: {% raw %} {% endraw %}.  System &gt;  PythonScript🔗 This task enables users to write scripts in the Python language in places where the one-line    task does not provide enough room for a more sophisticated code. The task executes the Python function “run” defined in the python script. User can access the Job and Macro parameters, captured images and import and use many python libraries (which can be installed to the NIS-Elements Python folder).User can define parameters of the task (NewParam) that are visible to other tasks in the job and to itself during the run. The parameters are initialized to specified values before the job is executed.After the code is modified click Apply. The code gets evaluated and any errors are displayed in the box below the editor.The Python code must:import limjob anddefine a run function (with this exact signature) at the module level.{% raw %}import limjob\n\ndef run(imgs: tuple[limjob.Image], Job: limjob.JobParam, macro: limjob.MacroParam, ctx: limjob.RunContext):\n    pass\n{% endraw %}Inputsimgs: tuple[limjob.Image] Access image defined in Input Images, index is given by “#”.Call method array() to get data (read only) as 4D numpy.ndarray (Z, Y, X, Component).Job: limjob.JobParam Access Job parameters, you can edit simple variables (numbers, strings).Same syntax as in the    task.macro: limjob.MacroParam – access global variables defined via NIS-Elements macro language.Same as Variables in the    task.ctx: limjob.RunContext Currently not implemented.Input Images Add new image by clicking on “+”, then select image in drop-down menu. You can select any image from task before in list. To access image and image data in python, see variable “imgs” in function “run” definition in python code. Currently image access is limited by loop. To access image put Python task and task generating image to same loop level.Task Parameters Add new parameter by clicking on “+”, then define name, type and initial value of parameter. Initial value is evaluated as python code. For example, for initializing float array you can write both [0, 1, 2, 3, 4], and range(0,5). Parameters are reinitialized on JOB program start. Parameters of this task can be accessed for read and write by this and by other tasks (e.g Expression). To edit array in python, you have to set whole array.{% raw %}Job.PythonScript.NewParam = (0, 1, 2, 3, 4){% endraw %}DeviceManager API There are currently implemented few “global” functions which can be called inside function run. Please note, that this API is experimental and may be changed completely in next version. Parameters and returned values are in micrometres.{% raw %}XY_GetPosition() -&gt; Tuple[float, float]\nXY_Move(x: float, y: float) -&gt; None\nXY_MoveRelative(x: float, y: float) -&gt; None\nZ_GetPosition() -&gt; float\nZ_Move(z: float) -&gt; None\nZ_MoveRelative(z: float) -&gt; None\n{% endraw %}PointSet limjob.PointSetParam class has dedicated methods for simple insertion of points.{% raw %}append(x: float|list[float], y: float|list[float], z: float|list[float] = None) -&gt; None\nset(x: float|list[float], y: float|list[float], z: float|list[float] = None) -&gt; None\n{% endraw %}Image limjob.Image class has these methods and parameters.{% raw %}componentCount: int\nbitsPerComponent: int\nsize: tuple[int, int, int]\ncalibration: tuple[float, float, float]\nalignment: int\ncalibrated: tuple[bool, bool, bool]\nunits: tuple[str, str, str]\ntransformPxToStage(x: float|list[float], y: float|list[float]) -&gt; tuple[float|list[float], float|list[float]]\narray() -&gt; ndarray\n{% endraw %} Warning: Currently, NIS-Elements functions do not take numpy datatypes as input parameters so these have to be converted to classic python integers or floats.ExamplesAnalyzing captured image and storing points {% raw %}import limjob\nimport numpy as np\n\ndef run(imgs, Job, macro, ctx):\n    img = imgs[0]\n    img_data = img.array()[0, :, :, 0]\n    t = np.max(img_data)\n    pixels = np.argwhere(img_data == t)[0]\n    (x,y) = (float(pixels[1]), float(pixels[0]))\n    (x,y) = img.transformPxToStage(x, y)\n    Job.NewPointSet.PointSet.append(x, y)\n{% endraw %}Moving microscope to selected point {% raw %}import limjob\n\ndef run(imgs, Job, macro, ctx):\n    if len(Job.NewPointSet.PointSet.Positions):\n        stg = Job.NewPointSet.PointSet.Positions[0].Position.Stage\n        XY_Move(stg.x, stg.y)\n{% endraw %}  System &gt;  Expression🔗 Assigns values to variables available within the job.Can be used to modify settings of any task, manipulate motorized devices, etc. For more information about working with expressions please see   in the electronic help.  System &gt;  Variables🔗 Declares global variables within NIS-Elements. In programming, a global variable is a variable accessible from anywhere in the application ( NIS-Elements  in this case) unlike a local variable which is accessible in a limited scope (such as a macro function).Global variables declared by this task can be accessed e.g. by task   . Add new Add a new variable to the list. Remove the variable Removes the selected row. Remove All Clears the whole list of variables.Variable name Name your variable.Type Choose the numeric type of your variable.Integer Integral numberDouble Real numberString Sequence of characters.Array If selected, the variable will be of the type “array” which usually contains a set of values of the selected Type.Current value A value can be assigned to the variable within this task or within task   .Always Initiate Each variable keeps its value after the job is finished. Initialization resets the variable value to Initial Value at the beginning of the job.Initial value Set your initial variable value.Number of Elements If the Array option is selected, define the number of values which can fit in the array here.  System &gt;  Custom Metadata🔗 Enables the user to enter custom metadata to be used before (in wizard) or during the job run. Metadata can be created and managed in the Manage Custom Metadata dialog window (see  ). Defined metadata of different types (e.g. numeric value, text, selection, date, ...) are grouped into metadata sets and stored in the database. Metadata information applied by this task during the job are recorded into the job results and into the corresponding captured images.Define your labels and quantities in the job wizard and use them later on in a multiple well plate job. Figure&nbsp;1194.&nbsp;Jobrun Name Custom name of the jobrun.Jobrun Description Custom description of the jobrun.Custom Metadata If checked, custom metadata will be used. Select a metadata set and choose which metadata from this set will be used from the neighboring combo box. Active metadata values can be adjusted in the edit boxes below depending on the use of this task in your job definition.Presets Presets defined in the Presets tab of Manage Custom Metadata (see:  ) are displayed here as buttons for quick switching between them.  System &gt;  Wait🔗 This task pauses the job for a specified amount of time.Use this task whenever your job experiment needs a time pause, e.g. when illumination prolongs the sample emission. Figure&nbsp;1195.&nbsp;Name Name of your Wait task.Duration Amount of time for which the job will be paused.  System &gt;  Comment🔗 This task can be used for adding constantly visible notes / comments right into the job sequence.Use this task to write down information explaining the particular job procedure to a user seeing the job for the first time.  System &gt;  Send E-mail Notification🔗 Sends an email to the specified address. SMTP server configuration (within the task) must be correct.Use this task to send results of the experiment, messages about the experiment progress (failed, finished, in progress, ... ), etc. Figure&nbsp;1196.&nbsp;To Enter the recipient's e-mail address.Email Setup... This function opens the SMTP Configuration window (please see    ).Subject Type the subject of your e-mail.Send Now This function allows you to send your e-mail immediately.Text Box Here you can type your message combined with your job variables.... The ... button on the right side adjacent to the text box enables the user to insert job variables. You can also use your custom variables defined in the Variables task (see   ).Examples: Notification about the Job experiment end, notification to exchange well plates, notification that auto focus failed, etc.  System &gt;  Send SMS Notification🔗 Sends a text message to the specified phone number if the particular mobile phone operator provides such services (sending SMS over email).Send notifications about variables changing during the experiment (e.g. temperature in the incubator). Figure&nbsp;1197.&nbsp;Phone Number Type the recipient's phone number.Carrier Select the local mobile operator of the entered Phone number.Send Now This function allows you to send your SMS immediately.Email Setup... This function opens the SMTP Configuration window (see    ).Text Box Here you can type your message combined with your job variables.... The ... button on the right side adjacent to the text box enables the user to insert job variables. You can also use your custom variables defined in the Variables task (see   ).  System &gt;  Create Labels🔗 This task defines labels which can be used for labeling single wells or single points during runtime.Define labels to be used to mark contents of the wells of a well plate. Figure&nbsp;1198.&nbsp;Add labels by clicking the Select Labels... button or by clicking the  Add New. Set their name and choose the labeling color. You must use the    task to assign these settings.  System &gt;  Assign Label🔗 This task assigns labels set in    to wells or points during runtime.Use this task with a    task inside a well / point loop. During the first loop you can use the question buttons to manually decide which wells / points are suitable / unsuitable e.g. for a precise capture. The high magnification capture can then be made during the second loop only over wells / points specifically marked in the first loop.Requires:Task:&nbsp;  ,   ,     Figure&nbsp;1199.&nbsp;Filtering by these labels inside    and    is possible. Select a label from the Set Label list and assign it to a well/point loop. Check Reset other Labels if you want to clear existing labels. Figure&nbsp;1200.&nbsp;Label use case  System &gt;  Set Class🔗 Assigns the defined class to the selected well(s)/point(s)/slide in a loop.   detects that no cells are growing in a particular well plate so it is marked with a class number and removed from the capture.Requires:Task:&nbsp;   or    or   .Assign Class Number of the class which will be assigned.to Defines the loop to which the number will be assigned.  System &gt;  Debug🔗 Selects specific parameters and lists them inside the Job Execution Progress window. Values are displayed and the Job is paused until Continue is pushed.Select parameters influencing the quality of the acquisition and check them before the job is fully executed. Figure&nbsp;1201.&nbsp; Continue Moves to the next task in the job sequence.Automatically continue after If there is no user interaction during the specified time limit, the job automatically continues.  System &gt;  Phase🔗 Some use cases can be logically separated into Phases (preparation, verification, acquisition).This task can be used for holding parts of the job, creating conditions between these parts and running these parts in a custom order. Example&nbsp;14.&nbsp;Example of a well plate acquisition job separated into 4 phases:Find interesting locations within wellsRun Autofocus at each locationVerify locations and proper FocusRun Acquisition Figure&nbsp;1202.&nbsp;Phase taskAll task contained in the dashed area at the bottom of the task can be executed Anytime or Only after other Phases. Such condition can be created in the Expression area using basic Operators and Insert for inserting the selected phase. After defining phases and clicking   Run Job, options for their direct execution or execution of the whole job are shown below. Figure&nbsp;1203.&nbsp;Job Execution sub-menuPhases can also be launched in the   from the context menu over a job definition containing phases. Figure&nbsp;1204.&nbsp;Context menu in the JOBs ExplorerAfter a phase is executed, this information is stored so that the job can later be executed from the following phase. Each phase can be executed multiple times and the job can still be executed whole at once.  System &gt;  Finish🔗 This task finishes the current job run.When a    evaluates the job as done, this task finishes the job and only images captured so far will be saved.  System &gt;  Abort🔗 This task cancels the current job run.Place this task in the Else  section of   . If a condition is not met, the job is terminated without saving any data acquired so far.  System &gt;  Execute Macro After Run🔗 Enters a macro command which can be performed on Finish and/or on Abort of the job.This task is used when a macro command is required to be run after the job is finished or aborted.Please see   in the electronic help for more information. Figure&nbsp;1205.&nbsp;Macro on Finish Executes the specified macro on job Finish.Macro on Abort Executes the specified macro on job Abort. Click the arrow button to reveal a menu which can be used to Insert Content of Macro... from a {% raw %}.mac{% endraw %} file, Execute Macro... from a {% raw %}.mac{% endraw %} file or to Append Command... from the Command list.  System &gt;  Execute Python After Run🔗 Executes the python function “run” defined in the python script after the job run ended.Use it to open the current job folder in the MS Windows explorer.Inputsis_aborted: bool False on finished job and True on aborted job.Job_run_key: int Current job key number.To call macro use nis.mac.MacroFunctionName(macro_inputs...).{% raw %}nis.mac.OpenLogFile(){% endraw %}When input is a value, you can use basic python types - int, floats or string.If input is a pointer you have to construct and pass objects of nis.ptr types.Numeric types are nis.ptr.char8, nis.ptr.byte, nis.ptr.int, nis.ptr.int64, nis.ptr.word, nis.ptr.long, nis.ptr.dword, nis.ptr.double.Text types are nis.ptr.char.Number or number array is created as{% raw %}nis.ptr.int()                      nis.ptr.int() -&gt; 0, nis.ptr.double() -&gt; 0.0\nnis.ptr.int(count)                 nis.ptr.int(1) -&gt; 0, nis.ptr.double(2) -&gt; [0.0, 0.0]\nnis.ptr.int(count, value)          nis.ptr.int(1, 1) -&gt; 1, nis.ptr.double(2, 1) -&gt; [1.0, 1.0]\nnis.ptr.int(sequence)              nis.ptr.int([5]) -&gt; 5, nis.ptr.double([3, 4]) -&gt; [3.0, 4.0]{% endraw %}Text is created as{% raw %}nis.ptr.char()                     nis.ptr.char() -&gt; \"\"\nnis.ptr.char(\"text\")               nis.ptr.char(\"text\") - &gt; \"text\"{% endraw %}Value inside nis.ptr objects is changed via macro functions and could be retrieved via method get().{% raw %}x = nis.ptr.double()\ny = nis.ptr.double()\nnis.mac.PiezoXYGetPosition(x, y)\nprint(x.get(), y.get()){% endraw %}Example:{% raw %}import nis\nimport subprocess\n\ndef run(is_aborted: bool, job_run_key: int):\n    if is_aborted:\n        return\n    path = nis.ptr.char()\n    ret = nis.mac.Jobs_GetJobrunFolder(job_run_key, path)\n    subprocess.Popen(fr'explorer \"{path.get()}\"'){% endraw %}  System &gt;  Backup After Run🔗 Backs up the current Jobs database after the job run is finished.Automatically backup the database after each job run. Backup database filename Path to the file to which the database will be saved.Image files operation Determines what happens to all ND2 images linked to the current database.  System &gt;  Alternative Storage Location🔗 Changes the storage location. NIS-Elements database is ignored and all acquired files are saved into the specified folder.Use this task to save files from each job run into their own separate folder. Figure&nbsp;1206.&nbsp;Store Files to Defines where the acquired files will be saved.Put files from all runs into specified folder (add a distinguishing unique prefix) All runs are placed into a single folder. Unique prefix is added.Put files of each run into separate unique subfolder Files from each job run are placed into a separate folder.Open First Acquired File After Run First file acquired in the particular job is opened in NIS-Elements after the job run.  System &gt;  Storage🔗 This task enables the user to decide whether all acquired files are saved into a folder or thrown away.Use this task in a part of the job where it is useful not to save the big generated files. Figure&nbsp;1207.&nbsp;Name Name of your task.Filename Name of the file to be created.Do not save file(s) Acquired files are not saved if this option is checked.Save file(s) into folder Files are saved into the specified folder if this option is checked.  System &gt;  Block🔗 A general task for wrapping blocks of other tasks.Wrapped tasks can be easily collapsed and expanded or disabled.  System &gt;  Exit with Error🔗 Upon execution, this task will terminate the currently running job and display the designated error message.Use this task in a job sequence to terminate the job when the defined experiment does not follow the protocol.  System &gt;  Alternate_FileName🔗 Changes the output image file naming for a job loop.Insert the Z-position information to each image file name of the captured Z-Stack.Requires:Task:&nbsp;Any loop task. Name Name of your    task.Prefix New filename prefix. Adds a new alternate filename definition. Moves the selected alternate filename definition one step up. Moves the selected alternate filename definition one step down. Deletes the selected alternate filename definition. Deletes all alternate filename definitions.Name Name after the “Prefix”.Loop Selects the loop on which the filename changes will be set.Properties Sets other properties added to the filename right after “Name”. This menu changes with the selected Loop.Format Sets the format for the consecutive images - number of digits (Fixed number or Variable numbers) and a starting number (Start Index at ). This menu changes with the selected Properties.Filename Sample Displays a sample filename based on the settings applied above. ",
     id: 259 }, 
   { title: "Time Series",
     xmlid: "id|task.timeseries_sectionx",
     content: "        Time Series &gt;  Time Lapse🔗 Repeats the contained tasks according to the specified parameters (total duration / number of loops, interval, ... ). This task typically contains just the    task, but can contain any meaningful task as well.Suitable for capturing any “living” processes such as cell proliferation, FRAP, etc. Figure&nbsp;1108.&nbsp;Loop Type the name of your loop.Total Duration Select one of the following time lapse modes:Time Set a precise amount of time to be used as the Timelapse duration [msec / sec / min / hour(s)].Number of Loops Define the number of loops (how many times will the containing tasks executed).Unknown (User or Program Break) Use this function if you don't know when to stop the time lapse. With this setting it can be stopped manually or automatically after a program break.Acquisition Period Select No-delay to let the task run as fast as possible. Two other options specify a time interval between two adjacent repetitions of the task. Run Loop Every option specifies how fast the tasks are started. Wait Between Loops defines a time pause between the end of one loop and the beginning of the next one.Split Storage per Time point If checked, each time loop is stored in a separate ND2 file.Run Tasks while Waiting Other tasks can be performed repeatedly if there is a spare time between two adjacent loops. When you check this option, additional space named Run while waiting  appears at the bottom of this task. Place the tasks which you want to run while waiting for the next loop into this area.Finish when condition is TRUE Especially if a time lapse with Unknown total duration is defined, you can specify a conditional expression which will cause the loop to be discontinued. Click Define to specify the conditional expression (Please see  ).  Time Series &gt;  Redefine Time Lapse🔗 Modifies an existing Time Lapse.This task is usually used within a Condition (  ). If something happens, redefine Time Lapse (e.g. increase the frame rate).Requires:Task:&nbsp;  See    for description of the task options.  Time Series &gt;  Time Sequence🔗 Chains acquisition time-phases and phases of different types (temperature / gas control, perfusion control).Typical sequence: set temperature, capture timelapse, change temperature, wait, capture timelapse.Please see the   chapter for description of the task options.  Time Series &gt;  Repeat N times🔗 Repeats the contained tasks for the specified number of iterations or indefinitely until   .Manual creation of a point set (  ) containing a specified number of points. Figure&nbsp;1109.&nbsp;Name Type the name of your repeat task.Repeat Here you can choose from the following options:Number of iterations Insert a number to define how many times the tasks will be repeated.Undefined number of iterations Use this function if the loop will be exited either by the    task or by the Finish when condition is TRUE option.Finish when condition is TRUE You can specify a conditional expression which will cause the loop to be exited. Click Define to specify the conditional expression. See also  .  Time Series &gt;  Keep Object In View🔗 This function finds the object of interest based on the correlation between the previous and the next frame and always keeps it in the center of view using the microscope stage to compensate the object's motion.Keeping the moving cell in the center of the field of view while capturing a timelapse.Requires:Task:&nbsp;  Device:&nbsp;Motorized StageModule:&nbsp;   and    Figure&nbsp;1110.&nbsp; Move With The Stage, Move and Update Point's Coordinates The first action moves the stage to keep the object in view, whereas the second option moves the stage to the current center of the object and updates coordinates of your points defined in different tasks (e.g.    defined by   ).Select Capture Source: Select the source from which the image will be taken.Select Channel for Tracking: Select the channel which will be used for following the object. ",
     id: 260 }, 
   { title: "Well Plates",
     xmlid: "id|task.wellplates_sectionx",
     content: "                               Well Plates &gt;  Define Plate🔗 This task specifies shape and size of a well plate as well as the working area of single wells.Select a wellplate type you use from the database or define a custom wellplate. The wellplate definition includes dimensions, number of wells and placement of wells.Requires:Device:&nbsp;Motorized Stage Figure&nbsp;1082.&nbsp;Well Plate name Type the name of your well plate. Select from DB... This button opens the well plate database, where you can find many standardised well plates. Figure&nbsp;1083.&nbsp; Type your well code into the Search field to find your well faster.Custom Well Plate... This function opens the Define custom well plate window where it is possible to adjust all key parameters of nonstandard well plates. Please see  .Re-Align Plate Holder... Opens the Re-Align Well Plate dialog window. Please see   . Well plate alignment can be done inside the job definition using the    and is job dependent - has to be repeated with each new job. It is also possible to align the plate globally by   . This alignment is saved into the database and applied on new well plates automatically. Custom Well Plate Options Figure&nbsp;1084.&nbsp;Plate Name Type the name of your well plate.Rows Type how many rows does your well plate have.Columns Type how many columns does your well plate have.Well Shape Select which shape does your well plate have. Circular, Rectangular and Rounded Rectangular are available.Well Names Here you can choose the single well labeling method. The Per axis  method labels the wells with letters and numbers, applicable on both axis. The Per well  method labels the wells from number one, from the left side to the right side and from top to bottom. Specify your choice in the Column and Row combo box.Plate Length, Plate Width Define size of the wellplate.Column Spacing, Row Spacing Define distances between adjacent well rows.Row Offset, Column Offset Define distances from the edge of your plate to the center of the A1 well.Well Diameter Define the diameter of a single circular well.Well Height, Well Width In case of rectangular well shape, define its height and width.Corner Radius If you are using a Rounded Rectangular well shape, here you can set the corner radius in [mm].Column, Row In Per axis mode this function assigns either letters or numbers to all columns and rows. You can set the numbers and letters Order to be Ascending or Descending.  Well Plates &gt;  Use Autodetected Plate🔗 This task selects the globally stored auto-aligned well plate as the well plate to be used inside a particular job.  Well Plates &gt;  Align Well Plate Holder🔗 This function determines the exact position of a well plate on the XY stage. Typically, this task is executed during runtime.Before a wellplate is scanned, it must be aligned properly, so that the coordinates of wells are not misplaced. Select wellplate orientation and find the A1 well.Requires:Task:&nbsp;  Device:&nbsp;Motorized Stage Figure&nbsp;1085.&nbsp;Correct Wellplate Alignment - Well Calibration&nbsp;For a precise alignment, at least two wells should be calibrated. Each well is calibrated using one or more points based on its shape and selected calibration method. To align your wellplate correctly follow this procedure:Select a calibrating Method from the combo box (defines how many points have to be added for each well).Choose a well in the wellplate preview and click on in (Recommended Wells are colored in gray).Display live image from the camera. A cross will be displayed in the image.Move the XY stage so that the edge/center/corner (based on the chosen method) of the well is in the center of the cross.Click Add.Now use the stage to add more edge points on the same well (add as many points as you selected in the Method combo box; e.g. for Center Point one is sufficient). If all calibration points of a single well were correctly added, the well is highlighted green.Choose a second well in the wellplate preview and click on it.Add calibration points for this well (repeat procedure from step 3).If done properly, two wells are calibrated (highlighted green) and the well plate is correctly aligned. To increase accuracy of the alignment, calibrate more than two wells.  The more wells you calibrate, the more well plate alignment imperfections are eliminated. Two calibrated wells can rectify the XY position and rotation of the well plate. Three calibrated wells can rectify the XY position and a slight tilt of the well plate. Four calibrated wells can rectify the XY position and a large tilt.However one well is also enough for rough calibration. If one well is used for calibration it is assumed that the whole well plate is placed orthogonally on the stage with its top-left well oriented to the top-left stage corner.Align Wells tabName Type the name of your align.Align Choose a well plate defined by the    which will be aligned.Method Specify the calibration method based on the number and position of the calibration points on the currently calibrated well.Add Adds point coordinates into the well alignment table.Remove Removes the selected XY coordinate.Remove All Removes all points from the alignment table.Test Well Select a calibrated well and click this button to move to its edge. This way you can check the correct edge calibration. Figure&nbsp;1086.&nbsp;Test tab&nbsp;This tab is useful for checking the correct alignment of the whole well plate. Hover the cursor over any well. Available test move locations are displayed. Click on a gray circle to move the stage to this exact position and verify the alignment precision.  Well Plates &gt;  Detect Plate Automatically🔗 Automatically detects and aligns the well plate on the stage. AI driven algorithm is used for the detections. The result is stored globally - one auto-aligned well plate can be used by any other job or by Well Plate and Slide Navigation. Advantage of this task is that it is not necessary to align the plate holder on the stage or to know/define an exact plate type. It can be be directly used by the    task as plate definition.Requires:Device:&nbsp;Motorized Stage, 4x objective  Well Plates &gt;  Select Wells🔗 Specifies which wells of the selected well plate are actually used in the experiment.Use the A1 well for alignment and leave the outer perimeter of wells unused.Requires:Task:&nbsp;  Device:&nbsp;Motorized StageBefore selecting any wells you need to define the well plate by the    task. After the plate definition, you can select the wells using the Select Wells task. The wells can be easily selected by clicking and dragging over the well board. If you hold down the shift key, you can select remote parts of the well plate. If you hold down the control key and drag over wells already selected, you exclude them from your actual selection. You can also select single rows / columns by clicking on their letter / number. If using mouse for the well selection, the selected values automatically appear in the Select rows or Select columns fields where you can manually rewrite them using comma (for separating the values) and dash (for entering range values). Figure&nbsp;1087.&nbsp;Name of the Selection Type the name of your well selection.On Well Plate Here you can switch between different well plates created in your actual job.Selection color Choose any color you wish. This color is then going to be used to visualize the wells selected.Well Ordering This function defines the order in which the wells are passed (e.g. during a Loop). Meander (the most effective way of moving between wells), Left to Right and Top to Bottom options are available.Select all Selects all wells from a chosen well plate.Select Rows  Figure&nbsp;1088.&nbsp;Enter a single row or a range of rows that you want to select.Select Columns  Figure&nbsp;1089.&nbsp;Enter a single column or a range of columns that you want to select.  Well Plates &gt;  Empty/Full Well Selection🔗 Creates a well selection with either all wells selected or de-selected.Use it before a loop over wells to reset the selection before it is modified within the loop.Requires:Task:&nbsp;  Selection Full Adds all wells to the selection.Empty Removes all wells from the selection.Selection Color Resulting color of the selection.Well Ordering The movement direction used when going over the selection in a loop.  Well Plates &gt;  Add Well to Selection🔗 Adds a well to an existing selection of wells.Use it inside    and    to modify a well selection.Requires:Task:&nbsp;   or     Well Plates &gt;  Remove Well from Selection🔗 Removes a well from an existing selection of wells.Use it inside    and    to modify a well selection.Requires:Task:&nbsp;   or     Well Plates &gt;  Edit Well Selection🔗 This task enables the user to manually adjust the automatically generated well selection at run time.The user can modify the selection of wells right after detecting the cells using AI.Requires:Task:&nbsp;    Well Plates &gt;  Use Well Selection from Sample Navigation🔗 This task loads the selection of wells currently defined in the    window.Requires:Task:&nbsp;    Well Plates &gt;  Create Well Selection from Point Set🔗 This task uses a point set to create a new well selection.Requires:Task:&nbsp;  Well Selection on Choose a well plate definition.containing Specify the point set.  Well Plates &gt;  Loop over Wells🔗 Runs the contained tasks on each well of the selected well selection.Used for wellplate analysis. The wells are either analysed one-by-one (fast timelapse experiments) or in multiple well-loops (e.g. 1: pre-focus all wells, 2: capture each well).Requires:Task:&nbsp;  Device:&nbsp;Motorized Stage Figure&nbsp;1090.&nbsp;Loop Type the name of your loop.over Switch between different well selections available within your job.Split storage per well This option influences how the system stores the captured data on the disk. If checked, separate ND2 file will be created for each well.Use Wells with If your wells are labeled (by the    task), you can loop only over wells marked with the selected label.Advanced If you select this option, the Loop over wells will be performed only on wells with defined class typed in the Loop over wells in class field. The class number can be assigned to each well using an expression. The Class can be defined in the    task - find a parameter named “Class” under Job / Name of your Well Loop / CurrentWell / Class. Insert this expression by double-clicking on it and use given operators to define your class (e.g.: Job.WellLoop.CurrentWell.Class=2). In our case “2” is the Class we fill into the Loop over wells in class field.Under the current well of the WellLoop there is also a Skip parameter under CurrentWell which is 0 (FALSE) by default. It can be changed in the    or    task inside this WellLoop or outside (different WellLoop). When set to 1 (TRUE), the well will be skipped next time when visited by any WellLoop (e.g. in a Time-lapse). The Skip parameter (same as Class and Labels) is on every well. The WellLoop makes it only available for current well under CurrentWell parameter. Therefore, when it is changed for a given well all other Well loops can see it.{% raw %}if (totalCells &gt; 10000)\n   Job.Wells1.CurrentWell.Skip = 1;{% endraw %}where the Wells1 is the name of the task.The node itself exposes a read-only parameter ValidCount reflecting the number of wells visited while taking into account skipped wells due to the Skip, Class and Labels parameter. It is useful for terminating a loop that runs over a WellLoop (typically a time-lapse) that is already skipping all the wells. Figure&nbsp;1091.&nbsp;Complete example using the Skip and ValidCount based on GA3 result can be found  . Loop over Wells does not move between the wells on a well plate by itself. It is therefore impossible to expect pictures of single wells by placing just Capture inside the Loop over Wells. To do so, you have to place the Move to Well Center task (  ) inside the loop right before capture. This task tells the loop to move to the appropriate well.  Well Plates &gt;  Label Wells🔗 Enables the user to label, dosing or custom labels to each well or a group of wells.Label positive, negative and control wells and add the quantity of the solution used so that the analysis results can be sorted based on the given label.Requires:Task:&nbsp;  Device:&nbsp;Motorized StageUnfold your    task inserted into the Job definition window by double clicking on its caption and then click   Label Wells to open the Labeling and Dosing window (closely described here:  ). To clear all the defined labels click  Clear All.  Well Plates &gt;  Use Well Labeling from Sample Navigation🔗 This task loads the labeling of wells currently defined in the    window.Set the labeling outside of JOBs in the main application (e.g. in Sample Navigation) and then use it in JOBs.  Well Plates &gt;  Export Well Labeling to Sample Navigation🔗 This task exports the well labeling done in JOBs into the    window.Set the labeling in JOBs and the re-use it in the Sample Navigation panel.  Well Plates &gt;  Export Well Selection to Sample Navigation🔗 This task exports the well selection done in JOBs into the    window.Set the well selection in JOBs and the re-use it in the Sample Navigation panel.  Well Plates &gt;  Plate Loader🔗 This task communicates with the well plate loader connected to your system.Put this task to wizard to initialize the loader and scan hotels for wellplates just before the experiment begins.Requires:Device:&nbsp;Well Plate LoaderClick the Initialize button and follow the instructions to initialize your plate loader.Scan for the loader hotels by clicking the Scan Hotels button.Select / deselect the plates and hotels found to be used in your job.Add notes to each of your plate if necessary.Other dialog options are similar to   . Figure&nbsp;1092.&nbsp;  Well Plates &gt;  Manual Plate List🔗 Specifies the list of well plates to be used in a loop over well plates. These well plates are changed manually.Assign names and barcodes to wellplates in an ordered list. This info will be included in the experiment results.Requires:Device:&nbsp;Motorized Stage Figure&nbsp;1093.&nbsp;Manual Plate List Name Name your manual plate list.Well Plate Definition Select a well plate to be looped.Multi Plate Holder If using a well plate holder, you can adjust the well plate offset from the first well plate in the table.First Plate is Already on Stage Check this option if your first plate is already on stage. Add New Plate Adds a new plate to the table. Remove Removes the selected plate. Remove All Removes all plates in the table.Plate # Automatic sequence number of your well plates.Name Name of your well plate.Barcode Here you can enter your own barcoding for each well plate.X Offset from Plate #1 X axis offset from plate 1.Y Offset from Plate #1 Y axis offset from plate 1.Notes for In the blank window below the well plate ordered list you can fill out any other information about each of your currently selected well plates.  Well Plates &gt;  Loop over Plates🔗 Runs the contained tasks on each well plate of the specified list of plates.Well-by-well (live) experiments: the whole analysis is inside a plate loop. Fixed/scanning experiments: several plate loops are used in a sequence - 1. pre-focus, 2. capture, etc.Requires:Task:&nbsp;   or   Module:&nbsp;   Figure&nbsp;1094.&nbsp;Loop Name your plate loop.over Choose a plate list from your current job.  Well Plates &gt;  Move to Well🔗 Moves the XY stage to a particular well of the specified well plate.Move to a specified well during runtime to check the sample.Requires:Task:&nbsp;  Device:&nbsp;Motorized Stage Figure&nbsp;1095.&nbsp; Task Name Name your move task.Well Plate Select a well plate for the move definition.Well Row Define a row position where to be moved.Well Column Define a column position where to be moved.Move to Z If checked, the Z-drive moves to the Z position assigned to the current well (e.g. Z value found by    during the first   ).Use Well PFS Offset These options tell the system to set the Z drive to a position previously assigned to the current well. Particular Z drive position can be assigned to each well by the    or    tasks used within   . The check boxes refer to this Z coordinate.  Well Plates &gt;  Move to Well Center🔗 Moves the XY stage to the center of the current well.Treatment is usually applied to the well center.Requires:Task:&nbsp;   or    or   Device:&nbsp;Motorized Stage Figure&nbsp;1096.&nbsp; Move to Select a sample holder used in your current job.Move to Z If checked, the Z-drive moves to the Z position assigned to the current well (e.g. Z value found by    during the first   ).Use well PFS offset These options tell the system to set the Z drive to a position previously assigned to the current well. Particular Z drive position can be assigned to each well by the    or    tasks used within   . The check boxes refer to this Z coordinate.  Well Plates &gt;  Move to Previous/Next Well🔗 Moves the XY stage to the previous/next well.Can be used with a condition during runtime, e.g. when there is a need to get to the previously treated well.Requires:Task:&nbsp;  Device:&nbsp;Motorized Stage Figure&nbsp;1097.&nbsp; Task name Name of your task.Move from Select the well from which you want to move.to Specify the moving distance to the previous/next well.Previous, Next Choose the direction to which well you want to move.  Well Plates &gt;  Copy Well Point Set🔗 Copies one point set to another point set.In the first step, auto focus is run inside each well on a few points and Z coordinates are saved to the well point set. In the second step, a timelapse over the wells is performed and the Z coordinates from the first step are used to create a focus surface in each well. Thus there is no need to repeat the slow auto focus procedure.Requires:Device:&nbsp;Motorized Stage  Well Plates &gt;  Copy Well Selection🔗 Copies the selected well selection to another well selection. This task works only with identical well plate types.If you capture cells (well selection 1) and there are some cells that show the desired response to a specific concentration of a drug (well selection 2), you can transfer them to different experimental conditions (well selection 3).Requires:Task:&nbsp;  Name Name of your task.From The    task from which the well selection is copied.To The    task to which the well selection is copied.  Well Plates &gt;  Continuous Scan of Well Centers🔗 It uses a continuous scan to create a well plate overview. Resulting images are shown in the Stage Overview. Images can be used for any General Analysis 3 recipe.Requires:Device:&nbsp;Motorized StageScan well centers on A well selection (  ) or plate definition (  ) on which the task will be performed.Binning A post-processing which shrinks the resulting data size.Stage speed Prefer current speed If the current speed of the XY stage is reduced for some reason (e.g. by the    task), it will be used.Maximum speed Use the maximum speed available to achieve the expected result.Limit motion blur Speed of the XY stage will be reduced to prevent motion blur in the resulting image.Start from closest corner The scanning will start in the corner of the scanned area which is closest to the current XY position.See  .  Well Plates &gt;  Find Well by Analysis🔗 This task uses continuous scan to find a well based on any parameter/criterion measured by the selected General Analysis 3 recipe.Find the first well where the sample is present, and then perform auto focus there.Requires:Task:&nbsp;Device:&nbsp;Motorized StageScan well centers on A well selection (  ) or plate definition (  ) on which the task will be performed.Recipe A GA3 recipe which is run on each well center an returns numerical results. Export Recipe Saves the GA3 recipe to a *.ga3 file.Synchronize recipe If turned ON, the latest version of the GA3 recipe saved in NIS-Elements analysis database is used whenever the task is executed. If turned OFF, the recipe will be saved as a part of the job and later changes to the original recipe will be ignored.Criterion Variables created by the GA3 recipe are listed in the table below. Use the Insert button to insert a reference to the selected value to this field. This way you can create a condition such as:{% raw %}Records.CellsPresent[0] &amp;&amp; Records.Confidence[0] &gt; 0.7{% endraw %}See  .  Well Plates &gt;  Load Plate to Stage🔗 Loads the selected plate onto the stage.When a manual plate load is required.Requires:Device:&nbsp;Well Plate Loader Figure&nbsp;1099.&nbsp;Name Name of your task.Plate Plate which will be loaded onto the stage.  Well Plates &gt;  Unload Plate from Stage🔗 Unloads the plate currently being on the stage.When a manual plate unload is required.Requires:Device:&nbsp;Well Plate LoaderName Name of your task. ",
     id: 261 }, 
   { title: "Z-Stack",
     xmlid: "id|task.zstack.group_sectionx",
     content: "            Z-Stack &gt;  Define Z-Stack🔗 This task defines parameters required for capturing a Z-Stack (a sequence of images each having a different focal plane).Please see the   chapter  for details about the Z-stack setup.Create all-in-focus images or 3D models regardless of your sample and depth of field parameters.Requires:Device:&nbsp;Stage Z Figure&nbsp;1137.&nbsp;Defined by top bottom mode Figure&nbsp;1138.&nbsp;Symmetric mode defined by range Figure&nbsp;1139.&nbsp;Asymmetric mode defined by rangeZ-Stack Name of your    task. Defined by top bottom This mode defines the Z-stack using two parallel planes (  Top and   Bottom).Required values: top position, bottom position, step size or number of steps. Symmetric mode defined by range This mode defines the Z-stack by its center (Home) position and the same range above and below the Home plane.Required value: range. Asymmetric mode defined by range This mode defines the Z-stack by a center (Home) position and a custom range above and below the Home plane.Required values: above position, below position. Top Sets the top Z position. Reset Discards the Top, Home, and Bottom position settings. Relative Changes the Home (absolute Z) position to Relative (current Z position deducted prior to the actual acquisition). Home In the Symmetric mode defined by range / Asymmetric mode defined by range this button sets the position in the middle of your range. Bottom Sets the bottom Z position.Step Define the range of a single step. Use suggested step size Click this button to use the recommended step size.Steps Insert the number of steps you want to acquire.Bottom Set the bottom Z position manually.Top Set the top Z position manually.Z Device Choose a Z device used for Z-stack acquisition. Piezo Specifies the role of the Piezo Z drive.Range Enter the range of the final Z-stack (in-focus portion of your sample).Below Set the bottom position of the Z-Stack manually.Above Set the top position of the Z-Stack manually.Direction Sets the Z axis direction in which the stage will move during acquisition.  Z-Stack &gt;  Z-Stack Bottom and Top🔗 Defines a Z-Stack simply by its top and bottom positions and step.Use the microscope joystick to find the bottom of the inspected cell, click  Bottom. Then find the top of the cell, click   Top. Set the Z-Step (Step Size). The absolute coordinate Z-Stack definition is now finished.Requires:Device:&nbsp;Stage Z  Top Sets the top Z position. Reset Discards the Top and Bottom position settings. Bottom Sets the bottom Z position.Device Selects the Z-Drive device connected to NIS-Elements.Step Size Defines the range of a single step. Use suggested step size Click this button to use the recommended step size.  Z-Stack &gt;  Z-Stack Piezo from Bottom🔗 Defines a Z-Stack to be acquired from the bottom position to the top using just the Piezo Z drive.Find the bottom of a cell and capture a Z-Stack with the full range of the Piezo Z drive. See also   .Requires:Device:&nbsp;Stage Z Name Name of the task.Distance Defines the size of the Z-Stack from the bottom to the top. Check Maximum to use the maximum possible distance.Step Size Defines the range of a single step. Use suggested step size Click this button to use the recommended step size.Step Count Displays information about the number of defined steps.  Z-Stack &gt;  Z-Stack by Expression🔗 Defines a Z-Stack using absolute coordinates or variables available in NIS-Elements. Use the ... button to insert variables into the particular edit box.Use this task when you need to define a Z-Stack with absolute Z positions inserted from variables.Requires:Device:&nbsp;Stage ZName Name of the task.From Absolute Z position where the Z-Stack will start.To Absolute Z position where the Z-Stack will end.Step Defines the range of a single step.Device Selects the preferred Z-Drive device connected to NIS-Elements.Global Z Coordinates Use Case&nbsp;In this following example with an inverted microscope, the stage Piezo Z moves against the direction of the microscope Z-drive and the global coordinates are subtracted. To capture a Z stack for global Z coordinates 10 to 50, the microscope Z-Drive goes to 110 and the Stage Piezo Z goes to 100 (global Z coordinate is 110-100 = 10). Then the Stage Piezo Z moves from 100 to 60 so the final global coordinate is 110-60 = 50. Figure&nbsp;1140.&nbsp;Jobs definition.The drawings below show how the global Z value is calculated for different types of Z-devices on different microscopes. Figure&nbsp;1141.&nbsp;Upright Microscope with Stage Piezo Z. Figure&nbsp;1142.&nbsp;Upright Microscope with Objective Piezo Z. Figure&nbsp;1143.&nbsp;Inverted Microscope with Objective Piezo Z. Figure&nbsp;1144.&nbsp;Inverted Microscope with Stage Piezo Z.  Z-Stack &gt;  Z-Stack Loop🔗 Repeats the contained tasks for each Z position of the selected Z Stack.This task can be used inside any other loop (time-lapse, point set) with    to acquire Z stacks for extending spatial resolution of the image sequence.Requires:Task:&nbsp;   or   Device:&nbsp;Stage Z Figure&nbsp;1145.&nbsp;  Z-Stack &gt;  Assign Z-Stack to Point/Well/ND Acquisition🔗 Every XY position (well, row of wells, column of wells) can have unique Z stack parameters after using this task.Scan a wider Z stack over the wells selected by the analysis.Requires:Task:&nbsp;  ,   ,   ,   ,   Device:&nbsp;Stage Z Figure&nbsp;1146.&nbsp; Assign Z-Stack definition Select your Z-Stack definition for the assignment.Offset Add an optional Z offset on top of the Z-Stack definition.to Select to which the Z-Stack definition will be assigned. If a second combo box appears (e.g. when using well plates) - specify your selection.  Z-Stack &gt;  Move to Z-Stack home🔗 Moves the Z drive to the home position of the selected Z stack definition.When capturing multi-channel Z-stacks, not all the channels are required to contain the Z dimension. These are usually captured in the home (center) position.Requires:Task:&nbsp;   or    or   Device:&nbsp;Stage Z Figure&nbsp;1147.&nbsp; Move to Home of Choose your Z-stack definition from your current job.  Z-Stack &gt;  Set Z Speed/Accuracy🔗 Sets the stage Z speed so that different speeds can be used throughout the job.Use a slower Z speed to prevent the sample shift and get stability for the PFS during acquisition. Then use faster speed when changing samples to reduce the total acquisition time.Requires:Module:&nbsp;  Name Name of your task.Z Speed [µm/s] Select the check box and enter a new Z speed value. Otherwise the indicated default value will be used.Accuracy [µm] Select the check box and select a different value to be used. Better accuracy (lower value) usually results in longer movement times. If you select the Open value, the stage Z position will not be corrected at all. It is the fastest but least precise option.Restore original values when the job finishes If selected, default speed and accuracy values are set after the job is finished.  Z-Stack &gt;  Reset Piezo Z Position🔗 Moves the Piezo Z device to the position selected in the Position drop-down menu and compensates the second Z-device to stay on the same Z plane as before.Find the bottom of a cell, reset the Piezo Z bottom position and capture a Z-Stack of the cell using the full range of the Piezo Z drive.Requires:Device:&nbsp;Stage Z Name Name of the task.Position Select the position to which the Piezo Z drive moves. The second Z drive compensates for this movement to stay on the same plane. If the Custom option is selected, an exact Z position can be entered into the field. Range displays the possible Z values falling into the current range of the device. ",
     id: 262 }, 
   { title: "Analysis",
     xmlid: "id|tasks.analysisx",
     content: "           Analysis &gt;  Define Analysis🔗 This task can be used to define multiple analyses which can be run during or after image acquisition.Link the task to your    task displayed in the Analyze combo box. Now choose your first analysis from the combo box in the table &lt;Select Analysis&gt;. Name it and choose when it will be executed. Click Define... and set up the current analysis. Please see   in the electronic help for details about each analysis.Requires:Task:&nbsp;    or similar. Figure&nbsp;1208.&nbsp;JOBS Recipe Manager&nbsp;Clicking on ... in the Analysis task caption opens the JOBS Recipe Manager where it is possible to  Import/ Load from NIS... any previously created analysis recipes. Recipes marked with a check mark are arranged in a new section called Analysis Recipes in the Job Definition Task Palette.  Analysis &gt;  General Analysis🔗 General analysis in jobs has the same functionality as General analysis from the Image menu, however in Jobs it brings much more versatility.Please see   in the electronic help for detailed examples.Requires:Task:&nbsp;    or similar.Module:&nbsp;  Once you set up a General analysis it can be later used as a custom task. To do so, first you have to save your analysis settings (for General analysis from the Image menu click Save as... and name your .ga file, for General analysis in Jobs click Save Recipe). Then click ... and search for your saved settings. Use the  Import... or Load from NIS... buttons if your analysis is not already listed. Then select the desired recipe and click OK. Selected analysis recipe carrying the analysis settings is now represented by a custom task inside the Analysis Recipes section of the task palette.Parameters of the    task are closely described here:  .  Analysis &gt;  Cell Count Analysis🔗 This analysis is designed for counting organic cells. It operates on a single channel and produces one binary mask and a set of predefined features.Count the number of cells in a sample and measure their properties.Requires:Task:&nbsp;    or similar.Parameters of this analysis are closely described here:  .  Analysis &gt;  Intensity Analysis🔗 Evaluates the intensity of the whole camera field of view.Decide whether to capture the whole well in a well plate or not (based on the signal in the field).Requires:Task:&nbsp;    or similar.Parameters of this analysis are closely described here:  .  Analysis &gt;  General Analysis RGB🔗 (requires:  )Allows to create a new binary layer defined as an intersection of the three RGB channels based on the RGB or HSI threshold values.Use the benefit of the RGB or HSI threshold to define the binary layer to be analysed.Requires:Task:&nbsp;    or similar.Module:&nbsp;  Parameters of this analysis are closely described here:   .  Analysis &gt;  Out Proc🔗 (requires:  )This task enables the user to use an external program to process/analyze images captured via JOBS. If the external program produces output in a form of binary images or an array of values in a text file, JOBS load and use them for further analysis.Process images created in Jobs using ImageJ.Requires:Task:&nbsp;    or similar.Module:&nbsp;    Figure&nbsp;1209.&nbsp; Analysis Name, Run: The top part of the window is common to all analysis windows are described in the   chapter.Input Image Name and path to the TIF image generated by NIS-Elements for analysis. E.g. “C:\\color.tif”Sync Status Name and path to the text file with status of the analysis. E.g. “C:\\sync.txt”. This file is shared between NIS-Elements and the external program. Running processes can communicate due to the sync status saved in this file. Possible values are:0 Not ready1 Ready for analysis2 Analysis is finished3 Cancel executionIn Params Path to the .txt file containing input parameters for the analysis.Out Params Name and path to the text file where analysis results will be saved. E.g. “C:\\outputparams.txt”. It will contain a list of values specified in the Output Parameters section. There are hidden parameters used for each external process start. Instead of the command: {% raw %}C:\\Windows\\System32\\cmd.exe /C c:\\tmp\\outproc\\outproc.bat{% endraw %} the following command is used by NIS-Elements: {% raw %}C:\\Windows\\System32\\cmd.exe -i c:/tmp/outproc/color.tif -s c:/tmp/outproc/sync.txt -p -c c:/tmp/outproc/points.txt /C c:\\tmp\\outproc\\outproc.bat{% endraw %}. These parameters correspond to the communication files paths set in the Out Proc window.Out Points Points are generated into this file by the external application (e.g. points representing centers of binaries). Example&nbsp;15.&nbsp;points.txt file formatOne stage coordinate per line prefixed by two characters which defines type of the coordinate (e.g. \"x=\") should be written inside, Z coordinate is optional.Two points including z coordinate:{% raw %}x=100\ny=100\nz=100\nx=200\ny=200\nz=200{% endraw %}Two points without z coordinate:{% raw %}x=100\ny=100\nx=200\ny=200{% endraw %}Run Process for Analysis Select this option if the external program you use runs on demand and needs to be called to process each image. Programs which run independently and are capable of checking the content of the Sync Status file do not need this function.Process Name Name and path of the external program.Process Parameters Any parameters needed for the external program.Kill Process, Timeout Programs which do not end automatically can be killed after the defined Timeout.Wait for sync file If this option is checked, the external program uses a sync file to synchronize with NIS-Elements. This way it knows when NIS-Elements prepared the data. When the data processing is finished it tells NIS-Elements that the processing is finished so that NIS-Elements can load the results.Wait for the process finish If this option is checked, NIS-Elements wait until the process is ended.Output Binaries Definition of binary TIF files generated by the external program which ought to be loaded by NIS-Elements after analysis. The first field specifies name of the binary layer, the second field is a name and path to the TIF file containing the binary image data.Output Parameters Definition of result values generated by the external program. Supposedly, the external program generates a list of values, each value on a new line. This definition pairs these values to its description under which they will be available within NIS-Elements.Input Parameters Maps the variables from a Job. The process can use these values as input parameters. For example if label = “posX” and value = “Job.Points.CurrentPoint.Stage.x” the external process can load from the “In params.txt” the X value of the stage.Output point coordinates Defines where the points are saved (point list defined by a task).Preview Runs the analysis on the current image. Processing with ImageJ&nbsp; Figure&nbsp;1210.&nbsp;Out Proc processing with ImageJOut Proc task also enables the user to process images using ImageJ. Set the path of the Input Image generated by NIS-Elements and the path to the file containing results generated by ImageJ (Out Params). It is important to let the Sync Status  field empty. Enable Run Process For Analysis and set the Process Name as an executable ImageJ file. Specify the macro and other options in the Process Parameters field. For example to measure the image area and mean gray value using ImageJ enter the following macro:{% raw %}path = getDirectory(\"imagej\")+\"NIS\"+File.separator;{% endraw %}{% raw %}print (path);{% endraw %}{% raw %}open(path+\"input.tif\");{% endraw %}{% raw %}run(\"Input/Output...\", \"jpeg=85 gif=-1 file=.txt use_file\");{% endraw %}{% raw %}run(\"Set Measurements...\", \"area mean redirect=None decimal=3\");{% endraw %}{% raw %}run(\"Measure\");{% endraw %}{% raw %}saveAs(\"Results\", path+\"Results.txt\");{% endraw %}{% raw %}eval(\"script\", \"System.exit(0);\");{% endraw %}Measured results are connected to the output parameters specified in the Output Parameters area of the Out Proc task and are shown in the job results window.  Analysis &gt;  Wound Healing🔗 This task controls the wound healing analysis used for measuring changes of cell distribution in time.Inspect the regrowth of damaged tissues.Requires:Task:&nbsp;    or similar.Module:&nbsp;   Figure&nbsp;1211.&nbsp;The top buttons which are common to all analysis windows are described in the   chapter.Analysis Name Name your wound healing analysis.Run Specify in which phase will the analysis be executed (During Acquisition or After Acquisition).Close Holes - Count This command fills holes in detected objects. Specify how many times the command will be run.Delete Small Objects - Count This command deletes the smallest objects. Set how many times the command will be run. Use the Preview button to check your actual image settings.Do Postprocessing Check if you want to use postprocessing.Preview Check if you want to see a live preview of your current wound healing settings.  Analysis &gt;  Live/Dead Analysis🔗 Is used for counting live and dead elements in one sample. This analysis is closely described here:  .Define one channel for counting Live elements and one for Dead elements to count both types in a single sample.Requires:Task:&nbsp;    or similar.This task section contains other jobs analyses. To be able to execute any analysis in Jobs, your job definition has to contain    and    or one of the alternative capture tasks (  ,   ,   ,   ). ",
     id: 263 }, 
   { title: "Large Images",
     xmlid: "id|tasks.largeimagesx",
     content: "                   Large Images &gt;  Scan Large Image🔗 Task enabling to scan large images using precise parameters.When a cell is bigger than the field of view of your objective, scan it using this task to create a large image.Requires:Task:&nbsp;  Module:&nbsp;   Figure&nbsp;1125.&nbsp;Name Name of your task.Scan using Select a    and specify its objective from the combo box (with ... objective).Save Image Choose whether or not to save the acquired images.Fields Define the field dimensions of the large image.Placement Around the Current Position places the corners of the fields directly next to the current stage position. Current Position is at top-left Corner uses the stage position as a position of the top-left field.Captured Shading Correction Shading correction saved with the Optical Configuration can be used. Select your Optical configuration and then go to    to capture the shading correction. Then you can check the Captured Shading Correction check box.Automatic Shading Correction To perform the shading correction automatically without capturing the shading image, check this option and choose the type of correction which best represents the background of your sample image.Overlap and Stitching This area contains parameters defining the overlap size, stitching mode (blending or optimal path) and a channel which is used for the stitching. Precise stitching (Image Registration) can be checked but be aware that it is more computationally demanding.Use Auto Focus Autofocus can be used checking this function. Select a    and choose where in the large image it will be performed (every n-th Field, after a specified stage distance or on the Center Tile at Start). Autofocusing on frames without the sample can be skipped (Skip frames without sample).Use Focus Surface Focuses using the surface defined in the Focus Surface tab of the    panel.  Large Images &gt;  Scan Large Image in Slide/Well/Area🔗 This task scans large images on a sample holder (slide, stage area or well).Create a well plate experiment and decide on which wells a large image will be captured.Requires:Task:&nbsp;   and    or    or   Module:&nbsp;   Figure&nbsp;1126.&nbsp;Holder Specifies your sample holder.Border restriction Enter an optional border restriction on your sample holder.Center restriction Enter an optional center restriction on your sample holder.Please see    describing other parameters which are similar to this task.  Large Images &gt;  Scan Large Image in Region🔗 This task is used to scan large images on a predefined set of regions. Regions can be generated automatically using    or drawn manually on a captured image or a large image.Create regions with General Analysis and scan a large image on them.Requires:Task:&nbsp;  Module:&nbsp;   Figure&nbsp;1127.&nbsp;Region    in which this task is placed.Correct Objectives XY Offset This feature corrects the XY offset previously set in   .Please see    describing other parameters which are similar to this task.  Large Images &gt;  Scan Large Image by Continuous Movement🔗 This task quickly scans any selected area on the stage or the whole stage.Use this task for a slide preview with a low magnification objective.Requires:Device:&nbsp;Motorized StageArea definition Position &amp; Size Specify a rectangle by origin and size.Corners (absolute) Specify a rectangle by opposite cornersStart-End Specify the start position and the end position. Use can use this to scan just a single line.Whole stage Scan the whole stage.Holder Scan the sample holder.Stage speed Prefer current speed If the current speed of the XY stage is reduced for some reason (e.g. by the    task), it will be used.Maximum speed Use the maximum speed available to achieve the expected result.Binning A post-processing which shrinks the resulting data size.Limit motion blur Speed of the XY stage will be reduced to prevent motion blur in the resulting image.Start from closest corner The scanning will start in the corner of the scanned area which is closest to the current XY position.Stage view Orientation of the resulting image will be adjusted to match the actual view on the XY stage without the eyepiece or camera.Precise stitching (image registration) Runs a precise image-registration function to compensate mis-alignment of images. The transitions between tiles of the large image will look better but the processing will take longer. If your system is properly aligned, this function should not be needed.See  .  Large Images &gt;  Draw Regions🔗 This task is used to manually draw regions during acquisition.Once the capture is made, the job is paused and Simple ROI Editor is shown over the Draw Regions window. Please see    for more details about drawing regions. Once you finish the drawing, click Finish to continue the job.Requires:Task:&nbsp;  Module:&nbsp;   Figure&nbsp;1128.&nbsp;Name Name of your    task.Manually draw on Select a capture on which the regions will be drawn.Region Division Regions can be treated Per Frame - all drawn regions are considered as one area in the particular frame, or Per Object - each region is treated as a single object.Window size Depending on your preference, choose one of the three window size options: General options (exerts the last used window size), Tiled view (tiles the live window alongside with other windows) or Maximized (Maximizes the live window).Window position If using multiple monitors, this function can put your Draw Regions window onto your Second screen, keep it on the Main screen or embed it directly into the Progress dialog.  Large Images &gt;  Shift Region🔗 When Multimodal Image Registration (  ) is used, shift of the regions can be made.Use this task for comparing images from SIM and Confocal.Requires:Task:&nbsp;  Module:&nbsp;   Figure&nbsp;1129.&nbsp;Name Name of your    task.Shift Select on which regions the shift will be performed.for acquisition with Select an Optical Configuration from which the shift values are taken. Figure&nbsp;1130.&nbsp;Typical example using    to compensate the shift between C2 and NSIM capture:The job defines one    for C2 which is used to    an image on which regions are defined using   . Then the job loops over these regions automatically compensating the shift (  ) between two registered systems (stored in   ).    moves the stage to the center of each region and captures an image using  NSIM Capture.  Large Images &gt;  Edit Regions🔗 Enables the user to edit regions during the job. The job is paused and Simple ROI Editor is shown. Please see    for more details about drawing and editing regions. Once you finish the edits, click Finish to continue the job.Execute the job and change the regions on the fly.Requires:Task:&nbsp;  Module:&nbsp;   Figure&nbsp;1131.&nbsp;Name Name of your    task.Edit regions of Select which regions are to be edited (e.g. from    or from   ).Window size Depending on your preference, choose one of the three window size options: General options (exerts the last used window size), Tiled view (tiles the live window alongside with other windows) or Maximized (Maximizes the live window).Window position If using multiple monitors, this function can put your Draw Regions window onto your Second screen, keep it on the Main screen or embed it directly into the Progress dialog.  Large Images &gt;  Region List🔗 This task is used as a blank list which can be filled with regions inside the General Analysis &gt; Calculations tab (please see  ).Use the analysis to automatically create the list of regions which can be utilized in the jobs sequence later on.Requires:Task:&nbsp;  Module:&nbsp;   Figure&nbsp;1132.&nbsp;Name Name of your task.  Large Images &gt;  Loop over Regions🔗 This task can be used to execute contained tasks over a set of regions in a loop.Capture high-magnification images of all predefined regions.Requires:Task:&nbsp;  Module:&nbsp;   Figure&nbsp;1133.&nbsp;Loop Name of your task.over Selected    task containing regions for the loop.Split Storage per Region This option influences how the system stores the captured data on the disk. If checked, separate ND2 file will be created for each region.  Large Images &gt;  Move to Region Center🔗 Moves to the center of the region.Capture region centers of all predefined regions.Requires:Task:&nbsp;  Module:&nbsp;   Figure&nbsp;1134.&nbsp;Move to Center Position of Defines in which    the movement to the center of a region will be performed.  Large Images &gt;  Append Region List to Region List🔗 Adds selected regions to the selected region list.Append user defined regions to the list created by general analysis.Requires:Task:&nbsp;  Module:&nbsp;   Figure&nbsp;1135.&nbsp;Name Name of your    task.Append all regions from Defines which regions are used for appending the region list.  Large Images &gt;  Clear Region List🔗 Deletes regions from the selected region list.Select a region list and delete its contents.Requires:Task:&nbsp;  Module:&nbsp;   Figure&nbsp;1136.&nbsp;Name Name of your    task.Remove all regions from Defines which region list is to be cleared.  Large Images &gt;  Empty Region🔗 Creates an empty region. This task is useful e.g. to scan an overview of all slides in the slide loader, then the regions are drawn on all slides and the system scans the regions.Add an empty region on low magnification capture for each slide in the slide loader. Then prescan all slides, draw ROIs on each slide manually and scan them in high magnification.Requires:Module:&nbsp;  Create Region on Adds an empty region on the selected capture.  Large Images &gt;  Store Region Overview🔗 Stores an overview image of the regions, which serves as a reference for the goal of the job which is to detect / draw regions in the image with a small resolution are then capture them in high resolution. Without this reference, one does not have much insight into where the individual regions are located.Capture a low magnification (2x objective) image of the whole slide, detect interesting regions on it using General Analysis 3, capture those regions in high magnification (40x objective). From these regions it is possible to save an overview showing where they are stored (as binaries in the intensity image, 0 = nothing, 255 = region). Also ROIs with their names are stored (vector plane over the image).Requires:Task:&nbsp;   or   Module:&nbsp;  Name Name of your    task.Save overview of Source task providing the regions.Images If checked, task requires to select the image source (typically from a    or    task). These images are then reduced to the overview resolution (e.g. 2x objective) and placed into the overview on corresponding places. Thus an overview with real data and not only binaries is created.Burn Region Names If checked, name of the region is burned to each binary region. Size of the region name is set to be legible when viewing all ROIs at once and changes with the image zoom because it is burned directly into the image.Resize Resizes the overview image based on the selected factors (Scale factor, Maximum size). E.g. if a low magnification image was captured using a 10x objective, the resulting image is still huge which is not necessary for a rough overview.  Large Images &gt;  Extract Region List🔗 This task is used for marking and filtering regions. It extracts regions from a region list (From) marked with a Region Tag into a specified region list (to). Use the Expression button to find the region tag.The typical use case involves creating a large list of regions through user interaction, often by drawing them, and then labeling each region with the corresponding text tag and sequentially extracting regions from the large list and populating a smaller list based on their assigned tags. While the extraction process can be accomplished using this specific task, the marking of regions needs to be performed through common tasks such as    and   .  uses the slide loader for iterations over slides. Initially, user defines regions on each slide and then the system automatically scans all the regions. The slide name serves as a tag, ensuring that the system uses the appropriate list of regions for the slide currently being on stage.Requires:Task:&nbsp;  ,   ,   Module:&nbsp;    Large Images &gt;  Use Regions from Sample Navigation🔗 Utilizes the regions created in the    panel.Use the previously drawn regions in the current job.Requires:Module:&nbsp;   ",
     id: 264 }, 
   { title: "Using Time Lapse",
     xmlid: "id|tl_temp_heat_use_casex",
     content: " In the following example we will build a job benefiting from the   ,    and    tasks. This job will guard the temperature of our device and will alter the image capture in compliance with its current temperature. This type of job is advisable especially when working with living objects (e.g. cells). Figure&nbsp;1077.&nbsp;Overview of the job.We start by defining our capture (selecting Optical Configuration) and inserting the first    task where we set the target temperature of our device to 37°C. We go on with inserting a    task. We want the user to have an option to stop the job progress at any time, so we define a Stop Acquisition button and check Continue execution and stay visible. Task setting details are shown in the following picture: Figure&nbsp;1078.&nbsp;During normal conditions we want to capture every 30 minutes and we want the    to enable quitting at any time. Therefore we check the Finish when condition is TRUE and Define the Stop Acquisition button from our question. We continue by inserting our    task followed by an    task containing another   ,    and a   . If the temperature of our device exceeds 40°C, the    stops and an e-mail is sent to the user with custom warnings about the job termination. Figure&nbsp;1079.&nbsp;Our live sample behaves differently under cold and warm temperatures, therefore we right-click on the frame of our    task and choose Add “If Else”. If the temperature of our device is greater and equal than 34°C we want to capture every 30 seconds. Because our specimen moves slower under colder temperatures we define another condition (“Devices.Temp&lt;34”) as in the first case by right-clicking the If task and selecting Add “If Else”. Accordingly if the temperature is lower than 34°C we want to augment the capture interval to 3 minutes (see the following settings). Figure&nbsp;1080.&nbsp; ",
     id: 265 }, 
   { title: "Binary Tracking Step by Step",
     xmlid: "id|track.binaryx",
     content: " (requires:  )This function allows you to track a huge amount of binary objects (thousands of them). Binary tracking assumes that you import a Timelapse image containing a binary layer that will be tracked. For information about preparing a binary layer suitable for tracking, please see:  . Figure&nbsp;1405.&nbsp;Binary TrackingNo universal procedure for binary object tracking exists because each Timelapse image is unique. There are differences in frame rate of the whole sequence, object background separability, size and speed of the objects, etc. In most cases the following course of action should be sufficient for tracking of well separated objects. Nevertheless this procedure may have to be modified with regards to your specific image. Common tracking procedure:Open your Timelapse image with a binary layer containing motile objects to be tracked.Open the Tracking control panel (  ).Click the Settings  button and reset all values to default by clicking Reset Page to Defaults on each tab.If all of your moving objects are not present in the frame and appear later, check Allow New Tracks After First Frame in the Object Tracking tab.Choose the Motion Model.To improve the view, go to the Display tab and check Show probability ellipse and Show trace only to current frame. The overlay mode (click the   Overlay button) can also help with the visual parameter setting.Set the Max Object Speed. Use the Graph view below to find the best fitting value.Set the Standard deviation multiplication factor. Use the ellipses to connect adjacent track points.Remove or stitch tracking fragments using functions available in the Track Processing tab.When satisfied with the tracking, start working with the data for each frame (Data tab) or each track (Tracks tab). Use the advantage of ordering, filtering and exporting. Click the Track Binaries button every time you change the parameter value and analyze the results in the whole time sequence to be able to fine tune the parameter settings.Parameter detailsMotion models are selected based on the captured motion. Generally Constant speed works well, however if you are not sure about the selection, check all of the models and the algorithm will evaluate the best fitting model by itself.In order to speed up the tracking it is a good idea to turn off all Object Features and Track Processing features for few initial tries.The Max Object Speed (maximum cut off speed) should be set first because it restricts the number of combinations the tracking algorithm must try. It is an absolute value which eliminates impossible links right away and no two objects can link if the speed is greater than this limit. Usually, the maximum speed of observed objects is well known for the investigated phenomenon. If you know the exact value, type it into the white field. If the object speed limit is not know, use the graph below to find out the speed value under which most of the tracks appear.The goal of the initial setup is to find the Standard Deviation Multiplication Factor for the given sample while reducing the Max Object Speed as much as possible.Standard Deviation Multiplication Factor (tracking rigidity) is a relative to each object and its history and represents the probability of non-linking. It cuts when the motion would be unexpected. There is an aid to understand it better - the Show probability ellipse in the Display tab. It works after first tracking has been made. A small cross with a solid ellipse is shown for each track point. The cross represents the predicted object position. If an object is found within the predicted restriction limit represented by the circle, its position becomes part of the resulting track. By changing the values, the ellipse grows or shrinks. Dashed ellipse is drawn around predicted point where the object was not found. It should be set so that all consecutive points are contained in the corresponding ellipses.Move the sliders in the Object Tracking tab to see the changes. The goal is to make the circles a little smaller to make gaps and then close them.If the sample is too dense and sampling in time sparse there may be too many points in each ellipse making the probability of failing higher (objects are assigned to incorrect tracks). In this situation it may be better to start the experiment with the Close Gaps Between Tracks function.When you have made the probability ellipses a bit smaller, your objects may sometimes disappear and therefore cannot be tracked in these frames. You should reconnect the separated tracks back to the original path by the Close gaps between Tracks function. In general as the Maximum Gap Size gets higher the prediction accuracy deteriorates quickly.When the rough combination of Standard Deviation Multiplication Factor, Max Object Speed and Maximum Gap Size is determined it is wise to switch on Allow New Tracks After First Frame and use functions in the Track Processing tab to filter out some bad tracks.If objects that belong to the same track have some distinguishing features (e.g. if there are large and small objects) switch on EqDiameter or other features from the Object Features tab to see if it improves the result.When objects cannot be segmented nicely and they break occasionally into smaller pieces it is good to break the tracks in such a place, let introduce a gap and then close it in Track processing. This requires finding the correct limit for EqDiameter or Circularity. The limit is entered as percentage change per second which may yield weird numbers depending on the timelapse frequency.Export your data and tracks to MS Excel or Clipboard, and save graphs as raster files or copy them to the clipboard. Mean square displacement ( ) of the tracks can also be exported to MS Excel and used for the Intra-Nuclear Single Particle Tracking ( ) to indicate the type of diffusion.For more information about Advanced Tracking settings please see:  . ",
     id: 266 }, 
   { title: "Tracking Prerequisites",
     xmlid: "id|track.prereqx",
     content: " Tracking algorithms work on segmented objects which it tries to follow. ROI tracking has the detection phase build in itself. Binary tracking operates on already segmented binary objects (e.g. from threshold, spot detection, ...). The images intended for tracking should have well defined objects in order to be consistently segmented throughout their lifetime.As in many cases this cannot be achieved during acquisition, some transformation (Contrast, Intensity Equalization, Background Removal) or preprocessing algorithms (such as Detect Regional Maxima / Minima or Detect Peaks / Valleys) should be used to improve object separation from the background. The document can be reverted back to original after being tracked with Revert to original Color image while maintaining the tracking result.When the scene brightness is oscillating and ROI tracking is used it will automatically take into account changing LUTs settings to compensate for such oscillations. The keep auto scale feature should be turned ON.When segmenting for binary tracking, special care should be taken of the number of objects, as this greatly influences the complexity of the problem to be solved. Many times it is better to have objects missing than too many objects. Missing object can introduce a gap into a track which can be later closed during tracking. On the other hand including “false” objects (dirt, debris of bigger, etc.) may cause bad tracking results because of high scene complexity.Specifically, when thresholding, it is usually good to consider Cleaning, Smoothing and Filling Holes as well as setting restrictions on object size and possibly on object circularity as well.When object segmentation is satisfactory the Timelapse must have a reasonable frequency (FPS - frames per second) for given object count and speed. The FPS may be lower when there is a low number of objects and therefore low ambiguity. However when the scene is dense the FPS requirement is much stronger. A good guess if any given time-lapse is well sampled is how well a human eye can assess individual object motion. If one cannot easily understand which object is which and where they are heading chances are that it is under sampled. Tracking results are always better with higher frame rate. ",
     id: 267 }, 
   { title: "ROI Tracking Step by Step",
     xmlid: "id|track.roix",
     content: " (requires:  )Object DefinitionStart by opening your Timelapse image and the Tracking control panel (  ). Now you need to define the objects to be tracked. There are two ways to do so:In the image timeline select the frame from which you would like to track an object. Zoom in the image, press the   Define new ROI... button. The Simple ROI editor appears. Click in the middle of an object you want to track. An auto-detection algorithm will determine the object area according to the pixel intensity values in the image. A color line indicates the computer-estimated object edges. Use the tools in the floating toolbar to edit the ROI. When satisfied with the object selection, confirm object definition with a right mouse click or by hitting Enter. Define as many objects as you would like to, then click the Finish button.Second option is to create a new ROI using commands from the ROI menu or the context menu next to the    Turn ROI On/Off button placed on the right toolbar inside the image view. Such created ROIs are tracked manually. The higher contrast between the objects and the background, the better results the tracking will bring. If the contrast in your image sequence is not sufficient, you may need to perform some image processing before the tracking (Please see:  ).You can switch display of defined ROIs to a tree view or a list view. If you need to delete a ROI or all defined ROIs, press the corresponding  or  buttons.TrackingAutomaticallyWhen you finished defining objects, run the automatic tracking procedure:Select the first frame of the image sequence.Press the Track Autodetected ROIs button.All autodetected ROIs are then automatically tracked.ManuallyDefine the objects to be tracked.Switch to the first frame when the object changes its position and move the ROI by mouse to fit the object again. This way you defined the second point of the object's trajectory.Continue with the next following frame. Use the left and right arrow keys to navigate through the Timelapse frame by frame.Trajectory Points in the Graph&nbsp;All trajectories created by tracking can be displayed in the Graph area in the Tracking control panel. Select your ROIs in the ROI list on the left to highlight their tracks in the image and display the track speed curves in the graph. The white dashed vertical line in the graph area represents the current point on the trajectory which corresponds to the displayed image frame. Click inside the graph to place the line and thus select a different frame. If you click and hold the primary mouse button with cursor over the line, velocity values will show up. You can delete a single trajectory point, the whole trajectory, or just one part of it. To do so, press the corresponding buttons in the right graph toolbar.Contextual MenuWhen you invoke the contextual menu over an automatically detected ROI, you can use following commands to improve tracking.Extrapolate ROI Performs the tracking from the last defined point to the end.Interpolate ROI Performs the tracking between two points.Separate ROI Available also for manually tracked objects. This command separates one ROI into two. ",
     id: 268 }, 
   { title: "Visualization",
     xmlid: "id|track.visualizationx",
     content: " The following tools help to visualize the tracking process:Show Polar Graph Element&nbsp;To open the Polar Graph dockable panel navigate to    or click its  icon from the Tracking panel. The graph can be zoomed inside the window - two types of trajectory selection are available, manual by mouse click and using the sector tool. Show Selected Displays only headings / trajectories of objects selected in the Tracking panel. Show All All tracked objects are displayed in the Polar graph. Show Heading Heading of each object is depicted as a color arrow. Its length corresponds to the appropriate object speed. Show Trajectory Projects real object movements into the Polar graph area as color paths. Their length corresponds to the appropriate track length. Sector Selection This tool can be used for filtering tracked objects. Move the yellow radius line to specify the heading range. You can also adjust the concentric circles / sectors position to delimit the line speed. Polar Graph Settings Opens the settings dialog enabling to change the Maximum Length Radius and Maximum Speed Radius so that a comparison between more polar graphs can be made. Figure&nbsp;1406.&nbsp;Show Heading Figure&nbsp;1407.&nbsp;Show TrajectoryOverlay&nbsp;You might want to see the whole image sequence in one image. The   Overlay button can display an additional layer over the current image. To adjust its properties, invoke the Settings command from the menu near the button. Three overlay image types can be selected:Maximum Intensity Projection - pixel values with the same XY coordinates are compared throughout the image sequence and only the pixels with the highest intensity value are displayed in the layer.Minimum Intensity Projection - pixel values with the minimum intensities of the whole image sequence are displayed in the image.Sequence Intensity Projection - this is a special overlay, which dyes the trajectories by a color scale. The movement direction and the object velocity become obvious. Color and Transparency of the layer can be adjusted as well.Show Trace&nbsp;Press the Settings  button and adjust the options in the Display tab. See   for more information about each option.Color of the objects&nbsp;During tracking, some objects may split. You can define which color will hold the new object - either the same color as the original object, or different.Kymograph&nbsp;A kymograph displays pixel intensities changes under a defined linear section over time. A trajectory of a tracked object, or a user-drawn line can be used as the kymograph line.To create a kymograph by line:Select the Create Kymograph by Line command from the   Kymograph pull-down menu. The mouse cursor changes.Click to draw a line in the image. Finalize it by a secondary mouse click outside the drawn line. To edit the line shape, select Edit Points from the context menu (secondary mouse click over the line). Now you can freely move the nodes around, add points using a single mouse click, convert segments to curves (Curve All Segments) and adjust the Bezier curves. Curved segments can be straightened back (Straighten All Segments). After adjusting the line, select Exit Edit Points. To change the width and color of the line, select   Properties from the context menu (secondary mouse click over the line).A new image called “Kymograph” is created. To create a kymograph on selected objectsSelect one of the objects in the objects table the trajectory of which you want to use for the kymograph line.Select the Create Kymograph on Selected Object command from the Kymograph pull-down menu.A new image called “Kymograph” is created.Thickness of the kymograph line and the intensity calculation method can be adjusted in the same pull-down menu after selecting Kymograph Line Settings... (see:   ). ",
     id: 269 }, 
   { title: "Tracking 3D",
     xmlid: "id|tracking.3dx",
     content: " (requires:  )Tracking 3D objects is possible only on images having both T and Z dimensions. 3D objects which will be tracked have to be defined first. This can be done in three ways:Threshold the image using    and then connect 3D objects using    or run the    tab, select your binary layer and click the  button. For each T frame, objects touching at least with their corners will be connected together.Use   Use    or   See:   for more details.Once your 3D objects are defined using one of the methods listed above, run   , display the Tracking Options dialog window by clicking the   Tracking Options... button, define the tracking parameters (see:  ) and click Track 3D Binaries. 3D objects are then connected through the whole timelapse and their tracks are visualized. Figure&nbsp;1426.&nbsp;Tracks of two 3D cells displayed in Volume ViewMeasured features are visualized in the Graph tab and all numeral results including 3D measurement features are listed in the Data table (see:  ). ",
     id: 270 }, 
   { title: "Tracking - Advanced",
     xmlid: "id|tracking.advancedx",
     content: "    (requires:  )If you have this module enabled in HASP, the    module gets extended with several advanced features.Please see also the   section. Object Tracking&nbsp; Figure&nbsp;1422.&nbsp;Object Tracking WindowLinking Select which options apply to the tracking.  Allow New Tracks After First Frame If a new object is detected in frames others than the first one, a new track is started. If this option is turned OFF, the algorithm tries to classify the new object according to the other settings - for example as a branch of an existing track.Track Binaries The algorithm tracks binary objects instead of the detected ROIs. Press this button to perform the tracking.Other Object Tracking options are closely described here:  . Track Properties Figure&nbsp;1423.&nbsp;Track Properties windowColor by Object The track has the same color as the object (or ROI). Select the Show tail option to indicate current direction of the motion. Specify how many segments of the trajectory are used. The Show centroids option displays crosses over the current centroid position.Color by Speed Color of the track changes according to the speed at a particular position.Color by Time The track color changes steadily from the beginning to the end.Coloring by image The colors are used relatively to the duration of the entire image sequence.Coloring by object The colors are used relatively to the duration of each track.Leave trace after object disappears When selected, all detected tracks are displayed on all image frames. Otherwise, only the tracks corresponding to currently visible objects are displayed.Show probability ellipse Check this item to display area of probable next movement predicted by the tracking algorithm.The predicted movement is depicted as a circle described by the Standard Deviation Multiplication Factor (see description here:  ).Show only selected binary objects By checking this option, only binary objects which are selected by mouse within the list of objects are displayed. Other binary objects (and their tracks) remain hidden. This works only with binary objects. ROIs can not be hidden.Show only selected tracks Use this option to hide trajectories of objects which are not currently selected.Unselected Tracks Transparency Set the percentage of unselected tracks transparency.Show trace only to current frame Check this item to display the trajectory always only to the current frame.Show spot velocity vector Check this item to display vector of speed either in every point of the trajectory, only on the current point, or from begin to end. Define the Vector arrow scale in pixels corresponding to μm per second. Advanced tracking contains additional Track processing options. Track processing operates on whole tracks. These tracks are built during the frame-to-frame linking phase defined by Object Tracking and Object Features settings. This is an additional phase that allows further improvement of the tracking results. Summary data obtained by track processing features are shown in the Tracks tab. Track processing is available for binary tracking only. Results of track processing features are shown in the Tracks tab inside the Tracking panel. Figure&nbsp;1424.&nbsp;Tracking Options WindowTrack processing&nbsp;Contrariwise to the Object features tab, these options are track-oriented.Delete Tracks with “N” or Less Frames If an object is detected tracked in less frames than the defined value, the whole track will be deleted.Delete tracks with Line speed lower than ... This option can effectively filter objects which are moving e.g. on the spot (in circles). Line speed of the whole track is taken.See also  .Close Gaps Between Tracks A tracked object can sometimes disappear so that it cannot be tracked within certain number of image frames. When it reappears, the track can be re-connected to the original. Select how many frames can be omitted at most.See Standard Deviation Multiplication Factor ( ). This option is available only if the   option is enabled.Find Splitting Tracks Choose whether a track may split. Specify the probability factor - a number which multiplies the probability of occurrence of a splitting track.  This option is available only if the   option is enabled. The tabs embedded into the Tracking panel show summary information for one track, a selection of tracks or for all tracks created by advanced tracking.The Graph tab displays speed of the objects on Y axis with time value on the X axis by default. More variables can be selected from the Left and Right combo boxes. Data tab displays total information for each frame of the ND2 file.Tracks tab displays overall information related to tracks. Performed operations from the Track Processing tab (Tracking Options window) are shown here.Column sorting is available in the Data and Tracks view. All gathered data can be filtered multiple times by any variable measured during the object tracking. E.g. tracks can be sorted by heading to display only tracks with heading from 0 to 180 degrees. Figure&nbsp;1425.&nbsp; Color as parent Objects are colored as their parent. Color as branch Objects are colored based on their branch. View as Tree Objects are displayed in a tree view. View as List Objects are displayed in a list view. Delete object Deletes the selected object(s). Delete all objects Deletes all objects present in the object list. Binary to ROI This button converts binaries into ROIs.Graph: Left, Right In this section you can select which variables will be displayed in the graph. Update Tracking Features This button updates the tracking features. Show Statistics Shows the tracking statistics in the Tracks tab. Use filter Filters the tracking results using the defined filter. Define Filter Is used to define, edit and manage filters which can be applied by the  Use Filter button.Correct for Stage Movement Turns on/off the stage movement correction eliminating measurement inaccuracies caused by the stage. Delete trajectory to current frame (ROI) Trajectory of the selected object(s) is deleted to the current frame. Delete trajectory from current frame (ROI) Trajectory of the selected object(s) is deleted from the current frame. Delete current tracking point (ROI) Deletes the current tracking point. Delete All Tracking points (ROI) Deletes all tracking points.Context menu over tracked objectsHighlight Object Highlights the selected object.Object Frames Selection Only frames where the object is present are selected.Select All Selects all tracked objects.Unselect All Deselect all tracked objects.Invert Selection Inverts the object selection.Convert Selected Bin Tracks to ROI Converts selected binary tracks into ROI.Copy Selected Bin Tracks into new Binary Layer Copies selected binary tracks into a new binary layer.Move Selected Bin Tracks into new Binary Layer Moves selected binary tracks into a new binary layer.Copy Untracked Objects to new Binary Layer Copies untracked objects into a new binary layer.Move Untracked Objects to new Binary Layer Moves untracked objects into a new binary layer.Delete Selected Deletes the selected object(s).Keep Selected Only Keeps the selected object(s).Bookmark Puts a color bookmark next to the name of the object.Unbookmark Removes the bookmark of the selected object(s).Unbookmark All Removes all bookmarks present in the object list.Invert Bookmarked Bookmark is removed from objects already bookmarked and is added to those not being bookmarked yet.Set object as reference object Selected object is used as a reference object (e.g. for distance measuring).Rename ROI Renames the selected ROI.Align ND Document... Opens the Align Current ND Document dialog window (see:   ).Insert Annotation... Opens the Annotation Properties dialog window used to define an annotation (arrow with text) which is added to each tracked object. Select where the arrow will point on each object (Insert to Position Calculated by Objects) and set the space between the arrow head and the object (Position Offset). Choose the arrow pointing angle (Select Arrow Angle) and adjust the text and arrow properties (Properties...). All the dialog settings can be saved into a preset (Save As Preset) and loaded at any time by selecting the preset name in the Preset combo box. ",
     id: 271 }, 
   { title: "Tracking Options",
     xmlid: "id|tracking.optionsx",
     content: " Running the    command displays the   Tracking  panel. Figure&nbsp;1411.&nbsp;Tracking control panelHere you can display the Tracking Options dialog window by clicking the   Tracking Options... button and later on adjust all your tracking results. Figure&nbsp;1412.&nbsp;Tracking options windowMotion Model Select which model describes best the captured motion. If you choose both options, the algorithm then evaluates which model fits better the motion characteristics of your objects.Centroid Calculation Intensity (Bright), Intensity (Dark) The brightest or the darkest object part is considered as the centroid.Area or Volume Centroids are calculated from object shapes.Linking Select which options apply to the tracking. Allow Gaps in Tracks Specify how many frames may be missing at most in one track.Allow Track Splitting Choose whether a track may split. Specify the probability factor - a number which multiplies the probability of occurrence of a splitting track. Splitting is disabled for objects detected by Spot Detection without the Growing option used.Allow New Tracks After First Frame Check this checkbox if you want to allow new tracks detection after the first frame. Standard Deviation Multiplication Factor The tracking algorithm uses statistics to predict the next position of an object (grey cross in the figure below and corresponding Gaussian curve) based on previous positions of the object (black crosses in figure below). Use this option to set the limits within which the tracked objects shall be found. The limits are basically defined by the multiple of standard deviation (green dashed circle marked by two red lines on the Gaussian curve). By increasing the value you extend the perimeter of the dashed circle. If an object is found within the predicted restriction limit, it is most probably the tracked object. Its position becomes part of the resulting track. Figure&nbsp;1413.&nbsp;Standard Deviation Multiplication FactorMax Object Speed Select if a speed limit is applied. Only trajectories of objects moving within the speed limit will be considered. Use this option when you are sure the object cannot move faster than the set limit.Track Binaries Tracks the binaries in the image.Show Tracking Results This button shows the Tracking results window (Graph).Reset Page To Defaults Click this button to set all settings of the current tab to default values.Track Autodetected ROIs Press this button to perform tracking of autodetected ROIs. The Tracking control panel opens.Load... You can load previously saved settings from an external *.txt file.Save... You can backup parameters of object tracking to an external *. txt file and share it or store it for further use.Help Displays relevant help page.The following options are available for Binary tracking only:Track All Multipoints Performs tracking on all multipoint frames contained in the current ND2 image.Ignore objects touching image borders Objects touching image borders are excluded from the tracking procedure.Track backwards Tracks the timelapse frames in reverse order - from the last frame to the first frame.Use average time difference Sometimes tracking can fail due to bad time intervals sent by the camera. This feature calculates and uses new frame intervals based on average frame length.Object Features&nbsp; Figure&nbsp;1414.&nbsp;Select features to make the tracking method more restrictive. Move the slider or write down the value of the restrictive deviation. Only predicted objects that fit in these limits are taken into account while creating the trajectory.These feature limitations work the similar way as Standard Deviation Multiplication Factor (see description here:  ).Track Processing&nbsp;For Track Processing options please see:  Track Properties&nbsp;For Track Properties options please see:  . ",
     id: 272 }, 
   { title: "Tracking Output Calculation",
     xmlid: "id|tracking.output.calculationx",
     content: "        Figure&nbsp;1415.&nbsp;An Object Track or trajectory (schematically shown in the picture) is a sequence of segments that connect successive object positions (P). Each object position has a time value (t) associated at which it was recorded. Distance between two consecutive Points is the length of that segment. Speed is the Segment Length divided by the amount of time elapsed between the two positions. As the segments are one less the number of points, they are by convention associated with the preceding point.Path is the portion of a track made up of segments from the beginning to the current position. Its Length is the sum of segment distances. Speed is the Length divided by the amount of time elapsed from the beginning to the current position.  Figure&nbsp;1416.&nbsp;The length of Acceleration vector |a| is the measure of total force that is acting on an object. Normal Acceleration (aN) is the part of Acceleration vector (a) that is perpendicular to the Velocity vector (v) incoming to the current object position (P). It represents the deviation force. Tangential Acceleration (aT) is the part of Acceleration vector (a) that has the same direction as the Velocity vector (v) incoming to the current object position (P). It represents the accelerating or decelerating force. In 3D the the calculation is the same: Every consecutive three points define a plane in which acceleration is computed.Since three consecutive positions are needed to calculate Acceleration, its value for the first and the last track point is not available. Figure&nbsp;1417.&nbsp;Size of the acceleration vector  Figure&nbsp;1418.&nbsp;Heading is the the angle between the direction of the Velocity vector and the X axis. Zero degrees is in the direction of X axis (to the right) and increasing counterclockwise until 360. Figure&nbsp;1419.&nbsp;Elevation is the angle between the direction of the Velocity vector and the XY plane. Positive Elevation is towards positive Z coordinates. The values range from -90 to 90.  Figure&nbsp;1420.&nbsp;Line Length is the length of a straight line from the track origin to the current point. The Line Speed is the Line Length divided by the time elapsed. It represents the speed of progression of a given object.  Figure&nbsp;1421.&nbsp;Reference Line Length is the length of a straight line from the Reference Point defined by the user to the current point. This feature may be used to evaluate if the object is proceeding towards the Reference Point or outwards. ",
     id: 273 }, 
   { title: "What You Need to Run Triggered Acquisition",
     xmlid: "id|triggering.needsx",
     content: " CamerasThe following cameras are supported for triggering within NIS-Elements.Andor Clara, Clara EiXon SeriesLucaEM SeriesNeoZylaPhotometrics CoolSNAP KINOCoolSNAP MYOQuantEM 512SCCascade® SeriesEvolve Series  CoolSNAP HQ2 was removed from the list due to a bug found in the camera firmware. Photometrics does not plan to fix it at the moment.Hamamatsu ORCA-R2ORCA-Flash4.0ORCA-Flash2.8ImagEM  The triggering is supported in single-camera and in dual-camera system setups as well. DevicesThe following devices are supported for triggering within NIS-Elements.Wavelength Switchers CoolLED pE-xSutter Lambda DG-4/5Lumencor Aura/Spectra Light EnginesNikon LU-N SeriesNikon LU4AAgilent MLC 400Piezo Z Drives Prior NanoScanZ PiezoZPhysik Instrumente E-665, E-712 and E-861 PiezoZMad City Labs Nano-Drive PiezoZ Software LicencesRT Acquisition, trigger control (LW_RTPIZMUX) This software licence is needed to run any triggered acquisition. Select the RT Acquisition, trigger control module within the AHUS ordering system.Wavelength Switcher (LW_WAVESW) This software licence is needed to run any of the supported wavelength-switching illumination devices. It also includes licences for a Filterwheel and a Shutter. Select the Wavelength Switcher device within the AHUS ordering system. ",
     id: 274 }, 
   { title: "HCA",
     xmlid: "id|ug.hcajobsx",
     content: "     (requires:  ) ",
     id: 275 }, 
   { title: "JOBS and HCA",
     xmlid: "id|ug.jobsx",
     content: "                                                                (requires:  ) ",
     id: 276 }, 
   { title: "Wizard Mode",
     xmlid: "id|wizard.modex",
     content: "     ",
     id: 277 }, 
   { title: "Creating Report",
     xmlid: "id|workflow.analyse.reportx",
     content: " Click   Report Input to open the following window: Figure&nbsp;627.&nbsp;Report input formIt is recommended to fill all the metadata fields but it is optional. When defining a report, the path to a Report template (“.rtt”) file has to be specified in the bottom field of this window. Click Report to open the specified report in the Report Generator. Here it can be adjusted and saved as a new report template.Click   Generate Report to generate the report using the report template specified in the previous step. Figure&nbsp;628.&nbsp;Example of a reportResults can be exported to MS Excel by the   Excel Report (MS Excel application needs to be installed on your PC, the layout of the data which are being exported cannot be modified). Results can also be exported to PDF or printed directly in the Report Generator. ",
     id: 278 }, 
   { title: "Size Classes Definition",
     xmlid: "id|workflow.classesx",
     content: " Before you start scanning and analyzing the filter it is necessary to define the size classes of the particles. You can use either the preset ISO 16232-10 standard, ISO 4406 or create your own size class table.Click   Options from the left toolbar, select the Data tab and click Size Classes Setup.... The following window appears. Figure&nbsp;644.&nbsp;Size Classes Definition Window Pressing the ISO 16232-10 or ISO 4406 automatically generates the classes according to the selected standard.Click New to create a new table. Fill in the empty table by size classes using the New... button on the right. Figure&nbsp;645.&nbsp;New Size ClassThe Class Name has to be a unique word in the scope of all other class names. The class is defined by the From and To field limit values. Set the values in micrometers. A particle belongs to a class if the following expression is valid: From &lt;= particle_size &lt; To. The variable particle_size is a value of the longest particle dimension (so called MaxFeret) according to the ISO 16232 standard. Critical Particle Count defines the critical number of particles which distinguishes whether the whole filter passes the test or not.Confirm the settings by clicking OK. ",
     id: 279 }, 
   { title: "Defining Detection",
     xmlid: "id|workflow.detectionx",
     content: " After scanning the filter (or opening it from a file) you can move on to the filter analysis, which consists of the Particle detection and Measuring.In the filter preview window, make sure the   Show Raster is pressed (it shows a scanning grid in the preview). Choose one field from the scanned filter, on which the detection will be defined. The image of this field can be opened either by double-clicking or by the context menu (Open picture of this field (Double Click) option). By default, 3×3 fields surrounding the opened field are displayed. It can be switched to a 5×5 area in the Measurement tab in   Options. Figure&nbsp;622.&nbsp;Context menu of one fieldA detection preset has to be defined before you start a measurement. It has to be done at least once if the application is executed for the first time. An already defined detection can be reused on the next start of the application.Select the Detection Control tab in the right docking pane: Figure&nbsp;623.&nbsp;Detection Control tab  Figure&nbsp;624.&nbsp;Detection settings dialogFirst select one of the saved Presets or use  Save as...  to create a new one. The key procedure of every detection is Thresholding. You can use it as follows:Adjust the limit values for thresholding in the histogram graph by mouse. Detection is immediately displayed in the opened image as a red binary layer.You can also set the threshold limits for each channel by rewriting the values in the Low and High fields above the histogram.It is also possible to use the picking tools    for thresholding - single point, 3-point circle, or 6-point circle. Click the primary mouse button over the image in a spot which should be part of the thresholded / binary layer. All the image points with the same intensity are selected. Click the secondary mouse click to finish the point thresholding. Binary layer with a highlighted border line around each object is created. To reset the thresholding values, click .You can invert the binary layer using the  button which is especially useful if you want to detect dark and bright (shiny, metallic) particles at once. In such a case, detect the background first (select the gray intensities in the histogram) and then invert the binary layer. The result is, that all particles (darker and brighter than the thresholding limits) are detected.Improve the thresholding by using additional functions such as Clean (Cleans small objects from binary image), Smooth (Border smoothing), Fill holes, or Separate (Separates touching objects). Thresholding is a sufficient procedure for most applications. When measuring according to ISO 16232, only thresholding as the only method is allowed for particle detection. Nevertheless, if an advanced detection is needed, continue with image Preprocessing and Postprocessing definition - see  .Save the changes in the Presets section under the selected preset by Save (see:  ). Unsaved changes to the selected preset are marked with a red dot.If more than one detection preset is available, select the active one from the Measurement Preset drop down menu which can be found in the Detection Control tab, in the Measurement tab in   Options (or directly on the main toolbar).Test the detection by clicking    in the Detection Control tab or by   Check Detection in the left toolbar. ",
     id: 280 }, 
   { title: "Advanced Measurement Options",
     xmlid: "id|workflow.measurement.settingsx",
     content: " Click   Options in the left toolbar to display the Options dialog and select the Measurement tab. Figure&nbsp;638.&nbsp;Measurement settingsSelect which options you want to use during the measurement process.Dialog Box OptionsDetection Select the detection preset which is used for particle analysis in the Preset for detection combo box. You can find more details about this preset here:  . Use the Measurement Preset drop-down menu on the main toolbar. Figure&nbsp;639.&nbsp;Delete particles smaller than option removes particles which do not fit to any of the size classes. This speeds up the detection process but all deleted particles will be omitted from the statistics. Delete particles smaller than [µm] function can be set to an exact size.Large Image “Large image” is created for each field in order to analyze the border-touching particles. Available sizes are 3x3 and 5x5 fields.Metallic Particles If you want to distinguish metallic particles among all the detected particles, check the Detect metallic particles option. Measured data will be displayed in the measurement statistics. Choose one of the two ways how to distinguish the particles:Metallic particle has greater Intensity than [%] If you expect to see bright shining particles or particles with high contrast (mottled), check this option and fill in the MaxIntensity value. Learn... starts the Metallic Particles Detection window: Figure&nbsp;640.&nbsp;The procedure should be done on an image which contains metallic particles. Use the arrows or move the slider to set the proper MaxIntensity value used for detection of metallic particles. Current value of the MaxIntensity feature is indicated below with information about the current number of detected metallic particles for selected MaxIntensity value - these are shown in the specified color directly in the image if the Show Detection option is on.Detect metallic particles using preset Check this option if you want to detect metallic particles using the selected detection preset. This detection should not be the same as the one selected in the Detection Control tab. For more information see:  .Fibers Check Detect Fibers if you want to detect and measure fibers. Measured data will be displayed in the measurement statistics. Only fibers with greater width/height ratio and/or more than a minimal length are detected.Detection Check By default, no images are displayed during the analysis in order to speed it up. If you need to see the images being measured anyway, select the Show detected picture with step option and specify the step. Then, every N-th image will be displayed in an overlay with the detected particles.The ISO 16232 standard specifies the way particles are analysed. A particle which takes place at the edge of the scanned field has to be measured as a whole, although it reaches the neighboring field. The application fulfills this requirement. Neighboring fields are connected to create one large image. The area measured at one time has the size of one scanned field and is marked with a measurement frame. The measurement frame ensures that all particles crossing its top and right border will be measured as a whole and the particles crossing its left and bottom border will not be measured at all (see the image below). A large image containing neighboring images is created for every measured field. In case the scanned field has no neighbor – e.g. a field placed at the edge of the filter – a plain white field is inserted automatically. Figure&nbsp;641.&nbsp;Analysis of filter particlesThe rectangular grid represents image fields. The objects displayed in black will be analysed, the grey objects will be analysed as a part of another field. The measurement frame is highlighted red. ",
     id: 281 }, 
   { title: "Displaying Results",
     xmlid: "id|workflow.resultsx",
     content: " When the filter analysis is finished, the Filter Analysis Results tab is automatically updated: Figure&nbsp;626.&nbsp;Measurement results in the bottom part of the applicationThe following features are measured and displayed in the Filter Analysis Results tab:Size Class Name of the size class.Count Total particle count in each size class.Cumulative Sum Cumulated particle count in each size class calculated in the direction from the smallest to the largest sizes.Reverse Cumulative Sum Cumulated particle count in each size class calculated in the direction from the largest to the smallest class.Count per Ref. Area Particle count recalculated to a reference area.Count per Ref. Volume Particle count recalculated to a reference volume.Critical Value (Count) Critical particle count which distinguishes whether the filter passes the test or not.Critical Value Test Result of the critical value test.In the middle portion of the tab, the following information is displayed:Filter filename Name of the analysed filter.Field count Count of the analysed fields.Area Area of the analysed filter surface.Particle material Material of the detected particles.Particle count Total count of all detected particles.Largest particle Size of the largest detected particle.The right portion of the tab displays a histogram. Choose the type of data which shall be used for the histogram calculation in the Histogram combo-box. The displayed histogram corresponds to the data in the left table and can be changed by clicking on the appropriate row in the table.The option selected in the Material pull-down menu determines which material is used for the statistics. Calculated results depend on the selected Size class which are defined in the Data tab (Size Classes Setup...) inside   Options. Changes made to this table automatically update the measured results. Volume/Area Displays a dialog box, which defines the wetted/reference area and volume and the number of components. This information can be used for calculating additional statistics. Delete all Objects Deletes all detected objects. ",
     id: 282 }, 
   { title: "Advanced Scanning",
     xmlid: "id|workflow.scan.setupx",
     content: " Follow the instructions below:Move the XY stage so that the analyzed part of the filter is in the field of view of the camera.Click   Adjust Light to run the Check Illumination window. This window displays mean pixel intensities and ratio of overexposed pixels in the image. (See  ).Adjust the illumination so that the ratio of overexposed pixels is slightly higher than zero.Start decreasing the intensity of illumination until the ratio equals zero. Stop decreasing the intensity at that moment.Close the illumination check dialog window and go to the Scanning tab in the Setup window. (You can open the dialog by pressing the   Options in the left toolbar). Figure&nbsp;632.&nbsp;Setting parameters for scanning filtersUse the  button to set the Recommended mean intensity [%] value. This function subtracts the mean intensity value of the scene which is currently in the camera field of view. Set the permitted deviation interval in the Tolerance [%] field. It is recommended to set a non-zero tolerance.Adjust the maximal permitted Overexposed pixels ratio. It is recommended to set a non-zero value.Fill in the recommended calibration value in the Calibration field. This prevents the user from scanning a filter with a wrong objective/calibration. If the current calibration on scanning start does not match the value within this field, you will be warned before the scanning starts.In the Scanning Method section you can choose to Use image registration which automatically adjusts the position of the scanned images and helps If there are pixel-shifts between neighboring images.Set the Overlap percentage of one frame overlapping its neighbor and Stitching. Stitching via Blending blends the overlapping image parts, Simple Overlap overlaps the images without blending and Optimal path method computes a contour in places where two overlapping images are least different and then they are stitched copying this contour.See also:  .Apply automatic shading correction automatically corrects illumination inhomogeneities of the captured images.Force to mono 8-bits converts images captured with a color camera to an 8-bit mono image. This function is turned on by default and disabled for mono cameras.Pre-process data during the scan function processes the scanned images during the stage movement. If the stage is decelerated by the computer, turn this function off.Use circular pattern acquires the images in a spiral direction which is recommended when Use autofocus is set in the Focusing tab of the   Options dialog. For None or Use manual focus options turn this function off. Set the output path where the filter image will be saved. The resulting file name will contain the Filename mask, the current user name, date and time of scanning. Example&nbsp;7.&nbsp;If “filter_scan” is filled into the Filename mask field and the user is connected to the computer as “User001”. The final name of the output file can look like “filter_scan-User001-2009-09-07_08-05-11.ND2”.If you check the Default (temporary file) option, the scanned filter has to be saved manually.Confirm the setting by clicking OK. ",
     id: 283 }, 

   // srcData #2 - from the list of all titles/term in an IDMAP - for each rootid
   // titles of separate pages are not added, because they are already there from the previous step (remap.hash == "")
   
   
   
   { title: "Preparing the training binary image", xmlid: "id|nis.ai.modalities.segment.preparex", content: "", id: 284 },
   { title: "Examples of binary drawing", xmlid: "id|p2c2s1s1s2x", content: "", id: 285 },
   { title: "Training, Testing and Running the Segment.ai", xmlid: "id|p2c2s1s1s3x", content: "", id: 286 },
   { title: "Training, Testing and Running the Convert.ai", xmlid: "id|p2c2s1s3s1x", content: "", id: 287 },
   { title: "Training, Testing and Running the Enhance.ai", xmlid: "id|p2c2s1s4s1x", content: "", id: 288 },
   { title: "Analysis Explorer Basics", xmlid: "id|p2c4s1s1x", content: "", id: 289 },
   { title: "Usage", xmlid: "id|p2c4s1s2x", content: "", id: 290 },
   { title: "Managing Recipes", xmlid: "id|p2c4s1s3x", content: "", id: 291 },
   { title: "Batch Analysis Options", xmlid: "id|batch.analysis.optionsx", content: "", id: 292 },
   { title: "Introduction to General Analysis", xmlid: "id|p2c4s6s1x", content: "", id: 293 },
   { title: "Graphical user interface", xmlid: "id|p2c4s6s2x", content: "", id: 294 },
   { title: "How To Set General Analysis", xmlid: "id|p2c4s6s3x", content: "", id: 295 },
   { title: "General Analysis in NIS-Elements - Model Use Case", xmlid: "id|analysis.general.examplex", content: "", id: 296 },
   { title: "General Analysis in JOBS Module - Appended Point Set Use Case", xmlid: "id|ga.usecase.mp.globalx", content: "", id: 297 },
   { title: "General Analysis in JOBS Module - Replaced Point Set Use Case", xmlid: "id|ga.usecase.mp.localx", content: "", id: 298 },
   { title: "Scheme: GA creates local point set", xmlid: "id|fig.local.pointsetx", content: "", id: 299 },
   { title: "Deconvolution methods", xmlid: "id|howto.deconv.methodsx", content: "", id: 300 },
   { title: "Landweber Deconvolution", xmlid: "id|mod.deconvolution.landweberx", content: "", id: 301 },
   { title: "Richardson-Lucy Deconvolution", xmlid: "id|mod.deconvolution.standardx", content: "", id: 302 },
   { title: "Blind Deconvolution", xmlid: "id|mod.deconvolution.blindx", content: "", id: 303 },
   { title: "Fast Deconvolution", xmlid: "id|mod.deconvolution.wienerx", content: "", id: 304 },
   { title: "Resources", xmlid: "id|p2c7s3s5x", content: "", id: 305 },
   { title: "Widefield Sampling", xmlid: "id|p2c7s5s1x", content: "", id: 306 },
   { title: "Confocal Sampling", xmlid: "id|p2c7s5s2x", content: "", id: 307 },
   { title: "Automatic Deconvolution", xmlid: "id|CR__Deconv_Automaticx", content: "", id: 308 },
   { title: "Numerical Aperture", xmlid: "id|numerical.aperture.dlgx", content: "", id: 309 },
   { title: "Immersion Refractive Index", xmlid: "id|immersion.refractive.index.dlgx", content: "", id: 310 },
   { title: "Calibration", xmlid: "id|calibration.dlgx", content: "", id: 311 },
   { title: "Channels", xmlid: "id|channels.dlgx", content: "", id: 312 },
   { title: "Use Spherical Aberration Correction", xmlid: "id|use.spherical.aberration.correction.dlgx", content: "", id: 313 },
   { title: "Create new document", xmlid: "id|create.new.document.dlgx", content: "", id: 314 },
   { title: "Do not show this dialog for images with valid metadata", xmlid: "id|do.not.show.dlgx", content: "", id: 315 },
   { title: "Deconvolution Method", xmlid: "id|deconvolution.method.dlgx", content: "", id: 316 },
   { title: "Modality", xmlid: "id|modality.dlgx", content: "", id: 317 },
   { title: "Pinhole size", xmlid: "id|pinhole.size.dlgx", content: "", id: 318 },
   { title: "Slit orientation", xmlid: "id|slit.orientation.dlgx", content: "", id: 319 },
   { title: "Magnification", xmlid: "id|magnification.dlgx", content: "", id: 320 },
   { title: "Numerical Aperture", xmlid: "id|numerical.aperture.dlgx", content: "", id: 321 },
   { title: "Immersion Refractive Index", xmlid: "id|immersion.refractive.index.dlgx", content: "", id: 322 },
   { title: "Calibration", xmlid: "id|calibration.dlgx", content: "", id: 323 },
   { title: "Z-Step", xmlid: "id|z.step.dlgx", content: "", id: 324 },
   { title: "Noise Level", xmlid: "id|noise.level.dlgx", content: "", id: 325 },
   { title: "Iterations", xmlid: "id|iterations.dlgx", content: "", id: 326 },
   { title: "Import PSF", xmlid: "id|import.psf.dlgx", content: "", id: 327 },
   { title: "Use Spherical Aberration Correction", xmlid: "id|use.spherical.aberration.correction.dlgx", content: "", id: 328 },
   { title: "Output", xmlid: "id|output.dlgx", content: "", id: 329 },
   { title: "Channels", xmlid: "id|channels.dlgx", content: "", id: 330 },
   { title: "Preprocessing", xmlid: "id|preprocessing.dlgx", content: "", id: 331 },
   { title: "Remove spurious high intensity pixels", xmlid: "id|remove.high.intensity.pixelsx", content: "", id: 332 },
   { title: "Use Spherical Aberration Correction", xmlid: "id|use.spherical.aberration.correction.dlgx", content: "", id: 333 },
   { title: "Import PSF", xmlid: "id|import.psf.dlgx", content: "", id: 334 },
   { title: "Run on ROI", xmlid: "id|region.of.interest.dlgx", content: "", id: 335 },
   { title: "Deconvolve", xmlid: "id|deconvolve.dlgx", content: "", id: 336 },
   { title: "Define Background", xmlid: "id|deconv.setting.detect.backgroundx", content: "", id: 337 },
   { title: "Deconvolution Method", xmlid: "id|deconvolution.method.dlgx", content: "", id: 338 },
   { title: "Modality", xmlid: "id|modality.dlgx", content: "", id: 339 },
   { title: "Pinhole size", xmlid: "id|pinhole.size.dlgx", content: "", id: 340 },
   { title: "Slit orientation", xmlid: "id|slit.orientation.dlgx", content: "", id: 341 },
   { title: "Magnification", xmlid: "id|magnification.dlgx", content: "", id: 342 },
   { title: "Numerical Aperture", xmlid: "id|numerical.aperture.dlgx", content: "", id: 343 },
   { title: "Refraction Index", xmlid: "id|refraction.index.dlgx", content: "", id: 344 },
   { title: "Calibration", xmlid: "id|calibration.dlgx", content: "", id: 345 },
   { title: "Output", xmlid: "id|output.dlgx", content: "", id: 346 },
   { title: "Z-Step", xmlid: "id|z.step.dlgx", content: "", id: 347 },
   { title: "Noise Level", xmlid: "id|noise.level.dlgx", content: "", id: 348 },
   { title: "Iterations", xmlid: "id|iterations.dlgx", content: "", id: 349 },
   { title: "Specimen Thickness", xmlid: "id|specimen.thickness.dlgx", content: "", id: 350 },
   { title: "Import PSF", xmlid: "id|import.psf.dlgx", content: "", id: 351 },
   { title: "Channels", xmlid: "id|channels.dlgx", content: "", id: 352 },
   { title: "Preprocessing", xmlid: "id|preprocessing.dlgx", content: "", id: 353 },
   { title: "Remove spurious high intensity pixels", xmlid: "id|remove.high.intensity.pixelsx", content: "", id: 354 },
   { title: "Use Spherical Aberration Correction", xmlid: "id|use.spherical.aberration.correction.dlgx", content: "", id: 355 },
   { title: "Import PSF", xmlid: "id|import.psf.dlgx", content: "", id: 356 },
   { title: "Run on ROI", xmlid: "id|region.of.interest.dlgx", content: "", id: 357 },
   { title: "Deconvolve", xmlid: "id|deconvolve.dlgx", content: "", id: 358 },
   { title: "Specimen Thickness", xmlid: "id|specimen.thickness.dlgx", content: "", id: 359 },
   { title: "Contrast enhancement", xmlid: "id|contrast.enhancement.dlgx", content: "", id: 360 },
   { title: "Exploring the surface view", xmlid: "id|surfacex", content: "", id: 361 },
   { title: "Analysis Overview", xmlid: "id|p2c9s1s1x", content: "", id: 362 },
   { title: "Filter Particle Analysis Layout", xmlid: "id|p2c9s1s2x", content: "", id: 363 },
   { title: "Hardware Setup", xmlid: "id|p2c10s2s1x", content: "", id: 364 },
   { title: "FRET Calibration", xmlid: "id|fret.calibrationx", content: "", id: 365 },
   { title: "FRET Method", xmlid: "id|fret.methodx", content: "", id: 366 },
   { title: "Setting up Optical configurations", xmlid: "id|p2c10s7s1x", content: "", id: 367 },
   { title: "Configuring 6D Dialog", xmlid: "id|p2c10s7s2x", content: "", id: 368 },
   { title: "Setting up ROIs on Preview Image", xmlid: "id|p2c10s7s3x", content: "", id: 369 },
   { title: "Time Measurements Set Up", xmlid: "id|p2c10s7s4x", content: "", id: 370 },
   { title: "Capturing the Data Set", xmlid: "id|p2c10s7s5x", content: "", id: 371 },
   { title: "Calcium Calibration", xmlid: "id|p2c10s7s6x", content: "", id: 372 },
   { title: "Export Data", xmlid: "id|p2c10s7s7x", content: "", id: 373 },
   { title: "Copy image", xmlid: "id|p2c10s8s1x", content: "", id: 374 },
   { title: "Inverted image", xmlid: "id|p2c10s8s2x", content: "", id: 375 },
   { title: "Gamma image", xmlid: "id|p2c10s8s3x", content: "", id: 376 },
   { title: "Difference image", xmlid: "id|p2c10s8s4x", content: "", id: 377 },
   { title: "Increasing intensity image", xmlid: "id|p2c10s8s5x", content: "", id: 378 },
   { title: "Measurement &gt; Basic &gt; Volume", xmlid: "id|CR_Meas2_VolumeMeasurementx", content: "", id: 379 },
   { title: "Measurement &gt; Basic &gt; Object", xmlid: "id|CR_Meas2_Object3dMeasurementx", content: "", id: 380 },
   { title: "Measurement &gt; Basic &gt; Object Count", xmlid: "id|CR_Meas2_Object3dCountx", content: "", id: 381 },
   { title: "Measurement &gt; Basic &gt; Parent", xmlid: "id|CR_Meas2_Parent3dMeasurementx", content: "", id: 382 },
   { title: "Measurement &gt; Basic &gt; Children", xmlid: "id|CR_Meas2_Children3dMeasurementx", content: "", id: 383 },
   { title: "Reference", xmlid: "id|ga3.parenting.reference.abx", content: "", id: 384 },
   { title: "Generate bins automatically", xmlid: "id|generate.bins.automaticallyx", content: "", id: 385 },
   { title: "All Columns", xmlid: "id|CR_GA3_GraphDataSeriesx", content: "", id: 386 },
   { title: "All Columns", xmlid: "id|CR_GA3_GraphErrorBarx", content: "", id: 387 },
   { title: "All Columns", xmlid: "id|CR_GA3_GraphDataSeries.duplx", content: "", id: 388 },
   { title: "All Columns", xmlid: "id|CR_GA3_GraphErrorBar.duplx", content: "", id: 389 },
   { title: "Jobs Explorer and Jobs Toolbar", xmlid: "id|jobs.explorer.hcax", content: "", id: 390 },
   { title: "Analysis performed on job results", xmlid: "id|analysis.on.captured.datax", content: "", id: 391 },
   { title: "Main Concept", xmlid: "id|p2c15s1s1x", content: "", id: 392 },
   { title: "Introduction to Jobs Explorer", xmlid: "id|jobs.explorerx", content: "", id: 393 },
   { title: "Jobs Explorer Options", xmlid: "id|p2c15s1s3x", content: "", id: 394 },
   { title: "Job Definition Window", xmlid: "id|jobs.definitionx", content: "", id: 395 },
   { title: "Job Definition Window Tools", xmlid: "id|p2c15s1s5x", content: "", id: 396 },
   { title: "Save OME Metadata", xmlid: "id|opt.save.ome.metadatax", content: "", id: 397 },
   { title: "Jobs Toolbar", xmlid: "id|jobs.jobstoolbarx", content: "", id: 398 },
   { title: "New Job from Template", xmlid: "id|jobs_using_job_templatesx", content: "", id: 399 },
   { title: "New Blank Job", xmlid: "id|jobs.createx", content: "", id: 400 },
   { title: "Using Job Definition Wizard", xmlid: "id|jobs.definition.wizardx", content: "", id: 401 },
   { title: "Edit Wizard", xmlid: "id|jobs.edit.wizardx", content: "", id: 402 },
   { title: "Defining Samples", xmlid: "id|jobs.define.samplex", content: "", id: 403 },
   { title: "Defining Capture", xmlid: "id|jobs.defining.capturex", content: "", id: 404 },
   { title: "Defining Loops", xmlid: "id|jobs.defining.loopsx", content: "", id: 405 },
   { title: "Defining Parameters", xmlid: "id|jobs.defining.parametersx", content: "", id: 406 },
   { title: "Job Execution Progress window", xmlid: "id|jobs.execution.windowx", content: "", id: 407 },
   { title: "Result View", xmlid: "id|p2c15s5s2x", content: "", id: 408 },
   { title: "Job Result: Thumbnail View (Heatmap)", xmlid: "id|fig.job.result.heatmapx", content: "", id: 409 },
   { title: "Job Result: Grid View", xmlid: "id|fig.job.result.gridx", content: "", id: 410 },
   { title: "Graph View", xmlid: "id|hca.graph.viewx", content: "", id: 411 },
   { title: "Context menu on a job", xmlid: "id|p2c15s5s4x", content: "", id: 412 },
   { title: "Context menu on a job run", xmlid: "id|p2c15s5s5x", content: "", id: 413 },
   { title: "Labels and Metadata in Result View", xmlid: "id|p2c15s8s2x", content: "", id: 414 },
   { title: "Importing predefined presets from spreadsheet applications", xmlid: "id|labeling.preset.formatsx", content: "", id: 415 },
   { title: "Labeling during runtime", xmlid: "id|p2c15s8s4x", content: "", id: 416 },
   { title: "Database context menu", xmlid: "id|jobs.explorer.dbx", content: "", id: 417 },
   { title: "Database Backup", xmlid: "id|jobs.backup.databasex", content: "", id: 418 },
   { title: "Database Restore", xmlid: "id|jobs.restore.databasex", content: "", id: 419 },
   { title: "Database Properties", xmlid: "id|jobs.database.propertiesx", content: "", id: 420 },
   { title: "Database Files", xmlid: "id|p2c15s9s1s5x", content: "", id: 421 },
   { title: "NIS-Elements Upgrades", xmlid: "id|p2c15s9s1s6x", content: "", id: 422 },
   { title: "Database Upload/Download", xmlid: "id|jobs.database.mergex", content: "", id: 423 },
   { title: "Nested loops", xmlid: "id|p2c15s9s3s3s1x", content: "", id: 424 },
   { title: "Succeeding loops", xmlid: "id|p2c15s9s3s3s2x", content: "", id: 425 },
   { title: "Automatic Index Finding Using a Pattern", xmlid: "id|p2c15s9s3s5s1x", content: "", id: 426 },
   { title: "Manual Index Definition", xmlid: "id|jobs.advanced.import.manual.indexx", content: "", id: 427 },
   { title: "Example 1", xmlid: "id|p2c15s9s3s7s1x", content: "", id: 428 },
   { title: "Example 2", xmlid: "id|p2c15s9s3s7s2x", content: "", id: 429 },
   { title: "Example 3", xmlid: "id|p2c15s9s3s7s3x", content: "", id: 430 },
   { title: "Select from DB...", xmlid: "id|jobs_select_wellplatex", content: "", id: 431 },
   { title: "Custom Well Plate Options", xmlid: "id|custom.wellplatex", content: "", id: 432 },
   { title: "Features of the Grain Size Analysis Module", xmlid: "id|p2c19s1s1x", content: "", id: 433 },
   { title: "Supported Industrial Standards", xmlid: "id|gs.standardsx", content: "", id: 434 },
   { title: "Methods", xmlid: "id|p2c19s1s3x", content: "", id: 435 },
   { title: "Measurement Modes", xmlid: "id|p2c19s1s4x", content: "", id: 436 },
   { title: "Copyright Information", xmlid: "id|p2c19s1s5x", content: "", id: 437 },
   { title: "Advanced Detection", xmlid: "id|advanced.detectionx", content: "", id: 438 },
   { title: "Sequential Measurement", xmlid: "id|CR_ME_GSStartMeasureSequencex", content: "", id: 439 },
   { title: "Batch Analysis", xmlid: "id|molecule.reference.batch.analysisx", content: "", id: 440 },
   { title: "Typical Workflow", xmlid: "id|p2c20s2s3s1s1x", content: "", id: 441 },
   { title: "Create raster document from molecules", xmlid: "id|molecule.reference.create.rasterx", content: "", id: 442 },
   { title: "Determining Min and Max Height", xmlid: "id|molecule.reference.determining.min.max.heightx", content: "", id: 443 },
   { title: "Identification Settings", xmlid: "id|molecule.reference.identification.settingsx", content: "", id: 444 },
   { title: "Molecule Drift Correction", xmlid: "id|molecule.reference.drift.correctionx", content: "", id: 445 },
   { title: "Molecule List Text File Format", xmlid: "id|molecule.reference.molecule.list.text.filex", content: "", id: 446 },
   { title: "ROI Statistics", xmlid: "id|molecule.reference.roi.statisticsx", content: "", id: 447 },
   { title: "XY Warp", xmlid: "id|molecule.reference.xy.warpx", content: "", id: 448 },
   { title: "Z-Calibration", xmlid: "id|molecule.reference.zcalibrationx", content: "", id: 449 },
   { title: "Z-Stack Alignment", xmlid: "id|molecule.reference.zstack.alignmentx", content: "", id: 450 },
   { title: "Z-Stacked STORM", xmlid: "id|molecule.reference.layersx", content: "", id: 451 },
   { title: "Overview", xmlid: "id|p2c20s2s3s11s1x", content: "", id: 452 },
   { title: "File naming convention", xmlid: "id|p2c20s2s3s11s2x", content: "", id: 453 },
   { title: "Z stack STORM analysis", xmlid: "id|p2c20s2s3s11s3x", content: "", id: 454 },
   { title: "Working with Z stack STORM molecule data", xmlid: "id|p2c20s2s3s11s4x", content: "", id: 455 },
   { title: "Managing N-STORM Z Stack (Layers table)", xmlid: "id|p2c20s2s3s11s5x", content: "", id: 456 },
   { title: "Frequently Asked Questions", xmlid: "id|molecule.reference.faqx", content: "", id: 457 },
   { title: "Overview", xmlid: "id|p2c20s3s1x", content: "", id: 458 },
   { title: "Acquisition of Z-Calibrations with Agilent laser box", xmlid: "id|p2c20s3s2x", content: "", id: 459 },
   { title: "Acquisition of Z-Calibrations with LU4 laser box", xmlid: "id|p2c20s3s3x", content: "", id: 460 },
   { title: "3D Calibration", xmlid: "id|p2c20s3s4x", content: "", id: 461 },
   { title: "Generating a 2D Warp", xmlid: "id|p2c20s3s5x", content: "", id: 462 },
   { title: "Error messages during 3D calibrations", xmlid: "id|p2c20s3s6x", content: "", id: 463 },
   { title: "N-STORM calibration commonly asked questions", xmlid: "id|p2c20s3s7x", content: "", id: 464 },
   { title: "Master Camera Wiring", xmlid: "id|p2c21s3s1x", content: "", id: 465 },
   { title: "Wavelength Switcher Wiring", xmlid: "id|p2c21s3s2x", content: "", id: 466 },
   { title: "DG4 Shutter HW box", xmlid: "id|cable.hwboxx", content: "", id: 467 },
   { title: "CoolLED pE-x break-out box", xmlid: "id|cable.coolledx", content: "", id: 468 },
   { title: "Piezo Z Wiring", xmlid: "id|p2c21s3s3x", content: "", id: 469 },
   { title: "68pin-2BNC Real Time Acquisition Cable", xmlid: "id|cable.piezox", content: "", id: 470 },
   { title: "Wiring Overview", xmlid: "id|triggering.wire.overviewx", content: "", id: 471 },
   { title: "Wiring Examples", xmlid: "id|p2c21s3s5x", content: "", id: 472 },
   { title: "Installation of a DAQ board", xmlid: "id|p2c21s4s1x", content: "", id: 473 },
   { title: "Camera Settings", xmlid: "id|p2c21s4s2x", content: "", id: 474 },
   { title: "Device Manager Settings", xmlid: "id|triggering.device.managerx", content: "", id: 475 },
   { title: "Acquisition Settings", xmlid: "id|howto.triggeredacquisition.settingsx", content: "", id: 476 },
   { title: "Advanced Settings...", xmlid: "id|triggering.setup.advancedx", content: "", id: 477 },
   { title: "Creating Optical Configurations for Multi-channel Triggered Acquisition", xmlid: "id|optical.conf.for.triggeringx", content: "", id: 478 },
   { title: "Using speed up with NIDAQ board", xmlid: "id|nidaq.tirecipex", content: "", id: 479 },
   { title: "Standard Deviation Multiplication Factor", xmlid: "id|standard.deviation.multi.factorx", content: "", id: 480 },
   { title: "Frame-to-frame object linking", xmlid: "id|p2c24s9s1x", content: "", id: 481 },
   { title: "Object Features", xmlid: "id|p2c24s9s2x", content: "", id: 482 },
   { title: "Track Processing", xmlid: "id|p2c24s9s3x", content: "", id: 483 },
   { title: "Track Segments", xmlid: "id|tracking.output.track.segmentsx", content: "", id: 484 },
   { title: "Acceleration", xmlid: "id|tracking.output.accelerationx", content: "", id: 485 },
   { title: "Heading and Elevation", xmlid: "id|tracking.output.heading.elevationx", content: "", id: 486 },
   { title: "Line Length and Speed", xmlid: "id|tracking.output.line.speedx", content: "", id: 487 },
   { title: "Reference Line Length", xmlid: "id|tracking.output.reference.line.lengthx", content: "", id: 488 },
   { title: "Tracking options", xmlid: "id|p2c24s11s1x", content: "", id: 489 },
   { title: "Allow New Tracks After First Frame", xmlid: "id|new.tracks.after.ffx", content: "", id: 490 },
   { title: "Track Properties", xmlid: "id|mod.tracking.options.displayx", content: "", id: 491 },
   { title: "Track processing", xmlid: "id|mod.tracking.options.processingx", content: "", id: 492 },
   { title: "Graph, Data and Tracks tabs", xmlid: "id|mod.tracking.options.graphx", content: "", id: 493 },
   { title: "NIS-Elements configuration shall include:", xmlid: "id|p2c24s16s2s1x", content: "", id: 494 },
   { title: "Images acquisition", xmlid: "id|p2c24s16s2s2x", content: "", id: 495 },
   { title: "Illumination sequence", xmlid: "id|p2c24s16s2s3x", content: "", id: 496 },
   { title: "Single file outputs", xmlid: "id|p2c24s16s7s1x", content: "", id: 497 },
   { title: "Batch", xmlid: "id|p2c24s16s7s2x", content: "", id: 498 },
   { title: "Mean square displacement (MSD)", xmlid: "id|ispt.msdx", content: "", id: 499 },
   { title: "Advanced detection", xmlid: "id|p2c24s16s8s1x", content: "", id: 500 },
   { title: "Miss-assignments", xmlid: "id|p2c24s16s9s1x", content: "", id: 501 },
   { title: "Miss-assignment and tracking radius", xmlid: "id|p2c24s16s9s2x", content: "", id: 502 },
   { title: "Estimating miss-assignments with I-SPT", xmlid: "id|p2c24s16s9s3x", content: "", id: 503 },
   { title: "MSD", xmlid: "id|p2c24s16s10s1x", content: "", id: 504 },
   { title: "Step-translocation histograms", xmlid: "id|p2c24s16s10s2x", content: "", id: 505 },
   { title: "Correlation statistics", xmlid: "id|p2c24s16s10s3x", content: "", id: 506 },
   { title: "Basic Workflow", xmlid: "id|weld.basicx", content: "", id: 507 },
   
   
   
   // srcData #3 - only GUI commands etc. (iscommand: 1)
   
   
   
   
   
]

// creates searchable index from source data
var indexmods = lunr(function () {
  this.use(lunr.multiLanguage('en'))
  this.field('title', {boost: 20})
  this.field('xmlid')
  this.field('content')
  this.ref('id')
  
  sourceDatamods.forEach((it) => {this.add(it)})

});
// builds reference data - data to be shown as search results

// refData #1 for HTML page: display title and ancestors
var storemods = [{
  "title": "Define the classifier of air voids",
  "ancestors":  "(Concrete)" ,
  "filename": "CR_CONC_ClassifyAirVoids.html",
  "hash": "",
  "id": 0, 
},{
  "title": "Measure",
  "ancestors":  "(Concrete)" ,
  "filename": "CR_CONC_Metering.html",
  "hash": "",
  "id": 1, 
},{
  "title": "Options",
  "ancestors":  "(Concrete)" ,
  "filename": "CR_CONC_Settings.html",
  "hash": "",
  "id": 2, 
},{
  "title": "Database View within Organizer",
  "ancestors":  "(Database)" ,
  "filename": "Database.View.html",
  "hash": "",
  "id": 3, 
},{
  "title": "Functions, groups and sections",
  "ancestors":  "(General Analysis 3 &gt; Basic Control)" ,
  "filename": "GA3_Functions.html",
  "hash": "",
  "id": 4, 
},{
  "title": "Illumination Sequence",
  "ancestors":  "" ,
  "filename": "GS_HowTo.IlluminationSequence.html",
  "hash": "",
  "id": 5, 
},{
  "title": "Creating illumination patterns",
  "ancestors":  "(Illumination Sequence)" ,
  "filename": "GS_HowTo.IlluminationSequence.pattern.html",
  "hash": "",
  "id": 6, 
},{
  "title": "Ratio Experiment, Calcium Calibration",
  "ancestors":  "(FRET)" ,
  "filename": "GS_HowTo.ratio.html",
  "hash": "",
  "id": 7, 
},{
  "title": "Supported cameras",
  "ancestors":  "(Illumination Sequence &gt; Supported cameras and illumination devices)" ,
  "filename": "ISS.supported.cameras.html",
  "hash": "",
  "id": 8, 
},{
  "title": "Supported illumination devices",
  "ancestors":  "(Illumination Sequence &gt; Supported cameras and illumination devices)" ,
  "filename": "ISS.supported.illumination.html",
  "hash": "",
  "id": 9, 
},{
  "title": "Usage Examples",
  "ancestors":  "(Illumination Sequence)" ,
  "filename": "IS_examples.html",
  "hash": "",
  "id": 10, 
},{
  "title": "Data Panel",
  "ancestors":  "(Metalo - Grain Size Analysis &gt; User Interface)" ,
  "filename": "Mod_Metalo.Results.html",
  "hash": "",
  "id": 11, 
},{
  "title": "Nodes",
  "ancestors":  "(General Analysis 3)" ,
  "filename": "action.reference.html",
  "hash": "",
  "id": 12, 
},{
  "title": "Continue Training",
  "ancestors":  "(NIS.ai)" ,
  "filename": "ai.continue.training.html",
  "hash": "",
  "id": 13, 
},{
  "title": "Cell Motility",
  "ancestors":  "(Bio Analysis Modules)" ,
  "filename": "analysis.cell.motility.html",
  "hash": "",
  "id": 14, 
},{
  "title": "Cell Proliferation",
  "ancestors":  "(Bio Analysis Modules)" ,
  "filename": "analysis.cell.proliferation.html",
  "hash": "",
  "id": 15, 
},{
  "title": "Cell Count Analysis",
  "ancestors":  "(Bio Analysis Modules)" ,
  "filename": "analysis.count.html",
  "hash": "",
  "id": 16, 
},{
  "title": "Analysis Explorer",
  "ancestors":  "(Bio Analysis Modules)" ,
  "filename": "analysis.explorer.html",
  "hash": "",
  "id": 17, 
},{
  "title": "General Analysis",
  "ancestors":  "(Bio Analysis Modules)" ,
  "filename": "analysis.general.html",
  "hash": "",
  "id": 18, 
},{
  "title": "Image Intensity Analysis",
  "ancestors":  "(Bio Analysis Modules)" ,
  "filename": "analysis.intensity.html",
  "hash": "",
  "id": 19, 
},{
  "title": "Live/Dead Analysis",
  "ancestors":  "(Bio Analysis Modules)" ,
  "filename": "analysis.livedead.html",
  "hash": "",
  "id": 20, 
},{
  "title": "Bio Analysis Modules",
  "ancestors":  "" ,
  "filename": "analysis.modules.html",
  "hash": "",
  "id": 21, 
},{
  "title": "Sperm Motility",
  "ancestors":  "(Bio Analysis Modules)" ,
  "filename": "analysis.sperm.motility.html",
  "hash": "",
  "id": 22, 
},{
  "title": "Single Particle Tracking",
  "ancestors":  "(Tracking)" ,
  "filename": "analysis.spt.html",
  "hash": "",
  "id": 23, 
},{
  "title": "Wound Healing",
  "ancestors":  "(Bio Analysis Modules)" ,
  "filename": "analysis.wound.healing.html",
  "hash": "",
  "id": 24, 
},{
  "title": "Concrete",
  "ancestors":  "" ,
  "filename": "app.concrete.html",
  "hash": "",
  "id": 25, 
},{
  "title": "Deconvolution",
  "ancestors":  "" ,
  "filename": "app.deconvolution.html",
  "hash": "",
  "id": 26, 
},{
  "title": "Extended Depth of Focus",
  "ancestors":  "" ,
  "filename": "app.edf.html",
  "hash": "",
  "id": 27, 
},{
  "title": "Filter Particle Analysis",
  "ancestors":  "" ,
  "filename": "app.filters.html",
  "hash": "",
  "id": 28, 
},{
  "title": "FRET",
  "ancestors":  "" ,
  "filename": "app.fret.html",
  "hash": "",
  "id": 29, 
},{
  "title": "General Analysis 3",
  "ancestors":  "" ,
  "filename": "app.ga3.html",
  "hash": "",
  "id": 30, 
},{
  "title": "HDR",
  "ancestors":  "" ,
  "filename": "app.hdr.html",
  "hash": "",
  "id": 31, 
},{
  "title": "Layer Thickness Measurement",
  "ancestors":  "" ,
  "filename": "app.layers.html",
  "hash": "",
  "id": 32, 
},{
  "title": "Metalo - Cast Iron Analysis",
  "ancestors":  "" ,
  "filename": "app.metalo.castiron.html",
  "hash": "",
  "id": 33, 
},{
  "title": "Metalo - Grain Size Analysis",
  "ancestors":  "" ,
  "filename": "app.metalo.grainsize.html",
  "hash": "",
  "id": 34, 
},{
  "title": "N-STORM",
  "ancestors":  "" ,
  "filename": "app.nstorm.html",
  "hash": "",
  "id": 35, 
},{
  "title": "Object Classifier",
  "ancestors":  "" ,
  "filename": "app.objectclassifier.html",
  "hash": "",
  "id": 36, 
},{
  "title": "Smart Experiment",
  "ancestors":  "" ,
  "filename": "app.se.html",
  "hash": "",
  "id": 37, 
},{
  "title": "Measurement Sequencer",
  "ancestors":  "" ,
  "filename": "app.sequencer.html",
  "hash": "",
  "id": 38, 
},{
  "title": "Weld Measurement",
  "ancestors":  "" ,
  "filename": "app.welds.html",
  "hash": "",
  "id": 39, 
},{
  "title": "Application of the Standards in the Planimetric Method",
  "ancestors":  "(Metalo - Grain Size Analysis &gt; Planimetric Method)" ,
  "filename": "application.of.standards.planimetric.html",
  "hash": "",
  "id": 40, 
},{
  "title": "Automatic Deconvolution",
  "ancestors":  "(Deconvolution &gt; Dialog Settings)" ,
  "filename": "automatic.deconvolution.html",
  "hash": "",
  "id": 41, 
},{
  "title": "Comparison Charts Workflow",
  "ancestors":  "(Metalo - Grain Size Analysis &gt; Comparative Method)" ,
  "filename": "comparison.charts.workflow.html",
  "hash": "",
  "id": 42, 
},{
  "title": "Data output and Reports",
  "ancestors":  "(Metalo - Grain Size Analysis)" ,
  "filename": "data.output.and.reports.met.html",
  "hash": "",
  "id": 43, 
},{
  "title": "Data Panel",
  "ancestors":  "(Metalo - Grain Size Analysis &gt; Reference)" ,
  "filename": "data.panel.html",
  "hash": "",
  "id": 44, 
},{
  "title": "Database",
  "ancestors":  "" ,
  "filename": "database.html",
  "hash": "",
  "id": 45, 
},{
  "title": "Algorithms",
  "ancestors":  "(Deconvolution)" ,
  "filename": "deconv.algorithms.html",
  "hash": "",
  "id": 46, 
},{
  "title": "2D Deconvolution",
  "ancestors":  "(Deconvolution &gt; Dialog Settings)" ,
  "filename": "deconv.settings.2d.html",
  "hash": "",
  "id": 47, 
},{
  "title": "3D Deconvolution",
  "ancestors":  "(Deconvolution &gt; Dialog Settings)" ,
  "filename": "deconv.settings.3d.html",
  "hash": "",
  "id": 48, 
},{
  "title": "Live De-Blur Setup",
  "ancestors":  "(Deconvolution &gt; Dialog Settings)" ,
  "filename": "deconv.settings.livedeblur.html",
  "hash": "",
  "id": 49, 
},{
  "title": "Detection Panel",
  "ancestors":  "(Metalo - Grain Size Analysis &gt; User Interface)" ,
  "filename": "detection.panel.html",
  "hash": "",
  "id": 50, 
},{
  "title": "Detection Panel",
  "ancestors":  "(Metalo - Grain Size Analysis &gt; Reference)" ,
  "filename": "detection.panel.ref.html",
  "hash": "",
  "id": 51, 
},{
  "title": "Direct Mode",
  "ancestors":  "(Metalo - Grain Size Analysis)" ,
  "filename": "direct.mode.html",
  "hash": "",
  "id": 52, 
},{
  "title": "Edit Area in the Focused Image",
  "ancestors":  "(Extended Depth of Focus)" ,
  "filename": "edf.edit.focused.area.html",
  "hash": "",
  "id": 53, 
},{
  "title": "Real Time EDF",
  "ancestors":  "(Extended Depth of Focus)" ,
  "filename": "edf.realtime.html",
  "hash": "",
  "id": 54, 
},{
  "title": "Real Time EDF Manually (without motorized Z)",
  "ancestors":  "(Extended Depth of Focus)" ,
  "filename": "edf.realtime.manual.html",
  "hash": "",
  "id": 55, 
},{
  "title": "EDF Step by Step",
  "ancestors":  "(Extended Depth of Focus)" ,
  "filename": "edf.step.by.step.html",
  "hash": "",
  "id": 56, 
},{
  "title": "Export to Report Template",
  "ancestors":  "(Metalo - Grain Size Analysis &gt; Data output and Reports)" ,
  "filename": "export.to.report.template.gs.html",
  "hash": "",
  "id": 57, 
},{
  "title": "Advanced particle detection",
  "ancestors":  "(Filter Particle Analysis &gt; Advanced Functions)" ,
  "filename": "filters.advanced.detection.html",
  "hash": "",
  "id": 58, 
},{
  "title": "Autofocus during scanning",
  "ancestors":  "(Filter Particle Analysis &gt; Advanced Functions)" ,
  "filename": "filters.autofocus.html",
  "hash": "",
  "id": 59, 
},{
  "title": "Manual Editing of Particles",
  "ancestors":  "(Filter Particle Analysis &gt; Advanced Functions)" ,
  "filename": "filters.manualobjectedit.html",
  "hash": "",
  "id": 60, 
},{
  "title": "System settings",
  "ancestors":  "(Filter Particle Analysis &gt; Advanced Functions)" ,
  "filename": "filters.systemsettings.html",
  "hash": "",
  "id": 61, 
},{
  "title": "Automatic particle measurement",
  "ancestors":  "(Filter Particle Analysis &gt; Basic Workflows)" ,
  "filename": "flt.measure.html",
  "hash": "",
  "id": 62, 
},{
  "title": "Filter Scanning",
  "ancestors":  "(Filter Particle Analysis &gt; Basic Workflows)" ,
  "filename": "flt.scanning.html",
  "hash": "",
  "id": 63, 
},{
  "title": "Capturing FRET Image, FRET Calibration",
  "ancestors":  "(FRET)" ,
  "filename": "fret.capture.html",
  "hash": "",
  "id": 64, 
},{
  "title": "Creating FRET Image",
  "ancestors":  "(FRET)" ,
  "filename": "fret.create.image.html",
  "hash": "",
  "id": 65, 
},{
  "title": "Creating FRET View",
  "ancestors":  "(FRET)" ,
  "filename": "fret.create.view.html",
  "hash": "",
  "id": 66, 
},{
  "title": "Introduction",
  "ancestors":  "(General Analysis 3)" ,
  "filename": "ga3.introduction.html",
  "hash": "",
  "id": 67, 
},{
  "title": "GA3 Processing",
  "ancestors":  "(JOBS and HCA &gt; Tasks)" ,
  "filename": "ga3.tasks.all.html",
  "hash": "",
  "id": 68, 
},{
  "title": "Toolbars and Menus",
  "ancestors":  "(General Analysis 3 &gt; Basic Control)" ,
  "filename": "ga3.toolbars.html",
  "hash": "",
  "id": 69, 
},{
  "title": "Data Integrity Report",
  "ancestors":  "(GPA)" ,
  "filename": "gpa.cfr.report.html",
  "hash": "",
  "id": 70, 
},{
  "title": "Creating a recipe",
  "ancestors":  "(GPA &gt; Basic Workflows)" ,
  "filename": "gpa.definition.window.html",
  "hash": "",
  "id": 71, 
},{
  "title": "Executing a recipe",
  "ancestors":  "(GPA &gt; Basic Workflows)" ,
  "filename": "gpa.executing.recipe.html",
  "hash": "",
  "id": 72, 
},{
  "title": "GPA Explorer",
  "ancestors":  "(GPA)" ,
  "filename": "gpa.explorer.window.html",
  "hash": "",
  "id": 73, 
},{
  "title": "GPA",
  "ancestors":  "" ,
  "filename": "gpa.html",
  "hash": "",
  "id": 74, 
},{
  "title": "Left Tool Bar",
  "ancestors":  "(Metalo - Grain Size Analysis &gt; Reference)" ,
  "filename": "grainsize.left.toolbar.html",
  "hash": "",
  "id": 75, 
},{
  "title": "Automatic Capture",
  "ancestors":  "(Metalo - Grain Size Analysis &gt; Wizard Mode)" ,
  "filename": "gs.automatic.capture.html",
  "hash": "",
  "id": 76, 
},{
  "title": "Tools of the Comparison Window",
  "ancestors":  "(Metalo - Grain Size Analysis &gt; Comparative Method)" ,
  "filename": "gs.comparison.charts.reference.html",
  "hash": "",
  "id": 77, 
},{
  "title": "Measurement Mask",
  "ancestors":  "(Metalo - Grain Size Analysis &gt; User Interface)" ,
  "filename": "gs.measurement.mask.html",
  "hash": "",
  "id": 78, 
},{
  "title": "Sequential Measurement",
  "ancestors":  "(Metalo - Grain Size Analysis &gt; Reference)" ,
  "filename": "gs.quick.sequential.html",
  "hash": "",
  "id": 79, 
},{
  "title": "Grain Size Wizard - Start",
  "ancestors":  "(Metalo - Grain Size Analysis &gt; Reference)" ,
  "filename": "gs.w1.html",
  "hash": "",
  "id": 80, 
},{
  "title": "Grain Size Wizard - Select Detection Preset",
  "ancestors":  "(Metalo - Grain Size Analysis &gt; Reference)" ,
  "filename": "gs.w2.html",
  "hash": "",
  "id": 81, 
},{
  "title": "Grain Size Wizard - Select Measurement Preset",
  "ancestors":  "(Metalo - Grain Size Analysis &gt; Reference)" ,
  "filename": "gs.w3.html",
  "hash": "",
  "id": 82, 
},{
  "title": "Job Database",
  "ancestors":  "(JOBS and HCA &gt; Advanced Topics)" ,
  "filename": "hca.db.html",
  "hash": "",
  "id": 83, 
},{
  "title": "Running a Job, Viewing Results and Graphs",
  "ancestors":  "(JOBS and HCA)" ,
  "filename": "hca.viewresults.html",
  "hash": "",
  "id": 84, 
},{
  "title": "HCA Analysis",
  "ancestors":  "(HCA)" ,
  "filename": "hcajobs.analysis.html",
  "hash": "",
  "id": 85, 
},{
  "title": "Custom Equation",
  "ancestors":  "(FRET)" ,
  "filename": "howto.custom.equation.html",
  "hash": "",
  "id": 86, 
},{
  "title": "Choosing the Deconvolution Method",
  "ancestors":  "(Deconvolution)" ,
  "filename": "howto.deconv.choose.method.html",
  "hash": "",
  "id": 87, 
},{
  "title": "Introduction to Deconvolution",
  "ancestors":  "(Deconvolution)" ,
  "filename": "howto.deconv.intro.html",
  "hash": "",
  "id": 88, 
},{
  "title": "Determining the Point Spread Function (PSF)",
  "ancestors":  "(Deconvolution)" ,
  "filename": "howto.deconv.psf.html",
  "hash": "",
  "id": 89, 
},{
  "title": "Tracking",
  "ancestors":  "" ,
  "filename": "howto.tracking.html",
  "hash": "",
  "id": 90, 
},{
  "title": "Triggered Acquisition",
  "ancestors":  "" ,
  "filename": "howto.triggeredacquisition.html",
  "hash": "",
  "id": 91, 
},{
  "title": "Import Molecule List",
  "ancestors":  "(Tracking)" ,
  "filename": "import.molecule.list.html",
  "hash": "",
  "id": 92, 
},{
  "title": "Intercept Methods",
  "ancestors":  "(Metalo - Grain Size Analysis)" ,
  "filename": "intercept.methods.html",
  "hash": "",
  "id": 93, 
},{
  "title": "Advanced I-SPT",
  "ancestors":  "(Tracking &gt; Intra-Nuclear Single Particle Tracking (I-SPT))" ,
  "filename": "ispt.advanced.html",
  "hash": "",
  "id": 94, 
},{
  "title": "Intra-Nuclear Single Particle Tracking (I-SPT)",
  "ancestors":  "(Tracking)" ,
  "filename": "ispt.html",
  "hash": "",
  "id": 95, 
},{
  "title": "Advanced Topics",
  "ancestors":  "(JOBS and HCA)" ,
  "filename": "jobs.advanced.features.html",
  "hash": "",
  "id": 96, 
},{
  "title": "Advanced Files Import",
  "ancestors":  "(JOBS and HCA &gt; Advanced Topics)" ,
  "filename": "jobs.advanced.import.html",
  "hash": "",
  "id": 97, 
},{
  "title": "Basic Job Rules",
  "ancestors":  "(JOBS and HCA)" ,
  "filename": "jobs.basic.rules.html",
  "hash": "",
  "id": 98, 
},{
  "title": "Creating a Job",
  "ancestors":  "(JOBS and HCA)" ,
  "filename": "jobs.creating.html",
  "hash": "",
  "id": 99, 
},{
  "title": "Using Questions",
  "ancestors":  "(JOBS and HCA &gt; Use Cases)" ,
  "filename": "jobs.introduction.to.questions.html",
  "hash": "",
  "id": 100, 
},{
  "title": "Introduction to the JOBS Module",
  "ancestors":  "(JOBS and HCA)" ,
  "filename": "jobs.overview.html",
  "hash": "",
  "id": 101, 
},{
  "title": "Analysis",
  "ancestors":  "(JOBS and HCA)" ,
  "filename": "jobs.run.analysis.html",
  "hash": "",
  "id": 102, 
},{
  "title": "SMTP Configuration for Sending Email/SMS Messages",
  "ancestors":  "(JOBS and HCA &gt; Advanced Topics)" ,
  "filename": "jobs.smtp.setup.html",
  "hash": "",
  "id": 103, 
},{
  "title": "Tasks",
  "ancestors":  "(JOBS and HCA)" ,
  "filename": "jobs.tasks.html",
  "hash": "",
  "id": 104, 
},{
  "title": "Creating PFS Surface",
  "ancestors":  "(JOBS and HCA &gt; Use Cases)" ,
  "filename": "jobs_PFS_surface.html",
  "hash": "",
  "id": 105, 
},{
  "title": "Using Conditions",
  "ancestors":  "(JOBS and HCA &gt; Use Cases)" ,
  "filename": "jobs_introduction_to_conditions.html",
  "hash": "",
  "id": 106, 
},{
  "title": "Using Expressions",
  "ancestors":  "(JOBS and HCA &gt; Use Cases)" ,
  "filename": "jobs_introduction_to_expressions.html",
  "hash": "",
  "id": 107, 
},{
  "title": "Labels and Metadata",
  "ancestors":  "(JOBS and HCA)" ,
  "filename": "labels.and.metadata.html",
  "hash": "",
  "id": 108, 
},{
  "title": "Manual Corrections Before Measurement",
  "ancestors":  "(Metalo - Grain Size Analysis &gt; Intercept Methods)" ,
  "filename": "manual.corrections.intercept.html",
  "hash": "",
  "id": 109, 
},{
  "title": "Manual Corrections Before Measurement",
  "ancestors":  "(Metalo - Grain Size Analysis &gt; Planimetric Method)" ,
  "filename": "manual.corrections.planimetric.html",
  "hash": "",
  "id": 110, 
},{
  "title": "3D Measurement",
  "ancestors":  "" ,
  "filename": "measurement.3d.html",
  "hash": "",
  "id": 111, 
},{
  "title": "Measurement Explorer",
  "ancestors":  "(Measurement Sequencer)" ,
  "filename": "measurement.explorer.html",
  "hash": "",
  "id": 112, 
},{
  "title": "Measurement Panel",
  "ancestors":  "(Metalo - Grain Size Analysis &gt; Reference)" ,
  "filename": "measurement.panel.ref.html",
  "hash": "",
  "id": 113, 
},{
  "title": "(Not) Merging Z-Stacks Different for Each Point",
  "ancestors":  "(JOBS and HCA &gt; Merging Different Acquisition Types)" ,
  "filename": "merging.different.zstacks.html",
  "hash": "",
  "id": 114, 
},{
  "title": "Comparative Method",
  "ancestors":  "(Metalo - Grain Size Analysis)" ,
  "filename": "metalo.comparative.method.html",
  "hash": "",
  "id": 115, 
},{
  "title": "Detection Panel",
  "ancestors":  "(Metalo - Cast Iron Analysis &gt; Reference)" ,
  "filename": "metalo.detection.options.html",
  "hash": "",
  "id": 116, 
},{
  "title": "Applications",
  "ancestors":  "" ,
  "filename": "mods.html",
  "hash": "",
  "id": 117, 
},{
  "title": "Molecule Analysis",
  "ancestors":  "(N-STORM &gt; Control Windows)" ,
  "filename": "molecule.analysis.html",
  "hash": "",
  "id": 118, 
},{
  "title": "Molecule Options",
  "ancestors":  "(N-STORM &gt; Control Windows)" ,
  "filename": "molecule.options.html",
  "hash": "",
  "id": 119, 
},{
  "title": "Reference",
  "ancestors":  "(N-STORM &gt; Control Windows)" ,
  "filename": "molecule.reference.html",
  "hash": "",
  "id": 120, 
},{
  "title": "Running a HCA job, viewing results",
  "ancestors":  "(HCA)" ,
  "filename": "new.hca.html",
  "hash": "",
  "id": 121, 
},{
  "title": "NIS.ai Explorer",
  "ancestors":  "(NIS.ai)" ,
  "filename": "nis.ai.catalogue.html",
  "hash": "",
  "id": 122, 
},{
  "title": "Segment.ai Example",
  "ancestors":  "(NIS.ai &gt; Examples (version 5.42))" ,
  "filename": "nis.ai.example.segment.html",
  "hash": "",
  "id": 123, 
},{
  "title": "NIS.ai",
  "ancestors":  "" ,
  "filename": "nis.ai.html",
  "hash": "",
  "id": 124, 
},{
  "title": "Functions",
  "ancestors":  "(NIS.ai)" ,
  "filename": "nis.ai.modalities.html",
  "hash": "",
  "id": 125, 
},{
  "title": "Convert.ai",
  "ancestors":  "(NIS.ai &gt; Functions)" ,
  "filename": "nis.ai.modalities.modality.transfer.html",
  "hash": "",
  "id": 126, 
},{
  "title": "Segment.ai",
  "ancestors":  "(NIS.ai &gt; Functions)" ,
  "filename": "nis.ai.modalities.segment.html",
  "hash": "",
  "id": 127, 
},{
  "title": "Segment Objects.ai",
  "ancestors":  "(NIS.ai &gt; Functions)" ,
  "filename": "nis.ai.modalities.segment.objects.html",
  "hash": "",
  "id": 128, 
},{
  "title": "Enhance.ai",
  "ancestors":  "(NIS.ai &gt; Functions)" ,
  "filename": "nis.ai.modalities.signal.enhancement.html",
  "hash": "",
  "id": 129, 
},{
  "title": "Binary operations",
  "ancestors":  "(General Analysis 3 &gt; Nodes)" ,
  "filename": "node.binaryoperations_section.html",
  "hash": "",
  "id": 130, 
},{
  "title": "Binary processing",
  "ancestors":  "(General Analysis 3 &gt; Nodes)" ,
  "filename": "node.binaryprocessing_section.html",
  "hash": "",
  "id": 131, 
},{
  "title": "Data management",
  "ancestors":  "(General Analysis 3 &gt; Nodes)" ,
  "filename": "node.datamanagement_section.html",
  "hash": "",
  "id": 132, 
},{
  "title": "Image Operations",
  "ancestors":  "(General Analysis 3 &gt; Nodes)" ,
  "filename": "node.imageoperations_section.html",
  "hash": "",
  "id": 133, 
},{
  "title": "Image Processing",
  "ancestors":  "(General Analysis 3 &gt; Nodes)" ,
  "filename": "node.imageprocessing_section.html",
  "hash": "",
  "id": 134, 
},{
  "title": "Measurement",
  "ancestors":  "(General Analysis 3 &gt; Nodes)" ,
  "filename": "node.measurement_section.html",
  "hash": "",
  "id": 135, 
},{
  "title": "ND Processing &amp; Conversions",
  "ancestors":  "(General Analysis 3 &gt; Nodes)" ,
  "filename": "node.ndprocessingconversions_section.html",
  "hash": "",
  "id": 136, 
},{
  "title": "NIS.ai",
  "ancestors":  "(General Analysis 3 &gt; Nodes)" ,
  "filename": "node.nis.ai_section.html",
  "hash": "",
  "id": 137, 
},{
  "title": "Results &amp; Graphs",
  "ancestors":  "(General Analysis 3 &gt; Nodes)" ,
  "filename": "node.results.graphs_section.html",
  "hash": "",
  "id": 138, 
},{
  "title": "Segmentation",
  "ancestors":  "(General Analysis 3 &gt; Nodes)" ,
  "filename": "node.segmentation_section.html",
  "hash": "",
  "id": 139, 
},{
  "title": "Sources &amp; Reference",
  "ancestors":  "(General Analysis 3 &gt; Nodes)" ,
  "filename": "node.sourcesreference_section.html",
  "hash": "",
  "id": 140, 
},{
  "title": "Tracking",
  "ancestors":  "(General Analysis 3 &gt; Nodes)" ,
  "filename": "node.tracking.group_section.html",
  "hash": "",
  "id": 141, 
},{
  "title": "N-STORM Acquisition and Analysis",
  "ancestors":  "(N-STORM)" ,
  "filename": "nstorm.external.html",
  "hash": "",
  "id": 142, 
},{
  "title": "N-STORM Z-Calibration (Agilent and LU4)",
  "ancestors":  "(N-STORM)" ,
  "filename": "nstorm.zcalibration.html",
  "hash": "",
  "id": 143, 
},{
  "title": "Ca 2+  Ion Concentration Measurement",
  "ancestors":  "(FRET)" ,
  "filename": "option.CalciumCalibration.html",
  "hash": "",
  "id": 144, 
},{
  "title": "Introduction to FRET",
  "ancestors":  "(FRET)" ,
  "filename": "p2c10s1.html",
  "hash": "",
  "id": 145, 
},{
  "title": "FRET Example",
  "ancestors":  "(FRET)" ,
  "filename": "p2c10s6.html",
  "hash": "",
  "id": 146, 
},{
  "title": "Changes from 5.40",
  "ancestors":  "(General Analysis 3)" ,
  "filename": "p2c11s2.html",
  "hash": "",
  "id": 147, 
},{
  "title": "Basic Control",
  "ancestors":  "(General Analysis 3)" ,
  "filename": "p2c11s3.html",
  "hash": "",
  "id": 148, 
},{
  "title": "Examples",
  "ancestors":  "(General Analysis 3)" ,
  "filename": "p2c11s4.html",
  "hash": "",
  "id": 149, 
},{
  "title": "Hidden Nodes (created in NIS-Elements 5.4x)",
  "ancestors":  "(General Analysis 3 &gt; Nodes)" ,
  "filename": "p2c11s5s13.html",
  "hash": "",
  "id": 150, 
},{
  "title": "Introduction to HCA",
  "ancestors":  "(HCA)" ,
  "filename": "p2c12s1.html",
  "hash": "",
  "id": 151, 
},{
  "title": "HDR Options",
  "ancestors":  "(HDR)" ,
  "filename": "p2c13s1.html",
  "hash": "",
  "id": 152, 
},{
  "title": "HDR module background",
  "ancestors":  "(HDR)" ,
  "filename": "p2c13s2.html",
  "hash": "",
  "id": 153, 
},{
  "title": "User Interface",
  "ancestors":  "(Illumination Sequence)" ,
  "filename": "p2c14s1.html",
  "hash": "",
  "id": 154, 
},{
  "title": "Creating Piezo Z Stack and Z Offset patterns",
  "ancestors":  "(Illumination Sequence)" ,
  "filename": "p2c14s3.html",
  "hash": "",
  "id": 155, 
},{
  "title": "Piezo Z Stack",
  "ancestors":  "(Illumination Sequence &gt; Creating Piezo Z Stack and Z Offset patterns)" ,
  "filename": "p2c14s3s1.html",
  "hash": "",
  "id": 156, 
},{
  "title": "Piezo Z Offset",
  "ancestors":  "(Illumination Sequence &gt; Creating Piezo Z Stack and Z Offset patterns)" ,
  "filename": "p2c14s3s2.html",
  "hash": "",
  "id": 157, 
},{
  "title": "Creating stimulation patterns for Other Devices",
  "ancestors":  "(Illumination Sequence)" ,
  "filename": "p2c14s4.html",
  "hash": "",
  "id": 158, 
},{
  "title": "Camera Timing Examples",
  "ancestors":  "(Illumination Sequence)" ,
  "filename": "p2c14s5.html",
  "hash": "",
  "id": 159, 
},{
  "title": "Supported cameras and illumination devices",
  "ancestors":  "(Illumination Sequence)" ,
  "filename": "p2c14s7.html",
  "hash": "",
  "id": 160, 
},{
  "title": "Use Cases",
  "ancestors":  "(JOBS and HCA)" ,
  "filename": "p2c15s10.html",
  "hash": "",
  "id": 161, 
},{
  "title": "Analysis Use Cases",
  "ancestors":  "(JOBS and HCA &gt; Use Cases)" ,
  "filename": "p2c15s10s1.html",
  "hash": "",
  "id": 162, 
},{
  "title": "Merging Different Acquisition Types",
  "ancestors":  "(JOBS and HCA)" ,
  "filename": "p2c15s4.html",
  "hash": "",
  "id": 163, 
},{
  "title": "Simple Acquisition Tasks Inside a Parent Loop",
  "ancestors":  "(JOBS and HCA &gt; Merging Different Acquisition Types)" ,
  "filename": "p2c15s4s1.html",
  "hash": "",
  "id": 164, 
},{
  "title": "Merging Matching Z-Stacks",
  "ancestors":  "(JOBS and HCA &gt; Merging Different Acquisition Types)" ,
  "filename": "p2c15s4s2.html",
  "hash": "",
  "id": 165, 
},{
  "title": "Multi-point inside Multi-Point Loop",
  "ancestors":  "(JOBS and HCA &gt; Merging Different Acquisition Types)" ,
  "filename": "p2c15s4s4.html",
  "hash": "",
  "id": 166, 
},{
  "title": "GA3 After Acquisition",
  "ancestors":  "(JOBS and HCA)" ,
  "filename": "p2c15s6.html",
  "hash": "",
  "id": 167, 
},{
  "title": "Overview",
  "ancestors":  "(JOBS and HCA &gt; Advanced Topics &gt; Advanced Files Import)" ,
  "filename": "p2c15s9s3s1.html",
  "hash": "",
  "id": 168, 
},{
  "title": "Source Files",
  "ancestors":  "(JOBS and HCA &gt; Advanced Topics &gt; Advanced Files Import)" ,
  "filename": "p2c15s9s3s2.html",
  "hash": "",
  "id": 169, 
},{
  "title": "Loops and Indexes",
  "ancestors":  "(JOBS and HCA &gt; Advanced Topics &gt; Advanced Files Import)" ,
  "filename": "p2c15s9s3s3.html",
  "hash": "",
  "id": 170, 
},{
  "title": "Defining Loops",
  "ancestors":  "(JOBS and HCA &gt; Advanced Topics &gt; Advanced Files Import)" ,
  "filename": "p2c15s9s3s4.html",
  "hash": "",
  "id": 171, 
},{
  "title": "Assigning Indexes to Frames",
  "ancestors":  "(JOBS and HCA &gt; Advanced Topics &gt; Advanced Files Import)" ,
  "filename": "p2c15s9s3s5.html",
  "hash": "",
  "id": 172, 
},{
  "title": "Adding Metadata to Loops",
  "ancestors":  "(JOBS and HCA &gt; Advanced Topics &gt; Advanced Files Import)" ,
  "filename": "p2c15s9s3s6.html",
  "hash": "",
  "id": 173, 
},{
  "title": "Examples",
  "ancestors":  "(JOBS and HCA &gt; Advanced Topics &gt; Advanced Files Import)" ,
  "filename": "p2c15s9s3s7.html",
  "hash": "",
  "id": 174, 
},{
  "title": "Basic Layer Measurement",
  "ancestors":  "(Layer Thickness Measurement)" ,
  "filename": "p2c16s1.html",
  "hash": "",
  "id": 175, 
},{
  "title": "Calo Test",
  "ancestors":  "(Layer Thickness Measurement)" ,
  "filename": "p2c16s2.html",
  "hash": "",
  "id": 176, 
},{
  "title": "Introduction",
  "ancestors":  "(Measurement Sequencer)" ,
  "filename": "p2c17s1.html",
  "hash": "",
  "id": 177, 
},{
  "title": "Basic Workflow",
  "ancestors":  "(Measurement Sequencer)" ,
  "filename": "p2c17s2.html",
  "hash": "",
  "id": 178, 
},{
  "title": "Features of the Cast Iron Analysis Module",
  "ancestors":  "(Metalo - Cast Iron Analysis)" ,
  "filename": "p2c18s1.html",
  "hash": "",
  "id": 179, 
},{
  "title": "Single Image Measurement",
  "ancestors":  "(Metalo - Cast Iron Analysis)" ,
  "filename": "p2c18s2.html",
  "hash": "",
  "id": 180, 
},{
  "title": "Sequential Measurement",
  "ancestors":  "(Metalo - Cast Iron Analysis)" ,
  "filename": "p2c18s3.html",
  "hash": "",
  "id": 181, 
},{
  "title": "Reference",
  "ancestors":  "(Metalo - Cast Iron Analysis)" ,
  "filename": "p2c18s4.html",
  "hash": "",
  "id": 182, 
},{
  "title": "Measurement Panel",
  "ancestors":  "(Metalo - Cast Iron Analysis &gt; Reference)" ,
  "filename": "p2c18s4s2.html",
  "hash": "",
  "id": 183, 
},{
  "title": "Left Tool Bar",
  "ancestors":  "(Metalo - Cast Iron Analysis &gt; Reference)" ,
  "filename": "p2c18s4s3.html",
  "hash": "",
  "id": 184, 
},{
  "title": "Data Panel",
  "ancestors":  "(Metalo - Cast Iron Analysis &gt; Reference)" ,
  "filename": "p2c18s4s4.html",
  "hash": "",
  "id": 185, 
},{
  "title": "Introduction",
  "ancestors":  "(Metalo - Grain Size Analysis)" ,
  "filename": "p2c19s1.html",
  "hash": "",
  "id": 186, 
},{
  "title": "User Interface",
  "ancestors":  "(Metalo - Grain Size Analysis)" ,
  "filename": "p2c19s2.html",
  "hash": "",
  "id": 187, 
},{
  "title": "Application Workspace",
  "ancestors":  "(Metalo - Grain Size Analysis &gt; User Interface)" ,
  "filename": "p2c19s2s1.html",
  "hash": "",
  "id": 188, 
},{
  "title": "Measurement Panel",
  "ancestors":  "(Metalo - Grain Size Analysis &gt; User Interface)" ,
  "filename": "p2c19s2s3.html",
  "hash": "",
  "id": 189, 
},{
  "title": "Display Modes",
  "ancestors":  "(Metalo - Grain Size Analysis &gt; User Interface)" ,
  "filename": "p2c19s2s5.html",
  "hash": "",
  "id": 190, 
},{
  "title": "Basic Workflow",
  "ancestors":  "(Metalo - Grain Size Analysis &gt; Wizard Mode)" ,
  "filename": "p2c19s4s1.html",
  "hash": "",
  "id": 191, 
},{
  "title": "Manual Capture",
  "ancestors":  "(Metalo - Grain Size Analysis &gt; Wizard Mode)" ,
  "filename": "p2c19s4s3.html",
  "hash": "",
  "id": 192, 
},{
  "title": "Planimetric Method Settings",
  "ancestors":  "(Metalo - Grain Size Analysis &gt; Planimetric Method)" ,
  "filename": "p2c19s5s1.html",
  "hash": "",
  "id": 193, 
},{
  "title": "Overlay Transparency",
  "ancestors":  "(Metalo - Grain Size Analysis &gt; Planimetric Method)" ,
  "filename": "p2c19s5s2.html",
  "hash": "",
  "id": 194, 
},{
  "title": "Introduction",
  "ancestors":  "(Metalo - Grain Size Analysis &gt; Comparative Method)" ,
  "filename": "p2c19s7s1.html",
  "hash": "",
  "id": 195, 
},{
  "title": "Physical Zoom",
  "ancestors":  "(Metalo - Grain Size Analysis &gt; Comparative Method)" ,
  "filename": "p2c19s7s2.html",
  "hash": "",
  "id": 196, 
},{
  "title": "How to Create Your Own Grain Size Comparison Charts",
  "ancestors":  "(Metalo - Grain Size Analysis &gt; Comparative Method)" ,
  "filename": "p2c19s7s5.html",
  "hash": "",
  "id": 197, 
},{
  "title": "Export Options",
  "ancestors":  "(Metalo - Grain Size Analysis &gt; Data output and Reports)" ,
  "filename": "p2c19s8s2.html",
  "hash": "",
  "id": 198, 
},{
  "title": "Reference",
  "ancestors":  "(Metalo - Grain Size Analysis)" ,
  "filename": "p2c19s9.html",
  "hash": "",
  "id": 199, 
},{
  "title": "Interface",
  "ancestors":  "(GPA)" ,
  "filename": "p2c1s1.html",
  "hash": "",
  "id": 200, 
},{
  "title": "Basic Workflows",
  "ancestors":  "(GPA)" ,
  "filename": "p2c1s2.html",
  "hash": "",
  "id": 201, 
},{
  "title": "Control Windows",
  "ancestors":  "(N-STORM)" ,
  "filename": "p2c20s2.html",
  "hash": "",
  "id": 202, 
},{
  "title": "Introduction to Triggered Acquisition",
  "ancestors":  "(Triggered Acquisition)" ,
  "filename": "p2c21s1.html",
  "hash": "",
  "id": 203, 
},{
  "title": "Wiring",
  "ancestors":  "(Triggered Acquisition)" ,
  "filename": "p2c21s3.html",
  "hash": "",
  "id": 204, 
},{
  "title": "Settings in NIS-Elements",
  "ancestors":  "(Triggered Acquisition)" ,
  "filename": "p2c21s4.html",
  "hash": "",
  "id": 205, 
},{
  "title": "Defining Classes",
  "ancestors":  "(Object Classifier)" ,
  "filename": "p2c22s1.html",
  "hash": "",
  "id": 206, 
},{
  "title": "Introduction",
  "ancestors":  "(Smart Experiment)" ,
  "filename": "p2c23s1.html",
  "hash": "",
  "id": 207, 
},{
  "title": "Selecting and Running an Assay",
  "ancestors":  "(Smart Experiment)" ,
  "filename": "p2c23s2.html",
  "hash": "",
  "id": 208, 
},{
  "title": "Introduction to Tracking",
  "ancestors":  "(Tracking)" ,
  "filename": "p2c24s1.html",
  "hash": "",
  "id": 209, 
},{
  "title": "Overview",
  "ancestors":  "(Tracking &gt; Intra-Nuclear Single Particle Tracking (I-SPT))" ,
  "filename": "p2c24s16s1.html",
  "hash": "",
  "id": 210, 
},{
  "title": "Statistics",
  "ancestors":  "(Tracking &gt; Intra-Nuclear Single Particle Tracking (I-SPT))" ,
  "filename": "p2c24s16s10.html",
  "hash": "",
  "id": 211, 
},{
  "title": "NIS-Elements configuration",
  "ancestors":  "(Tracking &gt; Intra-Nuclear Single Particle Tracking (I-SPT))" ,
  "filename": "p2c24s16s2.html",
  "hash": "",
  "id": 212, 
},{
  "title": "Image analysis",
  "ancestors":  "(Tracking &gt; Intra-Nuclear Single Particle Tracking (I-SPT))" ,
  "filename": "p2c24s16s3.html",
  "hash": "",
  "id": 213, 
},{
  "title": "Recipes",
  "ancestors":  "(Tracking &gt; Intra-Nuclear Single Particle Tracking (I-SPT))" ,
  "filename": "p2c24s16s4.html",
  "hash": "",
  "id": 214, 
},{
  "title": "Detection",
  "ancestors":  "(Tracking &gt; Intra-Nuclear Single Particle Tracking (I-SPT))" ,
  "filename": "p2c24s16s5.html",
  "hash": "",
  "id": 215, 
},{
  "title": "Tracking",
  "ancestors":  "(Tracking &gt; Intra-Nuclear Single Particle Tracking (I-SPT))" ,
  "filename": "p2c24s16s6.html",
  "hash": "",
  "id": 216, 
},{
  "title": "I-SPT Outputs",
  "ancestors":  "(Tracking &gt; Intra-Nuclear Single Particle Tracking (I-SPT))" ,
  "filename": "p2c24s16s7.html",
  "hash": "",
  "id": 217, 
},{
  "title": "Advanced tracking",
  "ancestors":  "(Tracking &gt; Intra-Nuclear Single Particle Tracking (I-SPT))" ,
  "filename": "p2c24s16s9.html",
  "hash": "",
  "id": 218, 
},{
  "title": "Measurement of Tracking Features",
  "ancestors":  "(Tracking)" ,
  "filename": "p2c24s6.html",
  "hash": "",
  "id": 219, 
},{
  "title": "Graph Properties",
  "ancestors":  "(Tracking)" ,
  "filename": "p2c24s7.html",
  "hash": "",
  "id": 220, 
},{
  "title": "Algorithm Overview",
  "ancestors":  "(Tracking)" ,
  "filename": "p2c24s9.html",
  "hash": "",
  "id": 221, 
},{
  "title": "Integration in General Analysis 3 (GA3)",
  "ancestors":  "(NIS.ai)" ,
  "filename": "p2c2s4.html",
  "hash": "",
  "id": 222, 
},{
  "title": "Examples (version 5.42)",
  "ancestors":  "(NIS.ai)" ,
  "filename": "p2c2s5.html",
  "hash": "",
  "id": 223, 
},{
  "title": "Introduction",
  "ancestors":  "(Concrete)" ,
  "filename": "p2c5s1.html",
  "hash": "",
  "id": 224, 
},{
  "title": "Step by step concrete measurement",
  "ancestors":  "(Concrete)" ,
  "filename": "p2c5s2.html",
  "hash": "",
  "id": 225, 
},{
  "title": "New Database",
  "ancestors":  "(Database)" ,
  "filename": "p2c6s1.html",
  "hash": "",
  "id": 226, 
},{
  "title": "New Connection",
  "ancestors":  "(Database)" ,
  "filename": "p2c6s2.html",
  "hash": "",
  "id": 227, 
},{
  "title": "Database Tables",
  "ancestors":  "(Database)" ,
  "filename": "p2c6s3.html",
  "hash": "",
  "id": 228, 
},{
  "title": "User Permissions",
  "ancestors":  "(Database)" ,
  "filename": "p2c6s4.html",
  "hash": "",
  "id": 229, 
},{
  "title": "Database Backup",
  "ancestors":  "(Database)" ,
  "filename": "p2c6s5.html",
  "hash": "",
  "id": 230, 
},{
  "title": "Creating Reports from Database",
  "ancestors":  "(Database)" ,
  "filename": "p2c6s7.html",
  "hash": "",
  "id": 231, 
},{
  "title": "Proper Sampling",
  "ancestors":  "(Deconvolution)" ,
  "filename": "p2c7s5.html",
  "hash": "",
  "id": 232, 
},{
  "title": "Dialog Settings",
  "ancestors":  "(Deconvolution)" ,
  "filename": "p2c7s6.html",
  "hash": "",
  "id": 233, 
},{
  "title": "Running Deconvolution",
  "ancestors":  "(Deconvolution)" ,
  "filename": "p2c7s7.html",
  "hash": "",
  "id": 234, 
},{
  "title": "Introduction to Filters",
  "ancestors":  "(Filter Particle Analysis)" ,
  "filename": "p2c9s1.html",
  "hash": "",
  "id": 235, 
},{
  "title": "Basic Workflows",
  "ancestors":  "(Filter Particle Analysis)" ,
  "filename": "p2c9s2.html",
  "hash": "",
  "id": 236, 
},{
  "title": "Operational Precautions",
  "ancestors":  "(Filter Particle Analysis &gt; Basic Workflows)" ,
  "filename": "p2c9s2s1.html",
  "hash": "",
  "id": 237, 
},{
  "title": "Advanced Functions",
  "ancestors":  "(Filter Particle Analysis)" ,
  "filename": "p2c9s3.html",
  "hash": "",
  "id": 238, 
},{
  "title": "Creating a Report Template",
  "ancestors":  "(Filter Particle Analysis &gt; Advanced Functions)" ,
  "filename": "p2c9s3s8.html",
  "hash": "",
  "id": 239, 
},{
  "title": "Planimetric Method",
  "ancestors":  "(Metalo - Grain Size Analysis)" ,
  "filename": "planimetric.method.html",
  "hash": "",
  "id": 240, 
},{
  "title": "Real Time Tracking Using XY Stage",
  "ancestors":  "(Tracking)" ,
  "filename": "real.time.tracking.html",
  "hash": "",
  "id": 241, 
},{
  "title": "Assay Settings",
  "ancestors":  "(Smart Experiment)" ,
  "filename": "se.assay.settings.html",
  "hash": "",
  "id": 242, 
},{
  "title": "Browsing Results",
  "ancestors":  "(Smart Experiment)" ,
  "filename": "se.results.html",
  "hash": "",
  "id": 243, 
},{
  "title": "User Management",
  "ancestors":  "(Smart Experiment)" ,
  "filename": "se.user.management.html",
  "hash": "",
  "id": 244, 
},{
  "title": "Measurement Sequencer - Definition",
  "ancestors":  "(Measurement Sequencer)" ,
  "filename": "sequencer.definition.html",
  "hash": "",
  "id": 245, 
},{
  "title": "Excel Template",
  "ancestors":  "(Measurement Sequencer &gt; Report Editing)" ,
  "filename": "sequencer.report.excel.html",
  "hash": "",
  "id": 246, 
},{
  "title": "Report Editing",
  "ancestors":  "(Measurement Sequencer)" ,
  "filename": "sequencer.report.html",
  "hash": "",
  "id": 247, 
},{
  "title": "Measurement Sequencer - Run",
  "ancestors":  "(Measurement Sequencer)" ,
  "filename": "sequencer.run.html",
  "hash": "",
  "id": 248, 
},{
  "title": "Acquisition",
  "ancestors":  "(JOBS and HCA &gt; Tasks)" ,
  "filename": "task.acquisition_section.html",
  "hash": "",
  "id": 249, 
},{
  "title": "Autofocus + Focus Surface",
  "ancestors":  "(JOBS and HCA &gt; Tasks)" ,
  "filename": "task.autofocus.group_section.html",
  "hash": "",
  "id": 250, 
},{
  "title": "Conditions",
  "ancestors":  "(JOBS and HCA &gt; Tasks)" ,
  "filename": "task.conditions_section.html",
  "hash": "",
  "id": 251, 
},{
  "title": "Device Control",
  "ancestors":  "(JOBS and HCA &gt; Tasks)" ,
  "filename": "task.devicecontrol_section.html",
  "hash": "",
  "id": 252, 
},{
  "title": "Optical Configurations",
  "ancestors":  "(JOBS and HCA &gt; Tasks)" ,
  "filename": "task.opticalconfigurations_section.html",
  "hash": "",
  "id": 253, 
},{
  "title": "PFS",
  "ancestors":  "(JOBS and HCA &gt; Tasks)" ,
  "filename": "task.pfs.group_section.html",
  "hash": "",
  "id": 254, 
},{
  "title": "ROIs",
  "ancestors":  "(JOBS and HCA &gt; Tasks)" ,
  "filename": "task.rois_section.html",
  "hash": "",
  "id": 255, 
},{
  "title": "Sample Holder",
  "ancestors":  "(JOBS and HCA &gt; Tasks)" ,
  "filename": "task.sampleholder_section.html",
  "hash": "",
  "id": 256, 
},{
  "title": "Stage XY Points",
  "ancestors":  "(JOBS and HCA &gt; Tasks)" ,
  "filename": "task.stagexy_section.html",
  "hash": "",
  "id": 257, 
},{
  "title": "Stimulation",
  "ancestors":  "(JOBS and HCA &gt; Tasks)" ,
  "filename": "task.stimulation_section.html",
  "hash": "",
  "id": 258, 
},{
  "title": "System",
  "ancestors":  "(JOBS and HCA &gt; Tasks)" ,
  "filename": "task.system_section.html",
  "hash": "",
  "id": 259, 
},{
  "title": "Time Series",
  "ancestors":  "(JOBS and HCA &gt; Tasks)" ,
  "filename": "task.timeseries_section.html",
  "hash": "",
  "id": 260, 
},{
  "title": "Well Plates",
  "ancestors":  "(JOBS and HCA &gt; Tasks)" ,
  "filename": "task.wellplates_section.html",
  "hash": "",
  "id": 261, 
},{
  "title": "Z-Stack",
  "ancestors":  "(JOBS and HCA &gt; Tasks)" ,
  "filename": "task.zstack.group_section.html",
  "hash": "",
  "id": 262, 
},{
  "title": "Analysis",
  "ancestors":  "(JOBS and HCA &gt; Tasks)" ,
  "filename": "tasks.analysis.html",
  "hash": "",
  "id": 263, 
},{
  "title": "Large Images",
  "ancestors":  "(JOBS and HCA &gt; Tasks)" ,
  "filename": "tasks.largeimages.html",
  "hash": "",
  "id": 264, 
},{
  "title": "Using Time Lapse",
  "ancestors":  "(JOBS and HCA &gt; Use Cases)" ,
  "filename": "tl_temp_heat_use_case.html",
  "hash": "",
  "id": 265, 
},{
  "title": "Binary Tracking Step by Step",
  "ancestors":  "(Tracking)" ,
  "filename": "track.binary.html",
  "hash": "",
  "id": 266, 
},{
  "title": "Tracking Prerequisites",
  "ancestors":  "(Tracking)" ,
  "filename": "track.prereq.html",
  "hash": "",
  "id": 267, 
},{
  "title": "ROI Tracking Step by Step",
  "ancestors":  "(Tracking)" ,
  "filename": "track.roi.html",
  "hash": "",
  "id": 268, 
},{
  "title": "Visualization",
  "ancestors":  "(Tracking)" ,
  "filename": "track.visualization.html",
  "hash": "",
  "id": 269, 
},{
  "title": "Tracking 3D",
  "ancestors":  "(Tracking)" ,
  "filename": "tracking.3d.html",
  "hash": "",
  "id": 270, 
},{
  "title": "Tracking - Advanced",
  "ancestors":  "(Tracking)" ,
  "filename": "tracking.advanced.html",
  "hash": "",
  "id": 271, 
},{
  "title": "Tracking Options",
  "ancestors":  "(Tracking)" ,
  "filename": "tracking.options.html",
  "hash": "",
  "id": 272, 
},{
  "title": "Tracking Output Calculation",
  "ancestors":  "(Tracking)" ,
  "filename": "tracking.output.calculation.html",
  "hash": "",
  "id": 273, 
},{
  "title": "What You Need to Run Triggered Acquisition",
  "ancestors":  "(Triggered Acquisition)" ,
  "filename": "triggering.needs.html",
  "hash": "",
  "id": 274, 
},{
  "title": "HCA",
  "ancestors":  "" ,
  "filename": "ug.hcajobs.html",
  "hash": "",
  "id": 275, 
},{
  "title": "JOBS and HCA",
  "ancestors":  "" ,
  "filename": "ug.jobs.html",
  "hash": "",
  "id": 276, 
},{
  "title": "Wizard Mode",
  "ancestors":  "(Metalo - Grain Size Analysis)" ,
  "filename": "wizard.mode.html",
  "hash": "",
  "id": 277, 
},{
  "title": "Creating Report",
  "ancestors":  "(Filter Particle Analysis &gt; Basic Workflows)" ,
  "filename": "workflow.analyse.report.html",
  "hash": "",
  "id": 278, 
},{
  "title": "Size Classes Definition",
  "ancestors":  "(Filter Particle Analysis &gt; Advanced Functions)" ,
  "filename": "workflow.classes.html",
  "hash": "",
  "id": 279, 
},{
  "title": "Defining Detection",
  "ancestors":  "(Filter Particle Analysis &gt; Basic Workflows)" ,
  "filename": "workflow.detection.html",
  "hash": "",
  "id": 280, 
},{
  "title": "Advanced Measurement Options",
  "ancestors":  "(Filter Particle Analysis &gt; Advanced Functions)" ,
  "filename": "workflow.measurement.settings.html",
  "hash": "",
  "id": 281, 
},{
  "title": "Displaying Results",
  "ancestors":  "(Filter Particle Analysis &gt; Basic Workflows)" ,
  "filename": "workflow.results.html",
  "hash": "",
  "id": 282, 
},{
  "title": "Advanced Scanning",
  "ancestors":  "(Filter Particle Analysis &gt; Advanced Functions)" ,
  "filename": "workflow.scan.setup.html",
  "hash": "",
  "id": 283, 
},

// refData #2 for the list of all titles: display title and ancestors
// titles of separate pages are not added, because they are already there from the previous step (remap.hash == "")
{
  "title": "Preparing the training binary image",
  "ancestors": " (NIS.ai &gt; Functions &gt; Segment.ai)",
  "filename": "nis.ai.modalities.segment.html",
  "hash": "#nis.ai.modalities.segment.prepare",
  "id": 284,
},{
  "title": "Examples of binary drawing",
  "ancestors": " (NIS.ai &gt; Functions &gt; Segment.ai)",
  "filename": "nis.ai.modalities.segment.html",
  "hash": "#p2c2s1s1s2",
  "id": 285,
},{
  "title": "Training, Testing and Running the Segment.ai",
  "ancestors": " (NIS.ai &gt; Functions &gt; Segment.ai)",
  "filename": "nis.ai.modalities.segment.html",
  "hash": "#p2c2s1s1s3",
  "id": 286,
},{
  "title": "Training, Testing and Running the Convert.ai",
  "ancestors": " (NIS.ai &gt; Functions &gt; Convert.ai)",
  "filename": "nis.ai.modalities.modality.transfer.html",
  "hash": "#p2c2s1s3s1",
  "id": 287,
},{
  "title": "Training, Testing and Running the Enhance.ai",
  "ancestors": " (NIS.ai &gt; Functions &gt; Enhance.ai)",
  "filename": "nis.ai.modalities.signal.enhancement.html",
  "hash": "#p2c2s1s4s1",
  "id": 288,
},{
  "title": "Analysis Explorer Basics",
  "ancestors": " (Bio Analysis Modules &gt; Analysis Explorer)",
  "filename": "analysis.explorer.html",
  "hash": "#p2c4s1s1",
  "id": 289,
},{
  "title": "Usage",
  "ancestors": " (Bio Analysis Modules &gt; Analysis Explorer)",
  "filename": "analysis.explorer.html",
  "hash": "#p2c4s1s2",
  "id": 290,
},{
  "title": "Managing Recipes",
  "ancestors": " (Bio Analysis Modules &gt; Analysis Explorer)",
  "filename": "analysis.explorer.html",
  "hash": "#p2c4s1s3",
  "id": 291,
},{
  "title": "Batch Analysis Options",
  "ancestors": " (Bio Analysis Modules &gt; Analysis Explorer)",
  "filename": "analysis.explorer.html",
  "hash": "#batch.analysis.options",
  "id": 292,
},{
  "title": "Introduction to General Analysis",
  "ancestors": " (Bio Analysis Modules &gt; General Analysis)",
  "filename": "analysis.general.html",
  "hash": "#p2c4s6s1",
  "id": 293,
},{
  "title": "Graphical user interface",
  "ancestors": " (Bio Analysis Modules &gt; General Analysis)",
  "filename": "analysis.general.html",
  "hash": "#p2c4s6s2",
  "id": 294,
},{
  "title": "How To Set General Analysis",
  "ancestors": " (Bio Analysis Modules &gt; General Analysis)",
  "filename": "analysis.general.html",
  "hash": "#p2c4s6s3",
  "id": 295,
},{
  "title": "General Analysis in NIS-Elements - Model Use Case",
  "ancestors": " (Bio Analysis Modules &gt; General Analysis)",
  "filename": "analysis.general.html",
  "hash": "#analysis.general.example",
  "id": 296,
},{
  "title": "General Analysis in JOBS Module - Appended Point Set Use Case",
  "ancestors": " (Bio Analysis Modules &gt; General Analysis)",
  "filename": "analysis.general.html",
  "hash": "#ga.usecase.mp.global",
  "id": 297,
},{
  "title": "General Analysis in JOBS Module - Replaced Point Set Use Case",
  "ancestors": " (Bio Analysis Modules &gt; General Analysis)",
  "filename": "analysis.general.html",
  "hash": "#ga.usecase.mp.local",
  "id": 298,
},{
  "title": "Scheme: GA creates local point set",
  "ancestors": " (Bio Analysis Modules &gt; General Analysis &gt; General Analysis in JOBS Module - Replaced Point Set Use Case)",
  "filename": "analysis.general.html",
  "hash": "#fig.local.pointset",
  "id": 299,
},{
  "title": "Deconvolution methods",
  "ancestors": " (Deconvolution &gt; Choosing the Deconvolution Method)",
  "filename": "howto.deconv.choose.method.html",
  "hash": "#howto.deconv.methods",
  "id": 300,
},{
  "title": "Landweber Deconvolution",
  "ancestors": " (Deconvolution &gt; Algorithms)",
  "filename": "deconv.algorithms.html",
  "hash": "#mod.deconvolution.landweber",
  "id": 301,
},{
  "title": "Richardson-Lucy Deconvolution",
  "ancestors": " (Deconvolution &gt; Algorithms)",
  "filename": "deconv.algorithms.html",
  "hash": "#mod.deconvolution.standard",
  "id": 302,
},{
  "title": "Blind Deconvolution",
  "ancestors": " (Deconvolution &gt; Algorithms)",
  "filename": "deconv.algorithms.html",
  "hash": "#mod.deconvolution.blind",
  "id": 303,
},{
  "title": "Fast Deconvolution",
  "ancestors": " (Deconvolution &gt; Algorithms)",
  "filename": "deconv.algorithms.html",
  "hash": "#mod.deconvolution.wiener",
  "id": 304,
},{
  "title": "Resources",
  "ancestors": " (Deconvolution &gt; Algorithms)",
  "filename": "deconv.algorithms.html",
  "hash": "#p2c7s3s5",
  "id": 305,
},{
  "title": "Widefield Sampling",
  "ancestors": " (Deconvolution &gt; Proper Sampling)",
  "filename": "p2c7s5.html",
  "hash": "#p2c7s5s1",
  "id": 306,
},{
  "title": "Confocal Sampling",
  "ancestors": " (Deconvolution &gt; Proper Sampling)",
  "filename": "p2c7s5.html",
  "hash": "#p2c7s5s2",
  "id": 307,
},{
  "title": "Automatic Deconvolution",
  "ancestors": " (Deconvolution &gt; Dialog Settings &gt; Automatic Deconvolution)",
  "filename": "automatic.deconvolution.html",
  "hash": "#CR__Deconv_Automatic",
  "id": 308,
},{
  "title": "Numerical Aperture",
  "ancestors": " (Deconvolution &gt; Dialog Settings &gt; Automatic Deconvolution &gt; Dialog window settings)",
  "filename": "automatic.deconvolution.html",
  "hash": "#numerical.aperture.dlg",
  "id": 309,
},{
  "title": "Immersion Refractive Index",
  "ancestors": " (Deconvolution &gt; Dialog Settings &gt; Automatic Deconvolution &gt; Dialog window settings)",
  "filename": "automatic.deconvolution.html",
  "hash": "#immersion.refractive.index.dlg",
  "id": 310,
},{
  "title": "Calibration",
  "ancestors": " (Deconvolution &gt; Dialog Settings &gt; Automatic Deconvolution &gt; Dialog window settings)",
  "filename": "automatic.deconvolution.html",
  "hash": "#calibration.dlg",
  "id": 311,
},{
  "title": "Channels",
  "ancestors": " (Deconvolution &gt; Dialog Settings &gt; Automatic Deconvolution &gt; Dialog window settings)",
  "filename": "automatic.deconvolution.html",
  "hash": "#channels.dlg",
  "id": 312,
},{
  "title": "Use Spherical Aberration Correction",
  "ancestors": " (Deconvolution &gt; Dialog Settings &gt; Automatic Deconvolution &gt; Dialog window settings)",
  "filename": "automatic.deconvolution.html",
  "hash": "#use.spherical.aberration.correction.dlg",
  "id": 313,
},{
  "title": "Create new document",
  "ancestors": " (Deconvolution &gt; Dialog Settings &gt; Automatic Deconvolution &gt; Dialog window settings)",
  "filename": "automatic.deconvolution.html",
  "hash": "#create.new.document.dlg",
  "id": 314,
},{
  "title": "Do not show this dialog for images with valid metadata",
  "ancestors": " (Deconvolution &gt; Dialog Settings &gt; Automatic Deconvolution &gt; Dialog window settings)",
  "filename": "automatic.deconvolution.html",
  "hash": "#do.not.show.dlg",
  "id": 315,
},{
  "title": "Deconvolution Method",
  "ancestors": " (Deconvolution &gt; Dialog Settings &gt; 3D Deconvolution  &gt; Dialog window settings)",
  "filename": "deconv.settings.3d.html",
  "hash": "#deconvolution.method.dlg",
  "id": 316,
},{
  "title": "Modality",
  "ancestors": " (Deconvolution &gt; Dialog Settings &gt; 3D Deconvolution  &gt; Dialog window settings)",
  "filename": "deconv.settings.3d.html",
  "hash": "#modality.dlg",
  "id": 317,
},{
  "title": "Pinhole size",
  "ancestors": " (Deconvolution &gt; Dialog Settings &gt; 3D Deconvolution  &gt; Dialog window settings)",
  "filename": "deconv.settings.3d.html",
  "hash": "#pinhole.size.dlg",
  "id": 318,
},{
  "title": "Slit orientation",
  "ancestors": " (Deconvolution &gt; Dialog Settings &gt; 3D Deconvolution  &gt; Dialog window settings)",
  "filename": "deconv.settings.3d.html",
  "hash": "#slit.orientation.dlg",
  "id": 319,
},{
  "title": "Magnification",
  "ancestors": " (Deconvolution &gt; Dialog Settings &gt; 3D Deconvolution  &gt; Dialog window settings)",
  "filename": "deconv.settings.3d.html",
  "hash": "#magnification.dlg",
  "id": 320,
},{
  "title": "Numerical Aperture",
  "ancestors": " (Deconvolution &gt; Dialog Settings &gt; 3D Deconvolution  &gt; Dialog window settings)",
  "filename": "deconv.settings.3d.html",
  "hash": "#numerical.aperture.dlg",
  "id": 321,
},{
  "title": "Immersion Refractive Index",
  "ancestors": " (Deconvolution &gt; Dialog Settings &gt; 3D Deconvolution  &gt; Dialog window settings)",
  "filename": "deconv.settings.3d.html",
  "hash": "#immersion.refractive.index.dlg",
  "id": 322,
},{
  "title": "Calibration",
  "ancestors": " (Deconvolution &gt; Dialog Settings &gt; 3D Deconvolution  &gt; Dialog window settings)",
  "filename": "deconv.settings.3d.html",
  "hash": "#calibration.dlg",
  "id": 323,
},{
  "title": "Z-Step",
  "ancestors": " (Deconvolution &gt; Dialog Settings &gt; 3D Deconvolution  &gt; Dialog window settings)",
  "filename": "deconv.settings.3d.html",
  "hash": "#z.step.dlg",
  "id": 324,
},{
  "title": "Noise Level",
  "ancestors": " (Deconvolution &gt; Dialog Settings &gt; 3D Deconvolution  &gt; Dialog window settings)",
  "filename": "deconv.settings.3d.html",
  "hash": "#noise.level.dlg",
  "id": 325,
},{
  "title": "Iterations",
  "ancestors": " (Deconvolution &gt; Dialog Settings &gt; 3D Deconvolution  &gt; Dialog window settings)",
  "filename": "deconv.settings.3d.html",
  "hash": "#iterations.dlg",
  "id": 326,
},{
  "title": "Import PSF",
  "ancestors": " (Deconvolution &gt; Dialog Settings &gt; 3D Deconvolution  &gt; Dialog window settings)",
  "filename": "deconv.settings.3d.html",
  "hash": "#import.psf.dlg",
  "id": 327,
},{
  "title": "Use Spherical Aberration Correction",
  "ancestors": " (Deconvolution &gt; Dialog Settings &gt; 3D Deconvolution  &gt; Dialog window settings)",
  "filename": "deconv.settings.3d.html",
  "hash": "#use.spherical.aberration.correction.dlg",
  "id": 328,
},{
  "title": "Output",
  "ancestors": " (Deconvolution &gt; Dialog Settings &gt; 3D Deconvolution  &gt; Dialog window settings)",
  "filename": "deconv.settings.3d.html",
  "hash": "#output.dlg",
  "id": 329,
},{
  "title": "Channels",
  "ancestors": " (Deconvolution &gt; Dialog Settings &gt; 3D Deconvolution  &gt; Dialog window settings)",
  "filename": "deconv.settings.3d.html",
  "hash": "#channels.dlg",
  "id": 330,
},{
  "title": "Preprocessing",
  "ancestors": " (Deconvolution &gt; Dialog Settings &gt; 3D Deconvolution  &gt; Dialog window settings)",
  "filename": "deconv.settings.3d.html",
  "hash": "#preprocessing.dlg",
  "id": 331,
},{
  "title": "Remove spurious high intensity pixels",
  "ancestors": " (Deconvolution &gt; Dialog Settings &gt; 3D Deconvolution  &gt; Dialog window settings)",
  "filename": "deconv.settings.3d.html",
  "hash": "#remove.high.intensity.pixels",
  "id": 332,
},{
  "title": "Use Spherical Aberration Correction",
  "ancestors": " (Deconvolution &gt; Dialog Settings &gt; 3D Deconvolution  &gt; Dialog window settings)",
  "filename": "deconv.settings.3d.html",
  "hash": "#use.spherical.aberration.correction.dlg",
  "id": 333,
},{
  "title": "Import PSF",
  "ancestors": " (Deconvolution &gt; Dialog Settings &gt; 3D Deconvolution  &gt; Dialog window settings)",
  "filename": "deconv.settings.3d.html",
  "hash": "#import.psf.dlg",
  "id": 334,
},{
  "title": "Run on ROI",
  "ancestors": " (Deconvolution &gt; Dialog Settings &gt; 3D Deconvolution  &gt; Dialog window settings)",
  "filename": "deconv.settings.3d.html",
  "hash": "#region.of.interest.dlg",
  "id": 335,
},{
  "title": "Deconvolve",
  "ancestors": " (Deconvolution &gt; Dialog Settings &gt; 3D Deconvolution  &gt; Dialog window settings)",
  "filename": "deconv.settings.3d.html",
  "hash": "#deconvolve.dlg",
  "id": 336,
},{
  "title": "Define Background",
  "ancestors": " (Deconvolution &gt; Dialog Settings &gt; 3D Deconvolution )",
  "filename": "deconv.settings.3d.html",
  "hash": "#deconv.setting.detect.background",
  "id": 337,
},{
  "title": "Deconvolution Method",
  "ancestors": " (Deconvolution &gt; Dialog Settings &gt; 2D Deconvolution &gt; Dialog window settings)",
  "filename": "deconv.settings.2d.html",
  "hash": "#deconvolution.method.dlg",
  "id": 338,
},{
  "title": "Modality",
  "ancestors": " (Deconvolution &gt; Dialog Settings &gt; 2D Deconvolution &gt; Dialog window settings)",
  "filename": "deconv.settings.2d.html",
  "hash": "#modality.dlg",
  "id": 339,
},{
  "title": "Pinhole size",
  "ancestors": " (Deconvolution &gt; Dialog Settings &gt; 2D Deconvolution &gt; Dialog window settings)",
  "filename": "deconv.settings.2d.html",
  "hash": "#pinhole.size.dlg",
  "id": 340,
},{
  "title": "Slit orientation",
  "ancestors": " (Deconvolution &gt; Dialog Settings &gt; 2D Deconvolution &gt; Dialog window settings)",
  "filename": "deconv.settings.2d.html",
  "hash": "#slit.orientation.dlg",
  "id": 341,
},{
  "title": "Magnification",
  "ancestors": " (Deconvolution &gt; Dialog Settings &gt; 2D Deconvolution &gt; Dialog window settings)",
  "filename": "deconv.settings.2d.html",
  "hash": "#magnification.dlg",
  "id": 342,
},{
  "title": "Numerical Aperture",
  "ancestors": " (Deconvolution &gt; Dialog Settings &gt; 2D Deconvolution &gt; Dialog window settings)",
  "filename": "deconv.settings.2d.html",
  "hash": "#numerical.aperture.dlg",
  "id": 343,
},{
  "title": "Refraction Index",
  "ancestors": " (Deconvolution &gt; Dialog Settings &gt; 2D Deconvolution &gt; Dialog window settings)",
  "filename": "deconv.settings.2d.html",
  "hash": "#refraction.index.dlg",
  "id": 344,
},{
  "title": "Calibration",
  "ancestors": " (Deconvolution &gt; Dialog Settings &gt; 2D Deconvolution &gt; Dialog window settings)",
  "filename": "deconv.settings.2d.html",
  "hash": "#calibration.dlg",
  "id": 345,
},{
  "title": "Output",
  "ancestors": " (Deconvolution &gt; Dialog Settings &gt; 2D Deconvolution &gt; Dialog window settings)",
  "filename": "deconv.settings.2d.html",
  "hash": "#output.dlg",
  "id": 346,
},{
  "title": "Z-Step",
  "ancestors": " (Deconvolution &gt; Dialog Settings &gt; 2D Deconvolution &gt; Dialog window settings)",
  "filename": "deconv.settings.2d.html",
  "hash": "#z.step.dlg",
  "id": 347,
},{
  "title": "Noise Level",
  "ancestors": " (Deconvolution &gt; Dialog Settings &gt; 2D Deconvolution &gt; Dialog window settings)",
  "filename": "deconv.settings.2d.html",
  "hash": "#noise.level.dlg",
  "id": 348,
},{
  "title": "Iterations",
  "ancestors": " (Deconvolution &gt; Dialog Settings &gt; 2D Deconvolution &gt; Dialog window settings)",
  "filename": "deconv.settings.2d.html",
  "hash": "#iterations.dlg",
  "id": 349,
},{
  "title": "Specimen Thickness",
  "ancestors": " (Deconvolution &gt; Dialog Settings &gt; 2D Deconvolution &gt; Dialog window settings)",
  "filename": "deconv.settings.2d.html",
  "hash": "#specimen.thickness.dlg",
  "id": 350,
},{
  "title": "Import PSF",
  "ancestors": " (Deconvolution &gt; Dialog Settings &gt; 2D Deconvolution &gt; Dialog window settings)",
  "filename": "deconv.settings.2d.html",
  "hash": "#import.psf.dlg",
  "id": 351,
},{
  "title": "Channels",
  "ancestors": " (Deconvolution &gt; Dialog Settings &gt; 2D Deconvolution &gt; Dialog window settings)",
  "filename": "deconv.settings.2d.html",
  "hash": "#channels.dlg",
  "id": 352,
},{
  "title": "Preprocessing",
  "ancestors": " (Deconvolution &gt; Dialog Settings &gt; 2D Deconvolution &gt; Dialog window settings)",
  "filename": "deconv.settings.2d.html",
  "hash": "#preprocessing.dlg",
  "id": 353,
},{
  "title": "Remove spurious high intensity pixels",
  "ancestors": " (Deconvolution &gt; Dialog Settings &gt; 2D Deconvolution &gt; Dialog window settings)",
  "filename": "deconv.settings.2d.html",
  "hash": "#remove.high.intensity.pixels",
  "id": 354,
},{
  "title": "Use Spherical Aberration Correction",
  "ancestors": " (Deconvolution &gt; Dialog Settings &gt; 2D Deconvolution &gt; Dialog window settings)",
  "filename": "deconv.settings.2d.html",
  "hash": "#use.spherical.aberration.correction.dlg",
  "id": 355,
},{
  "title": "Import PSF",
  "ancestors": " (Deconvolution &gt; Dialog Settings &gt; 2D Deconvolution &gt; Dialog window settings)",
  "filename": "deconv.settings.2d.html",
  "hash": "#import.psf.dlg",
  "id": 356,
},{
  "title": "Run on ROI",
  "ancestors": " (Deconvolution &gt; Dialog Settings &gt; 2D Deconvolution &gt; Dialog window settings)",
  "filename": "deconv.settings.2d.html",
  "hash": "#region.of.interest.dlg",
  "id": 357,
},{
  "title": "Deconvolve",
  "ancestors": " (Deconvolution &gt; Dialog Settings &gt; 2D Deconvolution &gt; Dialog window settings)",
  "filename": "deconv.settings.2d.html",
  "hash": "#deconvolve.dlg",
  "id": 358,
},{
  "title": "Specimen Thickness",
  "ancestors": " (Deconvolution &gt; Dialog Settings &gt; Live De-Blur Setup &gt; Live De-Blur Options)",
  "filename": "deconv.settings.livedeblur.html",
  "hash": "#specimen.thickness.dlg",
  "id": 359,
},{
  "title": "Contrast enhancement",
  "ancestors": " (Deconvolution &gt; Dialog Settings &gt; Live De-Blur Setup &gt; Live De-Blur Options)",
  "filename": "deconv.settings.livedeblur.html",
  "hash": "#contrast.enhancement.dlg",
  "id": 360,
},{
  "title": "Exploring the surface view",
  "ancestors": " (Extended Depth of Focus &gt; EDF Step by Step &gt; View the result)",
  "filename": "edf.step.by.step.html",
  "hash": "#surface",
  "id": 361,
},{
  "title": "Analysis Overview",
  "ancestors": " (Filter Particle Analysis &gt; Introduction to Filters)",
  "filename": "p2c9s1.html",
  "hash": "#p2c9s1s1",
  "id": 362,
},{
  "title": "Filter Particle Analysis Layout",
  "ancestors": " (Filter Particle Analysis &gt; Introduction to Filters)",
  "filename": "p2c9s1.html",
  "hash": "#p2c9s1s2",
  "id": 363,
},{
  "title": "Hardware Setup",
  "ancestors": " (FRET &gt; Capturing FRET Image, FRET Calibration)",
  "filename": "fret.capture.html",
  "hash": "#p2c10s2s1",
  "id": 364,
},{
  "title": "FRET Calibration",
  "ancestors": " (FRET &gt; Capturing FRET Image, FRET Calibration)",
  "filename": "fret.capture.html",
  "hash": "#fret.calibration",
  "id": 365,
},{
  "title": "FRET Method",
  "ancestors": " (FRET &gt; Capturing FRET Image, FRET Calibration)",
  "filename": "fret.capture.html",
  "hash": "#fret.method",
  "id": 366,
},{
  "title": "Setting up Optical configurations",
  "ancestors": " (FRET &gt; Ratio Experiment, Calcium Calibration)",
  "filename": "GS_HowTo.ratio.html",
  "hash": "#p2c10s7s1",
  "id": 367,
},{
  "title": "Configuring 6D Dialog",
  "ancestors": " (FRET &gt; Ratio Experiment, Calcium Calibration)",
  "filename": "GS_HowTo.ratio.html",
  "hash": "#p2c10s7s2",
  "id": 368,
},{
  "title": "Setting up ROIs on Preview Image",
  "ancestors": " (FRET &gt; Ratio Experiment, Calcium Calibration)",
  "filename": "GS_HowTo.ratio.html",
  "hash": "#p2c10s7s3",
  "id": 369,
},{
  "title": "Time Measurements Set Up",
  "ancestors": " (FRET &gt; Ratio Experiment, Calcium Calibration)",
  "filename": "GS_HowTo.ratio.html",
  "hash": "#p2c10s7s4",
  "id": 370,
},{
  "title": "Capturing the Data Set",
  "ancestors": " (FRET &gt; Ratio Experiment, Calcium Calibration)",
  "filename": "GS_HowTo.ratio.html",
  "hash": "#p2c10s7s5",
  "id": 371,
},{
  "title": "Calcium Calibration",
  "ancestors": " (FRET &gt; Ratio Experiment, Calcium Calibration)",
  "filename": "GS_HowTo.ratio.html",
  "hash": "#p2c10s7s6",
  "id": 372,
},{
  "title": "Export Data",
  "ancestors": " (FRET &gt; Ratio Experiment, Calcium Calibration)",
  "filename": "GS_HowTo.ratio.html",
  "hash": "#p2c10s7s7",
  "id": 373,
},{
  "title": "Copy image",
  "ancestors": " (FRET &gt; Custom Equation)",
  "filename": "howto.custom.equation.html",
  "hash": "#p2c10s8s1",
  "id": 374,
},{
  "title": "Inverted image",
  "ancestors": " (FRET &gt; Custom Equation)",
  "filename": "howto.custom.equation.html",
  "hash": "#p2c10s8s2",
  "id": 375,
},{
  "title": "Gamma image",
  "ancestors": " (FRET &gt; Custom Equation)",
  "filename": "howto.custom.equation.html",
  "hash": "#p2c10s8s3",
  "id": 376,
},{
  "title": "Difference image",
  "ancestors": " (FRET &gt; Custom Equation)",
  "filename": "howto.custom.equation.html",
  "hash": "#p2c10s8s4",
  "id": 377,
},{
  "title": "Increasing intensity image",
  "ancestors": " (FRET &gt; Custom Equation)",
  "filename": "howto.custom.equation.html",
  "hash": "#p2c10s8s5",
  "id": 378,
},{
  "title": "Measurement &gt; Basic &gt; Volume",
  "ancestors": " (General Analysis 3  &gt; Nodes &gt;  Measurement)",
  "filename": "node.measurement_section.html",
  "hash": "#CR_Meas2_VolumeMeasurement",
  "id": 379,
},{
  "title": "Measurement &gt; Basic &gt; Object",
  "ancestors": " (General Analysis 3  &gt; Nodes &gt;  Measurement)",
  "filename": "node.measurement_section.html",
  "hash": "#CR_Meas2_Object3dMeasurement",
  "id": 380,
},{
  "title": "Measurement &gt; Basic &gt; Object Count",
  "ancestors": " (General Analysis 3  &gt; Nodes &gt;  Measurement)",
  "filename": "node.measurement_section.html",
  "hash": "#CR_Meas2_Object3dCount",
  "id": 381,
},{
  "title": "Measurement &gt; Basic &gt; Parent",
  "ancestors": " (General Analysis 3  &gt; Nodes &gt;  Measurement)",
  "filename": "node.measurement_section.html",
  "hash": "#CR_Meas2_Parent3dMeasurement",
  "id": 382,
},{
  "title": "Measurement &gt; Basic &gt; Children",
  "ancestors": " (General Analysis 3  &gt; Nodes &gt;  Measurement)",
  "filename": "node.measurement_section.html",
  "hash": "#CR_Meas2_Children3dMeasurement",
  "id": 383,
},{
  "title": "Reference",
  "ancestors": " (General Analysis 3  &gt; Nodes &gt;  Measurement)",
  "filename": "node.measurement_section.html",
  "hash": "#ga3.parenting.reference.ab",
  "id": 384,
},{
  "title": "Generate bins automatically",
  "ancestors": " (General Analysis 3  &gt; Nodes &gt;  Data management)",
  "filename": "node.datamanagement_section.html",
  "hash": "#generate.bins.automatically",
  "id": 385,
},{
  "title": "All Columns",
  "ancestors": " (General Analysis 3  &gt; Nodes &gt;  Results &amp; Graphs &gt; Data)",
  "filename": "node.results.graphs_section.html",
  "hash": "#CR_GA3_GraphDataSeries",
  "id": 386,
},{
  "title": "All Columns",
  "ancestors": " (General Analysis 3  &gt; Nodes &gt;  Results &amp; Graphs &gt; Data)",
  "filename": "node.results.graphs_section.html",
  "hash": "#CR_GA3_GraphErrorBar",
  "id": 387,
},{
  "title": "All Columns",
  "ancestors": " (General Analysis 3  &gt; Nodes &gt; Hidden Nodes (created in NIS-Elements 5.4x) &gt; Data)",
  "filename": "p2c11s5s13.html",
  "hash": "#CR_GA3_GraphDataSeries.dupl",
  "id": 388,
},{
  "title": "All Columns",
  "ancestors": " (General Analysis 3  &gt; Nodes &gt; Hidden Nodes (created in NIS-Elements 5.4x) &gt; Data)",
  "filename": "p2c11s5s13.html",
  "hash": "#CR_GA3_GraphErrorBar.dupl",
  "id": 389,
},{
  "title": "Jobs Explorer and Jobs Toolbar",
  "ancestors": " (HCA &gt; Introduction to HCA)",
  "filename": "p2c12s1.html",
  "hash": "#jobs.explorer.hca",
  "id": 390,
},{
  "title": "Analysis performed on job results",
  "ancestors": " (HCA &gt; HCA Analysis)",
  "filename": "hcajobs.analysis.html",
  "hash": "#analysis.on.captured.data",
  "id": 391,
},{
  "title": "Main Concept",
  "ancestors": " (JOBS and HCA &gt; Introduction to the JOBS Module)",
  "filename": "jobs.overview.html",
  "hash": "#p2c15s1s1",
  "id": 392,
},{
  "title": "Introduction to Jobs Explorer",
  "ancestors": " (JOBS and HCA &gt; Introduction to the JOBS Module)",
  "filename": "jobs.overview.html",
  "hash": "#jobs.explorer",
  "id": 393,
},{
  "title": "Jobs Explorer Options",
  "ancestors": " (JOBS and HCA &gt; Introduction to the JOBS Module)",
  "filename": "jobs.overview.html",
  "hash": "#p2c15s1s3",
  "id": 394,
},{
  "title": "Job Definition Window",
  "ancestors": " (JOBS and HCA &gt; Introduction to the JOBS Module)",
  "filename": "jobs.overview.html",
  "hash": "#jobs.definition",
  "id": 395,
},{
  "title": "Job Definition Window Tools",
  "ancestors": " (JOBS and HCA &gt; Introduction to the JOBS Module)",
  "filename": "jobs.overview.html",
  "hash": "#p2c15s1s5",
  "id": 396,
},{
  "title": "Save OME Metadata",
  "ancestors": " (JOBS and HCA &gt; Introduction to the JOBS Module &gt; Job Definition Window Tools)",
  "filename": "jobs.overview.html",
  "hash": "#opt.save.ome.metadata",
  "id": 397,
},{
  "title": "Jobs Toolbar",
  "ancestors": " (JOBS and HCA &gt; Introduction to the JOBS Module)",
  "filename": "jobs.overview.html",
  "hash": "#jobs.jobstoolbar",
  "id": 398,
},{
  "title": "New Job from Template",
  "ancestors": " (JOBS and HCA &gt; Creating a Job)",
  "filename": "jobs.creating.html",
  "hash": "#jobs_using_job_templates",
  "id": 399,
},{
  "title": "New Blank Job",
  "ancestors": " (JOBS and HCA &gt; Creating a Job)",
  "filename": "jobs.creating.html",
  "hash": "#jobs.create",
  "id": 400,
},{
  "title": "Using Job Definition Wizard",
  "ancestors": " (JOBS and HCA &gt; Creating a Job)",
  "filename": "jobs.creating.html",
  "hash": "#jobs.definition.wizard",
  "id": 401,
},{
  "title": "Edit Wizard",
  "ancestors": " (JOBS and HCA &gt; Creating a Job &gt; Using Job Definition Wizard)",
  "filename": "jobs.creating.html",
  "hash": "#jobs.edit.wizard",
  "id": 402,
},{
  "title": "Defining Samples",
  "ancestors": " (JOBS and HCA &gt; Creating a Job)",
  "filename": "jobs.creating.html",
  "hash": "#jobs.define.sample",
  "id": 403,
},{
  "title": "Defining Capture",
  "ancestors": " (JOBS and HCA &gt; Creating a Job)",
  "filename": "jobs.creating.html",
  "hash": "#jobs.defining.capture",
  "id": 404,
},{
  "title": "Defining Loops",
  "ancestors": " (JOBS and HCA &gt; Creating a Job)",
  "filename": "jobs.creating.html",
  "hash": "#jobs.defining.loops",
  "id": 405,
},{
  "title": "Defining Parameters",
  "ancestors": " (JOBS and HCA &gt; Creating a Job)",
  "filename": "jobs.creating.html",
  "hash": "#jobs.defining.parameters",
  "id": 406,
},{
  "title": "Job Execution Progress window",
  "ancestors": " (JOBS and HCA &gt; Running a Job, Viewing Results and Graphs)",
  "filename": "hca.viewresults.html",
  "hash": "#jobs.execution.window",
  "id": 407,
},{
  "title": "Result View",
  "ancestors": " (JOBS and HCA &gt; Running a Job, Viewing Results and Graphs)",
  "filename": "hca.viewresults.html",
  "hash": "#p2c15s5s2",
  "id": 408,
},{
  "title": "Job Result: Thumbnail View (Heatmap)",
  "ancestors": " (JOBS and HCA &gt; Running a Job, Viewing Results and Graphs &gt; Result View &gt; Top Toolbar options)",
  "filename": "hca.viewresults.html",
  "hash": "#fig.job.result.heatmap",
  "id": 409,
},{
  "title": "Job Result: Grid View",
  "ancestors": " (JOBS and HCA &gt; Running a Job, Viewing Results and Graphs &gt; Result View &gt; Top Toolbar options)",
  "filename": "hca.viewresults.html",
  "hash": "#fig.job.result.grid",
  "id": 410,
},{
  "title": "Graph View",
  "ancestors": " (JOBS and HCA &gt; Running a Job, Viewing Results and Graphs)",
  "filename": "hca.viewresults.html",
  "hash": "#hca.graph.view",
  "id": 411,
},{
  "title": "Context menu on a job",
  "ancestors": " (JOBS and HCA &gt; Running a Job, Viewing Results and Graphs)",
  "filename": "hca.viewresults.html",
  "hash": "#p2c15s5s4",
  "id": 412,
},{
  "title": "Context menu on a job run",
  "ancestors": " (JOBS and HCA &gt; Running a Job, Viewing Results and Graphs)",
  "filename": "hca.viewresults.html",
  "hash": "#p2c15s5s5",
  "id": 413,
},{
  "title": "Labels and Metadata in Result View",
  "ancestors": " (JOBS and HCA &gt; Labels and Metadata)",
  "filename": "labels.and.metadata.html",
  "hash": "#p2c15s8s2",
  "id": 414,
},{
  "title": "Importing predefined presets from spreadsheet applications",
  "ancestors": " (JOBS and HCA &gt; Labels and Metadata)",
  "filename": "labels.and.metadata.html",
  "hash": "#labeling.preset.formats",
  "id": 415,
},{
  "title": "Labeling during runtime",
  "ancestors": " (JOBS and HCA &gt; Labels and Metadata)",
  "filename": "labels.and.metadata.html",
  "hash": "#p2c15s8s4",
  "id": 416,
},{
  "title": "Database context menu",
  "ancestors": " (JOBS and HCA &gt; Advanced Topics &gt; Job Database)",
  "filename": "hca.db.html",
  "hash": "#jobs.explorer.db",
  "id": 417,
},{
  "title": "Database Backup",
  "ancestors": " (JOBS and HCA &gt; Advanced Topics &gt; Job Database)",
  "filename": "hca.db.html",
  "hash": "#jobs.backup.database",
  "id": 418,
},{
  "title": "Database Restore",
  "ancestors": " (JOBS and HCA &gt; Advanced Topics &gt; Job Database)",
  "filename": "hca.db.html",
  "hash": "#jobs.restore.database",
  "id": 419,
},{
  "title": "Database Properties",
  "ancestors": " (JOBS and HCA &gt; Advanced Topics &gt; Job Database)",
  "filename": "hca.db.html",
  "hash": "#jobs.database.properties",
  "id": 420,
},{
  "title": "Database Files",
  "ancestors": " (JOBS and HCA &gt; Advanced Topics &gt; Job Database)",
  "filename": "hca.db.html",
  "hash": "#p2c15s9s1s5",
  "id": 421,
},{
  "title": "NIS-Elements Upgrades",
  "ancestors": " (JOBS and HCA &gt; Advanced Topics &gt; Job Database)",
  "filename": "hca.db.html",
  "hash": "#p2c15s9s1s6",
  "id": 422,
},{
  "title": "Database Upload/Download",
  "ancestors": " (JOBS and HCA &gt; Advanced Topics &gt; Job Database)",
  "filename": "hca.db.html",
  "hash": "#jobs.database.merge",
  "id": 423,
},{
  "title": "Nested loops",
  "ancestors": " (JOBS and HCA &gt; Advanced Topics &gt; Advanced Files Import &gt; Loops and Indexes)",
  "filename": "p2c15s9s3s3.html",
  "hash": "#p2c15s9s3s3s1",
  "id": 424,
},{
  "title": "Succeeding loops",
  "ancestors": " (JOBS and HCA &gt; Advanced Topics &gt; Advanced Files Import &gt; Loops and Indexes)",
  "filename": "p2c15s9s3s3.html",
  "hash": "#p2c15s9s3s3s2",
  "id": 425,
},{
  "title": "Automatic Index Finding Using a Pattern",
  "ancestors": " (JOBS and HCA &gt; Advanced Topics &gt; Advanced Files Import &gt; Assigning Indexes to Frames)",
  "filename": "p2c15s9s3s5.html",
  "hash": "#p2c15s9s3s5s1",
  "id": 426,
},{
  "title": "Manual Index Definition",
  "ancestors": " (JOBS and HCA &gt; Advanced Topics &gt; Advanced Files Import &gt; Assigning Indexes to Frames)",
  "filename": "p2c15s9s3s5.html",
  "hash": "#jobs.advanced.import.manual.index",
  "id": 427,
},{
  "title": "Example 1",
  "ancestors": " (JOBS and HCA &gt; Advanced Topics &gt; Advanced Files Import &gt; Examples)",
  "filename": "p2c15s9s3s7.html",
  "hash": "#p2c15s9s3s7s1",
  "id": 428,
},{
  "title": "Example 2",
  "ancestors": " (JOBS and HCA &gt; Advanced Topics &gt; Advanced Files Import &gt; Examples)",
  "filename": "p2c15s9s3s7.html",
  "hash": "#p2c15s9s3s7s2",
  "id": 429,
},{
  "title": "Example 3",
  "ancestors": " (JOBS and HCA &gt; Advanced Topics &gt; Advanced Files Import &gt; Examples)",
  "filename": "p2c15s9s3s7.html",
  "hash": "#p2c15s9s3s7s3",
  "id": 430,
},{
  "title": "Select from DB...",
  "ancestors": " (JOBS and HCA &gt; Tasks &gt; Well Plates)",
  "filename": "task.wellplates_section.html",
  "hash": "#jobs_select_wellplate",
  "id": 431,
},{
  "title": "Custom Well Plate Options",
  "ancestors": " (JOBS and HCA &gt; Tasks &gt; Well Plates)",
  "filename": "task.wellplates_section.html",
  "hash": "#custom.wellplate",
  "id": 432,
},{
  "title": "Features of the Grain Size Analysis Module",
  "ancestors": " (Metalo - Grain Size Analysis &gt; Introduction)",
  "filename": "p2c19s1.html",
  "hash": "#p2c19s1s1",
  "id": 433,
},{
  "title": "Supported Industrial Standards",
  "ancestors": " (Metalo - Grain Size Analysis &gt; Introduction)",
  "filename": "p2c19s1.html",
  "hash": "#gs.standards",
  "id": 434,
},{
  "title": "Methods",
  "ancestors": " (Metalo - Grain Size Analysis &gt; Introduction)",
  "filename": "p2c19s1.html",
  "hash": "#p2c19s1s3",
  "id": 435,
},{
  "title": "Measurement Modes",
  "ancestors": " (Metalo - Grain Size Analysis &gt; Introduction)",
  "filename": "p2c19s1.html",
  "hash": "#p2c19s1s4",
  "id": 436,
},{
  "title": "Copyright Information",
  "ancestors": " (Metalo - Grain Size Analysis &gt; Introduction)",
  "filename": "p2c19s1.html",
  "hash": "#p2c19s1s5",
  "id": 437,
},{
  "title": "Advanced Detection",
  "ancestors": " (Metalo - Grain Size Analysis &gt; Reference &gt; Detection Panel)",
  "filename": "detection.panel.ref.html",
  "hash": "#advanced.detection",
  "id": 438,
},{
  "title": "Sequential Measurement",
  "ancestors": " (Metalo - Grain Size Analysis &gt; Reference &gt; Sequential Measurement)",
  "filename": "gs.quick.sequential.html",
  "hash": "#CR_ME_GSStartMeasureSequence",
  "id": 439,
},{
  "title": "Batch Analysis",
  "ancestors": " (N-STORM &gt; Control Windows &gt; Reference)",
  "filename": "molecule.reference.html",
  "hash": "#molecule.reference.batch.analysis",
  "id": 440,
},{
  "title": "Typical Workflow",
  "ancestors": " (N-STORM &gt; Control Windows &gt; Reference &gt; Batch Analysis)",
  "filename": "molecule.reference.html",
  "hash": "#p2c20s2s3s1s1",
  "id": 441,
},{
  "title": "Create raster document from molecules",
  "ancestors": " (N-STORM &gt; Control Windows &gt; Reference)",
  "filename": "molecule.reference.html",
  "hash": "#molecule.reference.create.raster",
  "id": 442,
},{
  "title": "Determining Min and Max Height",
  "ancestors": " (N-STORM &gt; Control Windows &gt; Reference)",
  "filename": "molecule.reference.html",
  "hash": "#molecule.reference.determining.min.max.height",
  "id": 443,
},{
  "title": "Identification Settings",
  "ancestors": " (N-STORM &gt; Control Windows &gt; Reference)",
  "filename": "molecule.reference.html",
  "hash": "#molecule.reference.identification.settings",
  "id": 444,
},{
  "title": "Molecule Drift Correction",
  "ancestors": " (N-STORM &gt; Control Windows &gt; Reference)",
  "filename": "molecule.reference.html",
  "hash": "#molecule.reference.drift.correction",
  "id": 445,
},{
  "title": "Molecule List Text File Format",
  "ancestors": " (N-STORM &gt; Control Windows &gt; Reference)",
  "filename": "molecule.reference.html",
  "hash": "#molecule.reference.molecule.list.text.file",
  "id": 446,
},{
  "title": "ROI Statistics",
  "ancestors": " (N-STORM &gt; Control Windows &gt; Reference)",
  "filename": "molecule.reference.html",
  "hash": "#molecule.reference.roi.statistics",
  "id": 447,
},{
  "title": "XY Warp",
  "ancestors": " (N-STORM &gt; Control Windows &gt; Reference)",
  "filename": "molecule.reference.html",
  "hash": "#molecule.reference.xy.warp",
  "id": 448,
},{
  "title": "Z-Calibration",
  "ancestors": " (N-STORM &gt; Control Windows &gt; Reference)",
  "filename": "molecule.reference.html",
  "hash": "#molecule.reference.zcalibration",
  "id": 449,
},{
  "title": "Z-Stack Alignment",
  "ancestors": " (N-STORM &gt; Control Windows &gt; Reference)",
  "filename": "molecule.reference.html",
  "hash": "#molecule.reference.zstack.alignment",
  "id": 450,
},{
  "title": "Z-Stacked STORM",
  "ancestors": " (N-STORM &gt; Control Windows &gt; Reference)",
  "filename": "molecule.reference.html",
  "hash": "#molecule.reference.layers",
  "id": 451,
},{
  "title": "Overview",
  "ancestors": " (N-STORM &gt; Control Windows &gt; Reference &gt; Z-Stacked STORM)",
  "filename": "molecule.reference.html",
  "hash": "#p2c20s2s3s11s1",
  "id": 452,
},{
  "title": "File naming convention",
  "ancestors": " (N-STORM &gt; Control Windows &gt; Reference &gt; Z-Stacked STORM)",
  "filename": "molecule.reference.html",
  "hash": "#p2c20s2s3s11s2",
  "id": 453,
},{
  "title": "Z stack STORM analysis",
  "ancestors": " (N-STORM &gt; Control Windows &gt; Reference &gt; Z-Stacked STORM)",
  "filename": "molecule.reference.html",
  "hash": "#p2c20s2s3s11s3",
  "id": 454,
},{
  "title": "Working with Z stack STORM molecule data",
  "ancestors": " (N-STORM &gt; Control Windows &gt; Reference &gt; Z-Stacked STORM)",
  "filename": "molecule.reference.html",
  "hash": "#p2c20s2s3s11s4",
  "id": 455,
},{
  "title": "Managing N-STORM Z Stack (Layers table)",
  "ancestors": " (N-STORM &gt; Control Windows &gt; Reference &gt; Z-Stacked STORM)",
  "filename": "molecule.reference.html",
  "hash": "#p2c20s2s3s11s5",
  "id": 456,
},{
  "title": "Frequently Asked Questions",
  "ancestors": " (N-STORM &gt; Control Windows &gt; Reference)",
  "filename": "molecule.reference.html",
  "hash": "#molecule.reference.faq",
  "id": 457,
},{
  "title": "Overview",
  "ancestors": " (N-STORM &gt; N-STORM Z-Calibration (Agilent and LU4))",
  "filename": "nstorm.zcalibration.html",
  "hash": "#p2c20s3s1",
  "id": 458,
},{
  "title": "Acquisition of Z-Calibrations with Agilent laser box",
  "ancestors": " (N-STORM &gt; N-STORM Z-Calibration (Agilent and LU4))",
  "filename": "nstorm.zcalibration.html",
  "hash": "#p2c20s3s2",
  "id": 459,
},{
  "title": "Acquisition of Z-Calibrations with LU4 laser box",
  "ancestors": " (N-STORM &gt; N-STORM Z-Calibration (Agilent and LU4))",
  "filename": "nstorm.zcalibration.html",
  "hash": "#p2c20s3s3",
  "id": 460,
},{
  "title": "3D Calibration",
  "ancestors": " (N-STORM &gt; N-STORM Z-Calibration (Agilent and LU4))",
  "filename": "nstorm.zcalibration.html",
  "hash": "#p2c20s3s4",
  "id": 461,
},{
  "title": "Generating a 2D Warp",
  "ancestors": " (N-STORM &gt; N-STORM Z-Calibration (Agilent and LU4))",
  "filename": "nstorm.zcalibration.html",
  "hash": "#p2c20s3s5",
  "id": 462,
},{
  "title": "Error messages during 3D calibrations",
  "ancestors": " (N-STORM &gt; N-STORM Z-Calibration (Agilent and LU4))",
  "filename": "nstorm.zcalibration.html",
  "hash": "#p2c20s3s6",
  "id": 463,
},{
  "title": "N-STORM calibration commonly asked questions",
  "ancestors": " (N-STORM &gt; N-STORM Z-Calibration (Agilent and LU4))",
  "filename": "nstorm.zcalibration.html",
  "hash": "#p2c20s3s7",
  "id": 464,
},{
  "title": "Master Camera Wiring",
  "ancestors": " (Triggered Acquisition &gt; Wiring)",
  "filename": "p2c21s3.html",
  "hash": "#p2c21s3s1",
  "id": 465,
},{
  "title": "Wavelength Switcher Wiring",
  "ancestors": " (Triggered Acquisition &gt; Wiring)",
  "filename": "p2c21s3.html",
  "hash": "#p2c21s3s2",
  "id": 466,
},{
  "title": "DG4 Shutter HW box",
  "ancestors": " (Triggered Acquisition &gt; Wiring &gt; Wavelength Switcher Wiring)",
  "filename": "p2c21s3.html",
  "hash": "#cable.hwbox",
  "id": 467,
},{
  "title": "CoolLED pE-x break-out box",
  "ancestors": " (Triggered Acquisition &gt; Wiring &gt; Wavelength Switcher Wiring)",
  "filename": "p2c21s3.html",
  "hash": "#cable.coolled",
  "id": 468,
},{
  "title": "Piezo Z Wiring",
  "ancestors": " (Triggered Acquisition &gt; Wiring)",
  "filename": "p2c21s3.html",
  "hash": "#p2c21s3s3",
  "id": 469,
},{
  "title": "68pin-2BNC Real Time Acquisition Cable",
  "ancestors": " (Triggered Acquisition &gt; Wiring &gt; Piezo Z Wiring)",
  "filename": "p2c21s3.html",
  "hash": "#cable.piezo",
  "id": 470,
},{
  "title": "Wiring Overview",
  "ancestors": " (Triggered Acquisition &gt; Wiring)",
  "filename": "p2c21s3.html",
  "hash": "#triggering.wire.overview",
  "id": 471,
},{
  "title": "Wiring Examples",
  "ancestors": " (Triggered Acquisition &gt; Wiring)",
  "filename": "p2c21s3.html",
  "hash": "#p2c21s3s5",
  "id": 472,
},{
  "title": "Installation of a DAQ board",
  "ancestors": " (Triggered Acquisition &gt; Settings in NIS-Elements)",
  "filename": "p2c21s4.html",
  "hash": "#p2c21s4s1",
  "id": 473,
},{
  "title": "Camera Settings",
  "ancestors": " (Triggered Acquisition &gt; Settings in NIS-Elements)",
  "filename": "p2c21s4.html",
  "hash": "#p2c21s4s2",
  "id": 474,
},{
  "title": "Device Manager Settings",
  "ancestors": " (Triggered Acquisition &gt; Settings in NIS-Elements)",
  "filename": "p2c21s4.html",
  "hash": "#triggering.device.manager",
  "id": 475,
},{
  "title": "Acquisition Settings",
  "ancestors": " (Triggered Acquisition &gt; Settings in NIS-Elements)",
  "filename": "p2c21s4.html",
  "hash": "#howto.triggeredacquisition.settings",
  "id": 476,
},{
  "title": "Advanced Settings...",
  "ancestors": " (Triggered Acquisition &gt; Settings in NIS-Elements &gt; Acquisition Settings)",
  "filename": "p2c21s4.html",
  "hash": "#triggering.setup.advanced",
  "id": 477,
},{
  "title": "Creating Optical Configurations for Multi-channel Triggered Acquisition",
  "ancestors": " (Triggered Acquisition &gt; Settings in NIS-Elements)",
  "filename": "p2c21s4.html",
  "hash": "#optical.conf.for.triggering",
  "id": 478,
},{
  "title": "Using speed up with NIDAQ board",
  "ancestors": " (Triggered Acquisition &gt; Settings in NIS-Elements)",
  "filename": "p2c21s4.html",
  "hash": "#nidaq.tirecipe",
  "id": 479,
},{
  "title": "Standard Deviation Multiplication Factor",
  "ancestors": " (Tracking &gt; Tracking Options)",
  "filename": "tracking.options.html",
  "hash": "#standard.deviation.multi.factor",
  "id": 480,
},{
  "title": "Frame-to-frame object linking",
  "ancestors": " (Tracking &gt; Algorithm Overview)",
  "filename": "p2c24s9.html",
  "hash": "#p2c24s9s1",
  "id": 481,
},{
  "title": "Object Features",
  "ancestors": " (Tracking &gt; Algorithm Overview)",
  "filename": "p2c24s9.html",
  "hash": "#p2c24s9s2",
  "id": 482,
},{
  "title": "Track Processing",
  "ancestors": " (Tracking &gt; Algorithm Overview)",
  "filename": "p2c24s9.html",
  "hash": "#p2c24s9s3",
  "id": 483,
},{
  "title": "Track Segments",
  "ancestors": " (Tracking &gt; Tracking Output Calculation)",
  "filename": "tracking.output.calculation.html",
  "hash": "#tracking.output.track.segments",
  "id": 484,
},{
  "title": "Acceleration",
  "ancestors": " (Tracking &gt; Tracking Output Calculation)",
  "filename": "tracking.output.calculation.html",
  "hash": "#tracking.output.acceleration",
  "id": 485,
},{
  "title": "Heading and Elevation",
  "ancestors": " (Tracking &gt; Tracking Output Calculation)",
  "filename": "tracking.output.calculation.html",
  "hash": "#tracking.output.heading.elevation",
  "id": 486,
},{
  "title": "Line Length and Speed",
  "ancestors": " (Tracking &gt; Tracking Output Calculation)",
  "filename": "tracking.output.calculation.html",
  "hash": "#tracking.output.line.speed",
  "id": 487,
},{
  "title": "Reference Line Length",
  "ancestors": " (Tracking &gt; Tracking Output Calculation)",
  "filename": "tracking.output.calculation.html",
  "hash": "#tracking.output.reference.line.length",
  "id": 488,
},{
  "title": "Tracking options",
  "ancestors": " (Tracking &gt; Tracking - Advanced)",
  "filename": "tracking.advanced.html",
  "hash": "#p2c24s11s1",
  "id": 489,
},{
  "title": "Allow New Tracks After First Frame",
  "ancestors": " (Tracking &gt; Tracking - Advanced &gt; Tracking options)",
  "filename": "tracking.advanced.html",
  "hash": "#new.tracks.after.ff",
  "id": 490,
},{
  "title": "Track Properties",
  "ancestors": " (Tracking &gt; Tracking - Advanced &gt; Tracking options)",
  "filename": "tracking.advanced.html",
  "hash": "#mod.tracking.options.display",
  "id": 491,
},{
  "title": "Track processing",
  "ancestors": " (Tracking &gt; Tracking - Advanced)",
  "filename": "tracking.advanced.html",
  "hash": "#mod.tracking.options.processing",
  "id": 492,
},{
  "title": "Graph, Data and Tracks tabs",
  "ancestors": " (Tracking &gt; Tracking - Advanced)",
  "filename": "tracking.advanced.html",
  "hash": "#mod.tracking.options.graph",
  "id": 493,
},{
  "title": "NIS-Elements configuration shall include:",
  "ancestors": " (Tracking &gt; Intra-Nuclear Single Particle Tracking (I-SPT) &gt; NIS-Elements configuration)",
  "filename": "p2c24s16s2.html",
  "hash": "#p2c24s16s2s1",
  "id": 494,
},{
  "title": "Images acquisition",
  "ancestors": " (Tracking &gt; Intra-Nuclear Single Particle Tracking (I-SPT) &gt; NIS-Elements configuration)",
  "filename": "p2c24s16s2.html",
  "hash": "#p2c24s16s2s2",
  "id": 495,
},{
  "title": "Illumination sequence",
  "ancestors": " (Tracking &gt; Intra-Nuclear Single Particle Tracking (I-SPT) &gt; NIS-Elements configuration)",
  "filename": "p2c24s16s2.html",
  "hash": "#p2c24s16s2s3",
  "id": 496,
},{
  "title": "Single file outputs",
  "ancestors": " (Tracking &gt; Intra-Nuclear Single Particle Tracking (I-SPT) &gt; I-SPT Outputs)",
  "filename": "p2c24s16s7.html",
  "hash": "#p2c24s16s7s1",
  "id": 497,
},{
  "title": "Batch",
  "ancestors": " (Tracking &gt; Intra-Nuclear Single Particle Tracking (I-SPT) &gt; I-SPT Outputs)",
  "filename": "p2c24s16s7.html",
  "hash": "#p2c24s16s7s2",
  "id": 498,
},{
  "title": "Mean square displacement (MSD)",
  "ancestors": " (Tracking &gt; Intra-Nuclear Single Particle Tracking (I-SPT) &gt; I-SPT Outputs)",
  "filename": "p2c24s16s7.html",
  "hash": "#ispt.msd",
  "id": 499,
},{
  "title": "Advanced detection",
  "ancestors": " (Tracking &gt; Intra-Nuclear Single Particle Tracking (I-SPT) &gt; Advanced I-SPT)",
  "filename": "ispt.advanced.html",
  "hash": "#p2c24s16s8s1",
  "id": 500,
},{
  "title": "Miss-assignments",
  "ancestors": " (Tracking &gt; Intra-Nuclear Single Particle Tracking (I-SPT) &gt; Advanced tracking)",
  "filename": "p2c24s16s9.html",
  "hash": "#p2c24s16s9s1",
  "id": 501,
},{
  "title": "Miss-assignment and tracking radius",
  "ancestors": " (Tracking &gt; Intra-Nuclear Single Particle Tracking (I-SPT) &gt; Advanced tracking)",
  "filename": "p2c24s16s9.html",
  "hash": "#p2c24s16s9s2",
  "id": 502,
},{
  "title": "Estimating miss-assignments with I-SPT",
  "ancestors": " (Tracking &gt; Intra-Nuclear Single Particle Tracking (I-SPT) &gt; Advanced tracking)",
  "filename": "p2c24s16s9.html",
  "hash": "#p2c24s16s9s3",
  "id": 503,
},{
  "title": "MSD",
  "ancestors": " (Tracking &gt; Intra-Nuclear Single Particle Tracking (I-SPT) &gt; Statistics)",
  "filename": "p2c24s16s10.html",
  "hash": "#p2c24s16s10s1",
  "id": 504,
},{
  "title": "Step-translocation histograms",
  "ancestors": " (Tracking &gt; Intra-Nuclear Single Particle Tracking (I-SPT) &gt; Statistics)",
  "filename": "p2c24s16s10.html",
  "hash": "#p2c24s16s10s2",
  "id": 505,
},{
  "title": "Correlation statistics",
  "ancestors": " (Tracking &gt; Intra-Nuclear Single Particle Tracking (I-SPT) &gt; Statistics)",
  "filename": "p2c24s16s10.html",
  "hash": "#p2c24s16s10s3",
  "id": 506,
},{
  "title": "Basic Workflow",
  "ancestors": " (Weld Measurement)",
  "filename": "app.welds.html",
  "hash": "#weld.basic",
  "id": 507,
},

// refData #3 for the list of all titles: display title and ancestors
// titles of separate pages are not added, because they are already there from the previous step (remap.hash == "")
]

// calls lunr and returns searched id
$(document).ready(function() {

  $('input#search').attr("placeholder", "Fulltext search");
  $('input#search').on('keyup', function () {
   var resultdivmods = $('#results_mods');
   
   // Get query, WQ = with quotes
   
      // let the user decide what to search
      var inputvalWQ = $(this).val();
   

   // hledaní­ v uvozovkách
   const re = new RegExp(/\".*\"/g);
   let reg = re.exec(inputvalWQ);
   
   // začíná fráze uvozovkama?
   let openQuotes = inputvalWQ.slice(0,1) === '"' ? true : false;
   // Když jsou uvozovky pouze na začátku, LUNR hledá  pořád všechno po slovech bez uvozovek

   var inputval = inputvalWQ.replaceAll('"','');
   //vždy hledej exact i lunr inputval = hledejCely ? celyVyraz : inputval;
   let celyVyraz = inputval;
   let filteredSourceDatamods = sourceDatamods.filter((v) => {return (v.title.toLowerCase().indexOf(celyVyraz.toLowerCase()) > -1 || v.content.toLowerCase().indexOf(celyVyraz.toLowerCase()) > -1 || v.xmlid.toLowerCase().indexOf(celyVyraz.toLowerCase()) > -1)});

   //search with logical AND: when whitespace is input, all words preceding the whitespace are marked as compulsory (logical AND)
   // split to words by spaces and remove "+" which has special meaning for lunr and trailing \s
   var inputarray = inputval.replace(/\+|\s*$/,"").split(/\s+/); 
   // removes the last member and adds it to $lastword
   var lastword = inputarray.pop();

   // prepend "+" to all but stopWords
   var finishedwords = [];
   $.each(inputarray,function(indexmods, element) { // F*****G IE does not support: inputarray.forEach(( element ) => {
      //finishedwords.push(stopWords.includes(element.toLowerCase()) ? element : "+" + element)
      finishedwords.push((typeof(lunr.stopWordFilter(element.toLowerCase())) != 'undefined' ) ?  "+" + element : element)
   });
   // add space before the last word, add + if finished by space (compulsory word)
   var appendlast = (inputarray.length > 0) ? ( inputval.slice(-1) === " " && (typeof(lunr.stopWordFilter(lastword.toLowerCase())) != 'undefined'  )  ? " +" : " ") : ""; 
   
   // make fuzzy search until the word is finished by space
   var lastchar = (lastword.length > 0 && inputval.slice(-1) != " " ) ? "*" : "";
   
   // single word search, search all possible combinations including function names
   var altword = ( inputarray.length === 0 && inputval.length > 2 ) ?  ( lastword.charAt(0) === '_' ? ' ' + lastword.substr(1) + lastchar : ' _' + lastword + lastchar ) : '' ;
   // "wholeword*" searches for nothing so, one must search for "wholeword" at the same time
   var wholeword = ( inputval.slice(-1) === " " || inputval.length < 3 ) ? '' : ' ' + lastword ;

   // concatenate the query
   var query = finishedwords.join(" ") + appendlast + lastword + lastchar + altword + wholeword;
   //console.log(query);
   
    // Search for it
    if (inputval.length >= 2) {
      var resultmods = [];
      
         //exclude lunr search results for the "gui" exact search 
         // and exclude it also if the user starts with '"'
         if (openQuotes == false) {
            resultmods = indexmods.search(query);
         }
      
        //append exact search results to resultmods
        filteredSourceDatamods.forEach((element) => {
            resultmods.push({
               ref: element.id.toString(),
               score: 0.0
             });
         });
        
         //Make the array unique by (ref), but use the member with the higher score
         const resultmodsUnique = Array.from(
             resultmods.reduce((map, item) => {
                 if (!map.has(item.ref) || map.get(item.ref).score < item.score) {
                     map.set(item.ref, item);
                 }
                 return map;
             }, new Map()).values()
         );
        
        //number of results found
        let foundResults = resultmodsUnique.length;
        // Show results
        resultdivmods.empty();
        // Add status
       if ( foundResults > 0) {
          resultdivmods.prepend('<p class="found">Applications: '+foundResults+' results</p>');
       }
        $("#results_mods").addClass("results");
        // Loop through, match, and add results
        for (var item in resultmodsUnique) {
            var ref = resultmodsUnique[item].ref;
            var score = resultmodsUnique[item].score;
            var opacity = (score / 10).toFixed(2);
            var searchitem = '<a class="result" target="fastFrame" onclick="var lastClickPath = \''+storemods[ref].filename+'?mark='+encodeURI(inputval)+storemods[ref].hash+'\';"  href="'+storemods[ref].filename+'?mark='+encodeURI(inputval)+storemods[ref].hash+'"><span class="1"><em>'+storemods[ref].title+'</em><div class="score" style="opacity:'+opacity+';"></div></span><span>'+storemods[ref].ancestors+'</span></a>';

            resultdivmods.append(searchitem);
         }
        //todo pmit zrusit hilitor2 pri stisku tlacitka closeSearch
        var hil = new Hilitor2();
        hil.apply(inputval);
    } else {
        $("#results_mods").removeClass("results");
        resultdivmods.empty();
    }
  });
});
