"""
Quantify signal spread and decay from distance heatmap TSV files.

This module performs quantification analysis on pre-computed distance heatmap TSV files,
measuring signal spread (distance) and decay (time) using region growing segmentation.

Input: TSV files generated by quantify_distance_heatmap_v2.py
Output: Segmentation metrics and visualizations

Author: BIPHUB - Bioimage Informatics Hub, University of Oslo
License: MIT
"""
import argparse
import logging
import os
import sys
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.colors import Normalize
from pathlib import Path
from typing import Tuple, Optional, List, Dict
from scipy.ndimage import gaussian_filter
from scipy.interpolate import interp1d
from scipy.optimize import curve_fit
from skimage.measure import label
from scipy.ndimage import binary_fill_holes
from collections import deque
import glob
from multiprocessing import Pool, cpu_count
from functools import partial
from tqdm import tqdm

# Local imports
try:
    from .. import bioimage_pipeline_utils as rp
except ImportError:
    # Fallback for when script is run directly (not as module)
    parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    sys.path.insert(0, parent_dir)
    import bioimage_pipeline_utils as rp


def get_experiment_info_from_filename(filename: str, split: str = "__", colname: list[str] = ["Group"]) -> Dict[str, str]:
    """
    Extract experimental metadata from filename using split character.
    
    Args:
        filename: Input filename (can be full path or just filename)
        split: Character(s) to split filename on (default: "__")
        colname: List of column names for the split parts (default: ["Group"])
    
    Returns:
        Dictionary with extracted metadata
    
    Examples:
        'experiment__WT__replicate1.tif' with split="__" and colname=["Exp", "Group", "Rep"]
        -> {'Exp': 'experiment', 'Group': 'WT', 'Rep': 'replicate1'}
    """
    # Get just the filename without path and extension
    base = Path(filename).stem
    
    # Split by the specified character(s)
    parts = base.split(split)
    
    # Create dictionary with column names
    result = {}
    for i, name in enumerate(colname):
        if i < len(parts):
            result[name] = parts[i]
        else:
            result[name] = ""  # Empty string if not enough parts
    
    return result


def quantify_signal_spread_and_decay(
    tsv_path: str,
    output_path: str = None,
    colormap: str = 'viridis',
    smooth_sigma_distance: float = 1.5,
    force_show: bool = False,
    mask_alpha: float = 0.5,
    region_grow_threshold: float = 0.5,
    split_char: str = "__",
    metadata_columns: List[str] = ["Group"],
    quiet: bool = False
) -> Dict:
    """
    Quantify signal spread (distance) and decay (time) from TSV heatmap using segmentation.
    
    This function:
    1. Loads pre-computed heatmap data from TSV file
    2. Uses 'Sum_50_Random' column as heatmap values
    3. Smooths heatmap in DISTANCE DIMENSION ONLY
    4. Applies region growing segmentation from peak signal
    5. Measures center of mass, distance spread, and signal area per timepoint
    6. Plots signal trajectory and spread bounds over time
    7. Returns quantitative metrics
    8. Saves raw heatmap as TIF and metrics as TSV
    
    Args:
        tsv_path: Path to the TSV file with heatmap data (from v2)
        output_path: Optional path to save plots. If None, displays interactively.
        colormap: Matplotlib colormap name (default: 'viridis')
        smooth_sigma_distance: Gaussian smoothing sigma in distance dimension (default: 1.5)
        force_show: Display plots even when saving (default: False)
        mask_alpha: Alpha transparency for mask overlay (default: 0.5)
        region_grow_threshold: Fraction of running maximum for region growing (default: 0.5)
        split_char: Character(s) to split filename for metadata (default: "__")
        metadata_columns: List of column names for metadata extraction (default: ["Group"])
        quiet: Suppress info logging (default: False, used for parallel processing)
    
    Returns:
        Dictionary containing quantification metrics
    """
    # Helper for conditional logging
    def log_info(msg):
        if not quiet:
            logging.info(msg)
    
    log_info(f"Quantifying signal from TSV: {Path(tsv_path).name}")
    
    # Load TSV file (long format from quantify_per_maskindex.py)
    df = pd.read_csv(tsv_path, sep='\t')
    
    log_info(f"Loaded TSV with {len(df)} rows and columns: {df.columns.tolist()}")
    
    # Validate required columns
    if 'Mask_Index' not in df.columns:
        raise ValueError(f"'Mask_Index' column not found in {tsv_path}. Available columns: {df.columns.tolist()}")
    if 'Timepoint' not in df.columns:
        raise ValueError(f"'Timepoint' column not found in {tsv_path}. Available columns: {df.columns.tolist()}")
    
    # Find the intensity column (look for Sum_N_Random pattern)
    sum_cols = [col for col in df.columns if col.startswith('Sum_') and '_Random' in col]
    
    if len(sum_cols) == 0:
        raise ValueError(f"No Sum_*_Random column found in {tsv_path}. Available columns: {df.columns.tolist()}")
    
    intensity_col = sum_cols[0]  # Use the first matching column
    log_info(f"Using intensity column: {intensity_col}")
    
    # Extract base filename from TSV filename
    base_filename = Path(tsv_path).stem
    log_info(f"Base filename: {base_filename}")
    
    # Extract experimental metadata from filename
    experiment_info = get_experiment_info_from_filename(base_filename, split=split_char, colname=metadata_columns)
    log_info(f"Experimental metadata: {experiment_info}")
    
    # Pivot the data to create heatmap matrix
    # Rows = Mask_Index (distance bins), Columns = Timepoint
    pivot_df = df.pivot(index='Mask_Index', columns='Timepoint', values=intensity_col)
    
    # Get distance bins and timepoints
    distance_bins = pivot_df.index.values.astype(float)
    timepoints_all = pivot_df.columns.values
    
    num_distance_bins = len(distance_bins)
    T = len(timepoints_all)
    
    log_info(f"Loaded heatmap: {num_distance_bins} distance bins × {T} timepoints")
    log_info(f"Distance range: {distance_bins[0]:.2f} to {distance_bins[-1]:.2f}")
    
    # Convert to numpy array
    heatmap_data = pivot_df.values  # shape: (num_distance_bins, num_timepoints)
    
    # Skip T=0 for quantification - only analyze T=1 onwards
    if T > 1:
        log_info(f"Skipping T=0, analyzing T=1 to T={T-1}")
        heatmap_data = heatmap_data[:, 1:]  # Remove first timepoint
        T_quantify = T - 1
    else:
        logging.warning("Only one timepoint found - cannot skip T=0")
        T_quantify = T
    
    # Save raw heatmap as TIF (before smoothing)
    if output_path:
        raw_heatmap_path = output_path.replace('.png', '_raw_heatmap.tif')
        
        # Convert to uint16 for saving (scale to full range)
        heatmap_min = np.min(heatmap_data)
        heatmap_max = np.max(heatmap_data)
        
        if heatmap_max > heatmap_min:
            heatmap_scaled = ((heatmap_data - heatmap_min) / (heatmap_max - heatmap_min) * 65535).astype(np.uint16)
        else:
            heatmap_scaled = np.zeros_like(heatmap_data, dtype=np.uint16)
        
        # Reshape to TCZYX format (single timepoint, single channel, single Z, Y=distance, X=time)
        # For visualization: treat heatmap as an image where Y=distance bins, X=timepoints
        heatmap_img = heatmap_scaled.T[np.newaxis, np.newaxis, np.newaxis, :, :]  # (1, 1, 1, T_quantify, num_distance_bins)
        
        rp.save_tczyx_image(heatmap_img, raw_heatmap_path)
        log_info(f"Saved raw heatmap as TIF: {raw_heatmap_path}")
    
    # === SMOOTH THE HEATMAP IN DISTANCE DIMENSION ONLY ===
    heatmap_smoothed = gaussian_filter(heatmap_data, sigma=[smooth_sigma_distance, 0])
    log_info(f"Applied Gaussian smoothing in distance dimension (σ_distance={smooth_sigma_distance}, σ_time=0)")
    
    # === SEGMENTATION USING REGION GROWING ===
    from skimage.measure import regionprops
    
    log_info("Using region growing segmentation from peak signal")
    log_info(f"Heatmap shape: {heatmap_smoothed.shape} (distance × time)")
    log_info(f"Heatmap range: [{np.min(heatmap_smoothed):.2f}, {np.max(heatmap_smoothed):.2f}]")
    
    # Find the starting point: maximum intensity
    max_idx = np.unravel_index(np.argmax(heatmap_smoothed), heatmap_smoothed.shape)
    seed_distance_idx, seed_time_idx = max_idx
    seed_value = heatmap_smoothed[seed_distance_idx, seed_time_idx]
    
    log_info(f"Seed point: distance_idx={seed_distance_idx} (distance={distance_bins[seed_distance_idx]:.1f}), "
                f"time_idx={seed_time_idx} (T={seed_time_idx+1}), intensity={seed_value:.2f}")
    
    # Region growing with adaptive threshold
    mask = np.zeros_like(heatmap_smoothed, dtype=bool)
    visited = np.zeros_like(heatmap_smoothed, dtype=bool)
    
    queue = deque()
    queue.append((seed_distance_idx, seed_time_idx))
    visited[seed_distance_idx, seed_time_idx] = True
    mask[seed_distance_idx, seed_time_idx] = True
    
    current_max = seed_value
    log_info(f"Starting threshold: {current_max * region_grow_threshold:.2f} ({region_grow_threshold*100:.0f}% of seed)")
    
    n_distance, n_time = heatmap_smoothed.shape
    neighbors_offsets = [(-1, 0), (1, 0), (0, -1), (0, 1)]  # 4-connectivity
    
    pixels_added = 0
    max_iterations = n_distance * n_time
    iteration = 0
    
    while queue and iteration < max_iterations:
        iteration += 1
        d_idx, t_idx = queue.popleft()
        
        for d_offset, t_offset in neighbors_offsets:
            nd_idx = d_idx + d_offset
            nt_idx = t_idx + t_offset
            
            if 0 <= nd_idx < n_distance and 0 <= nt_idx < n_time:
                if not visited[nd_idx, nt_idx]:
                    visited[nd_idx, nt_idx] = True
                    
                    neighbor_val = heatmap_smoothed[nd_idx, nt_idx]
                    
                    # Update running maximum if we found a brighter pixel
                    if neighbor_val > current_max:
                        current_max = neighbor_val
                    
                    # Include if above threshold (% of running max)
                    if neighbor_val >= current_max * region_grow_threshold:
                        mask[nd_idx, nt_idx] = True
                        queue.append((nd_idx, nt_idx))
                        pixels_added += 1
    
    log_info(f"Region growing complete: {np.sum(mask)} pixels (seed + {pixels_added})")
    log_info(f"Final running maximum: {current_max:.2f}")
    
    # === CONNECTIVITY FILTERING ===
    log_info("Applying connected component filtering...")
    labeled_mask = label(mask, connectivity=1)
    seed_label = labeled_mask[seed_distance_idx, seed_time_idx]
    
    if seed_label == 0:
        logging.warning("Seed point has label 0 after labeling - mask may be empty")
        connected_mask = mask
    else:
        connected_mask = (labeled_mask == seed_label)
        removed_pixels = np.sum(mask) - np.sum(connected_mask)
        log_info(f"Removed {removed_pixels} disconnected pixels")
    
    mask = binary_fill_holes(connected_mask)
    final_mask = mask
    
    log_info(f"Final mask: {np.sum(final_mask)} pixels")
    
    # === CALCULATE QUANTIFICATION METRICS ===
    if np.sum(final_mask) == 0:
        logging.warning("No signal detected after segmentation - returning empty metrics")
        
        distance_peak = np.full(T_quantify, np.nan)
        distance_spread_lower = np.full(T_quantify, np.nan)
        distance_spread_upper = np.full(T_quantify, np.nan)
        distance_spread = np.full(T_quantify, np.nan)
        signal_area = np.zeros(T_quantify)
        temporal_intensity = np.zeros(T_quantify)
        
        signal_duration = 0
        time_to_50_percent = np.nan
        time_to_disappearance = np.nan
        decay_rate = np.nan
        intensity_T1 = 0
        intensity_T10 = np.nan
        intensity_T20 = np.nan
    else:
        # Initialize metrics arrays
        distance_peak = np.full(T_quantify, np.nan)
        distance_spread_lower = np.full(T_quantify, np.nan)
        distance_spread_upper = np.full(T_quantify, np.nan)
        distance_spread = np.full(T_quantify, np.nan)
        signal_area = np.zeros(T_quantify)
        temporal_intensity = np.zeros(T_quantify)
        
        # Calculate per-timepoint metrics
        for t in range(T_quantify):
            mask_t = final_mask[:, t]
            
            if np.sum(mask_t) > 0:
                signal_area[t] = np.sum(mask_t)
                
                # Get distance bins and intensities for this timepoint
                distances_t = distance_bins[mask_t]
                intensities_t = heatmap_smoothed[mask_t, t]
                
                # Total intensity
                temporal_intensity[t] = np.sum(intensities_t)
                
                # Center of mass (intensity-weighted)
                distance_peak[t] = np.sum(distances_t * intensities_t) / np.sum(intensities_t)
                
                # Spread bounds (min and max distance)
                distance_spread_lower[t] = np.min(distances_t)
                distance_spread_upper[t] = np.max(distances_t)
                distance_spread[t] = distance_spread_upper[t] - distance_spread_lower[t]
        
        # === TEMPORAL DECAY METRICS ===
        signal_present = signal_area > 0
        signal_duration = np.sum(signal_present)
        
        # Time to 50% intensity
        if len(temporal_intensity) > 0 and np.max(temporal_intensity) > 0:
            intensity_50 = temporal_intensity[0] * 0.5  # 50% of T1 (first timepoint after T0)
            idx_50 = np.where(temporal_intensity < intensity_50)[0]
            if len(idx_50) > 0:
                time_to_50_percent = idx_50[0] + 1  # +1 because we start from T1
            else:
                time_to_50_percent = T_quantify
        else:
            time_to_50_percent = np.nan
        
        # Time to disappearance
        if signal_duration > 0:
            last_signal_idx = np.where(signal_present)[0][-1]
            time_to_disappearance = last_signal_idx + 1  # +1 for T1 indexing
        else:
            time_to_disappearance = np.nan
        
        # Decay rate (linear fit of log(intensity))
        timepoints_arr = np.arange(1, T)
        
        if len(temporal_intensity) > 1 and np.sum(temporal_intensity > 0) > 1:
            valid_idx = temporal_intensity > 0
            if np.sum(valid_idx) > 1:
                log_intensity = np.log(temporal_intensity[valid_idx])
                time_vals = timepoints_arr[valid_idx]
                
                try:
                    coeffs = np.polyfit(time_vals, log_intensity, 1)
                    decay_rate = -coeffs[0]  # Negative of slope
                except:
                    decay_rate = np.nan
            else:
                decay_rate = np.nan
        else:
            decay_rate = np.nan
        
        # Intensity at specific timepoints
        intensity_T1 = temporal_intensity[0] if len(temporal_intensity) > 0 else 0
        intensity_T10 = temporal_intensity[9] if len(temporal_intensity) >= 10 else np.nan
        intensity_T20 = temporal_intensity[19] if len(temporal_intensity) >= 20 else np.nan
        
        log_info(f"Temporal decay metrics:")
        log_info(f"  Signal duration: {signal_duration} timepoints")
        log_info(f"  Time to 50% intensity: T={time_to_50_percent}")
        log_info(f"  Time to disappearance: T={time_to_disappearance}")
        log_info(f"  Decay rate: {decay_rate:.4f} (1/timepoint)")
        log_info(f"  Initial intensity (T=1): {intensity_T1:.2e}")
    
    # === CREATE VISUALIZATION ===
    fig = plt.figure(figsize=(18, 10))
    gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)
    
    # Subplot 1: Smoothed heatmap with segmentation mask overlay
    ax1 = fig.add_subplot(gs[0, :])
    
    df_smooth = pd.DataFrame(
        heatmap_smoothed,
        index=[f'{b:.1f}' for b in distance_bins],
        columns=[f'T{t+1}' for t in range(T_quantify)]
    )
    
    sns.heatmap(df_smooth, annot=False, cmap=colormap, ax=ax1, cbar_kws={'label': 'Intensity (smoothed)'})
    ax1.invert_yaxis()
    
    # Overlay segmentation mask
    mask_overlay = np.zeros((*final_mask.shape, 4))
    mask_overlay[final_mask] = [1, 1, 1, mask_alpha]
    
    ax1.imshow(mask_overlay, aspect='auto', extent=[0, T_quantify, 0, num_distance_bins],
              origin='upper', interpolation='nearest')
    
    # Overlay metrics (center and spread lines)
    distance_to_idx_func = lambda d: np.argmin(np.abs(distance_bins - d))
    valid_timepoints = [t for t in range(T_quantify) if not np.isnan(distance_peak[t])]
    
    if len(valid_timepoints) > 0:
        peak_idx = [distance_to_idx_func(distance_peak[t]) for t in valid_timepoints]
        lower_idx = [distance_to_idx_func(distance_spread_lower[t]) for t in valid_timepoints]
        upper_idx = [distance_to_idx_func(distance_spread_upper[t]) for t in valid_timepoints]
        
        valid_t_offset = [t + 0.5 for t in valid_timepoints]
        peak_idx_offset = [idx + 0.5 for idx in peak_idx]
        lower_idx_offset = [idx + 0.5 for idx in lower_idx]
        upper_idx_offset = [idx + 0.5 for idx in upper_idx]
        
        ax1.plot(valid_t_offset, peak_idx_offset, 'r-', linewidth=2, label='Signal Center')
        ax1.plot(valid_t_offset, lower_idx_offset, 'b--', linewidth=1.5, label='Lower bound')
        ax1.plot(valid_t_offset, upper_idx_offset, 'g--', linewidth=1.5, label='Upper bound')
        ax1.legend(loc='upper right', fontsize=9)
    
    ax1.set_title(f'Smoothed Heatmap with Segmentation (α={mask_alpha}, σ={smooth_sigma_distance})',
                 fontweight='bold', fontsize=14)
    ax1.set_xlabel('Timepoint', fontweight='bold')
    ax1.set_ylabel('Distance', fontweight='bold')
    
    # Subplot 2: Peak position and spread bounds
    ax2 = fig.add_subplot(gs[1, :])
    timepoints_arr = np.arange(1, T)
    
    ax2.fill_between(timepoints_arr, distance_spread_lower, distance_spread_upper,
                     alpha=0.2, color='blue', label='Signal extent')
    ax2.plot(timepoints_arr, distance_peak, 'r-', linewidth=3,
            label='Signal Center (CoM)', marker='o', markersize=6)
    ax2.plot(timepoints_arr, distance_spread_lower, 'b--', linewidth=2, label='Lower bound')
    ax2.plot(timepoints_arr, distance_spread_upper, 'g--', linewidth=2, label='Upper bound')
    
    ax2.set_xlabel('Timepoint', fontweight='bold', fontsize=12)
    ax2.set_ylabel('Distance', fontweight='bold', fontsize=12)
    ax2.set_title('Signal Center and Spread Bounds Over Time (T0 excluded)', fontweight='bold', fontsize=14)
    ax2.legend(loc='best', fontsize=10)
    ax2.grid(True, alpha=0.3)
    
    # Subplot 3: Distance spread width
    ax3 = fig.add_subplot(gs[2, 0])
    ax3.plot(timepoints_arr, distance_spread, 'purple', linewidth=2, marker='s', markersize=5)
    ax3.set_xlabel('Timepoint', fontweight='bold', fontsize=12)
    ax3.set_ylabel('Distance Spread', fontweight='bold', fontsize=12)
    ax3.set_title('Signal Distance Spread Over Time (T0 excluded)', fontweight='bold', fontsize=13)
    ax3.grid(True, alpha=0.3)
    
    # Subplot 4: Temporal intensity and signal area
    ax4 = fig.add_subplot(gs[2, 1])
    ax4_twin = ax4.twinx()
    
    line1 = ax4.plot(timepoints_arr, temporal_intensity, 'darkblue', linewidth=2.5,
                     marker='o', markersize=6, label='Total Intensity')
    ax4.set_xlabel('Timepoint', fontweight='bold', fontsize=12)
    ax4.set_ylabel('Total Intensity', fontweight='bold', fontsize=12, color='darkblue')
    ax4.tick_params(axis='y', labelcolor='darkblue')
    
    line2 = ax4_twin.plot(timepoints_arr, signal_area, 'darkgreen', linewidth=2.5,
                          marker='s', markersize=6, label='Signal Area (bins)')
    ax4_twin.set_ylabel('Signal Area (# bins)', fontweight='bold', fontsize=12, color='darkgreen')
    ax4_twin.tick_params(axis='y', labelcolor='darkgreen')
    
    # Combine legends
    lines = line1 + line2
    labels = [l.get_label() for l in lines]
    ax4.legend(lines, labels, loc='upper right', fontsize=10)
    
    ax4.set_title('Temporal Intensity and Signal Area (T0 excluded)', fontweight='bold', fontsize=13)
    ax4.grid(True, alpha=0.3)
    
    # Add overall title with metadata
    metadata_str = ", ".join([f"{k}={v}" for k, v in experiment_info.items()])
    fig.suptitle(f'{base_filename}\n{metadata_str}', fontsize=16, fontweight='bold')
    
    # Save or show
    if output_path:
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        log_info(f"Saved visualization: {output_path}")
        
        if force_show:
            plt.show()
        else:
            plt.close()
    else:
        plt.show()
    
    # === SAVE METRICS AS TSV ===
    if output_path:
        metrics_path = output_path.replace('.png', '_metrics.tsv')
        
        # Create metrics dataframe
        metrics_df = pd.DataFrame({
            'Timepoint': timepoints_arr,
            'Distance_Peak': distance_peak,
            'Distance_Spread_Lower': distance_spread_lower,
            'Distance_Spread_Upper': distance_spread_upper,
            'Distance_Spread': distance_spread,
            'Signal_Area': signal_area,
            'Total_Intensity': temporal_intensity
        })
        
        # Add metadata columns at the beginning
        metrics_df.insert(0, 'Base_Filename', base_filename)
        for i, (key, value) in enumerate(experiment_info.items(), start=1):
            metrics_df.insert(i, key, value)
        
        # Add summary metrics as additional columns (same value repeated for all rows)
        metrics_df['Signal_Duration'] = signal_duration
        metrics_df['Time_to_50_Percent'] = time_to_50_percent
        metrics_df['Time_to_Disappearance'] = time_to_disappearance
        metrics_df['Decay_Rate'] = decay_rate
        metrics_df['Intensity_T1'] = intensity_T1
        metrics_df['Intensity_T10'] = intensity_T10
        metrics_df['Intensity_T20'] = intensity_T20
        metrics_df['Smooth_Sigma_Distance'] = smooth_sigma_distance
        metrics_df['Region_Grow_Threshold'] = region_grow_threshold
        
        # Save
        metrics_df.to_csv(metrics_path, sep='\t', index=False)
        log_info(f"Saved metrics: {metrics_path}")
    
    # Return metrics dictionary
    return {
        'distance_peak': distance_peak,
        'distance_spread_lower': distance_spread_lower,
        'distance_spread_upper': distance_spread_upper,
        'distance_spread': distance_spread,
        'signal_area': signal_area,
        'temporal_intensity': temporal_intensity,
        'heatmap_smoothed': heatmap_smoothed,
        'segmentation_mask': final_mask,
        'distance_bins': distance_bins,
        'timepoints': timepoints_arr,
        'smooth_sigma_distance': smooth_sigma_distance,
        'signal_duration': signal_duration,
        'time_to_50_percent': time_to_50_percent,
        'time_to_disappearance': time_to_disappearance,
        'decay_rate': decay_rate,
        'intensity_T1': intensity_T1,
        'intensity_T10': intensity_T10,
        'intensity_T20': intensity_T20,
        'experiment_info': experiment_info,
        'base_filename': base_filename
    }


def process_single_tsv(
    tsv_path: str,
    output_dir: str,
    args: argparse.Namespace,
    quiet: bool = False
) -> Tuple[str, bool, Optional[str]]:
    """
    Process a single TSV file and generate quantification.
    
    Args:
        tsv_path: Path to TSV file
        output_dir: Output directory for results
        args: Command line arguments
        quiet: Suppress logging output (for parallel processing)
    
    Returns:
        Tuple of (filename, success, error_message)
    """
    try:
        # Create output filename
        tsv_name = Path(tsv_path).stem
        output_path = os.path.join(output_dir, f"{tsv_name}_quantification.png")
        
        # Run quantification
        metrics = quantify_signal_spread_and_decay(
            tsv_path=tsv_path,
            output_path=output_path,
            colormap=args.colormap,
            smooth_sigma_distance=args.smooth_sigma_distance,
            force_show=args.force_show,
            mask_alpha=args.mask_alpha,
            region_grow_threshold=args.region_grow_threshold,
            split_char=args.split_char,
            metadata_columns=args.metadata_columns.split(',') if isinstance(args.metadata_columns, str) else args.metadata_columns,
            quiet=quiet
        )
        
        return (tsv_path, True, None)
        
    except KeyboardInterrupt:
        raise  # Re-raise to allow graceful shutdown
    except Exception as e:
        error_msg = f"Error processing {Path(tsv_path).name}: {str(e)}"
        # Don't log here - will be logged in main() summary
        return (tsv_path, False, error_msg)


def main():
    parser = argparse.ArgumentParser(
        description='Quantify signal spread and decay from distance heatmap TSV files (v3)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Example YAML config for run_pipeline.exe:
---
run:
- name: Quantify distance heatmaps from TSV (v3)
  environment: uv@3.11:quantify-distance-heatmap
  commands:
  - python
  - '%REPO%/standard_code/python/plots/quantify_distance_heatmap_v3.py'
  - --input-search-pattern: '%YAML%/output_heatmaps/**/*_heatmap_data.tsv'
  - --output-folder: '%YAML%/output_quantifications'
  - --colormap: viridis
  - --smooth-sigma-distance: 1.5
  - --region-grow-threshold: 0.5
  - --split-char: '__'
  - --metadata-columns: Group,Replicate

- name: Quantify with custom parameters
  environment: uv@3.11:quantify-distance-heatmap
  commands:
  - python
  - '%REPO%/standard_code/python/plots/quantify_distance_heatmap_v3.py'
  - --input-search-pattern: '%YAML%/output_heatmaps/**/*_heatmap_data.tsv'
  - --output-folder: '%YAML%/output_quantifications'
  - --smooth-sigma-distance: 2.0
  - --region-grow-threshold: 0.6
  - --parallel
"""
    )
    
    parser.add_argument('--input-search-pattern', type=str, required=True,
                       help='Glob pattern for input TSV files (e.g., "data/**/*_heatmap_data.tsv")')
    parser.add_argument('--output-folder', type=str, required=True,
                       help='Output folder for quantification results')
    parser.add_argument('--colormap', type=str, default='viridis',
                       help='Matplotlib colormap name (default: viridis)')
    parser.add_argument('--smooth-sigma-distance', type=float, default=1.5,
                       help='Gaussian smoothing sigma in distance dimension (default: 1.5)')
    parser.add_argument('--mask-alpha', type=float, default=0.5,
                       help='Alpha transparency for mask overlay (default: 0.5)')
    parser.add_argument('--region-grow-threshold', type=float, default=0.5,
                       help='Fraction of running maximum for region growing (default: 0.5)')
    parser.add_argument('--split-char', type=str, default='__',
                       help='Character(s) to split filename for metadata (default: "__")')
    parser.add_argument('--metadata-columns', type=str, default='Group',
                       help='Comma-separated list of metadata column names (default: "Group")')
    parser.add_argument('--force-show', action='store_true',
                       help='Display plots even when saving to file')
    parser.add_argument('--parallel', action='store_true',
                       help='Use parallel processing for multiple files')
    parser.add_argument('--log-level', type=str, default='INFO',
                       choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
                       help='Logging level (default: INFO)')
    
    args = parser.parse_args()
    
    # Setup logging
    logging.basicConfig(
        level=getattr(logging, args.log_level),
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    # Find all TSV files matching the pattern
    tsv_files = glob.glob(args.input_search_pattern, recursive=True)
    
    if len(tsv_files) == 0:
        logging.error(f"No TSV files found matching pattern: {args.input_search_pattern}")
        sys.exit(1)
    
    logging.info(f"Found {len(tsv_files)} TSV files to process")
    
    # Create output directory
    os.makedirs(args.output_folder, exist_ok=True)
    
    # Process files
    if args.parallel and len(tsv_files) > 1:
        # Parallel processing with progress bar
        n_workers = min(cpu_count(), len(tsv_files))
        logging.info(f"Using {n_workers} parallel workers")
        
        process_func = partial(process_single_tsv, output_dir=args.output_folder, args=args, quiet=True)
        
        with Pool(n_workers) as pool:
            # Use tqdm with imap for progress bar
            results = list(tqdm(
                pool.imap(process_func, tsv_files),
                total=len(tsv_files),
                desc="Processing TSV files",
                unit="file"
            ))
    else:
        # Sequential processing with progress bar
        results = []
        for tsv_file in tqdm(tsv_files, desc="Processing TSV files", unit="file"):
            result = process_single_tsv(tsv_file, args.output_folder, args, quiet=False)
            results.append(result)
    
    # Summary
    successful = sum(1 for _, success, _ in results if success)
    failed = len(results) - successful
    
    logging.info(f"Processing complete: {successful} successful, {failed} failed")
    
    if failed > 0:
        logging.info("Failed files:")
        for tsv_path, success, error_msg in results:
            if not success:
                logging.info(f"  {tsv_path}: {error_msg}")


if __name__ == "__main__":
    main()
